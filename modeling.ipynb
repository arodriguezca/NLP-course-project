{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DataCleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMUa4s5DQvc"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.autonotebook import tqdm\n",
        "from functools import partial\n",
        "import torch\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeX5_0w9SBLL",
        "outputId": "e45e5aba-0ad9-42fa-d693-a4d2e698d170"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr 28 02:28:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "s9mUxiWiEh7F",
        "outputId": "d17e4f85-3d35-49fb-a573-6dacdb70b3e6"
      },
      "source": [
        "#data = pd.read_csv(\"data.csv\")\n",
        "data = pd.read_csv(\"data_filtered.csv\")\n",
        "data.head()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bc367997880a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L05dkowDbrW"
      },
      "source": [
        "#def read_GloVe(filename):\n",
        " # embeddings = {}\n",
        " #for line in open(filename).readlines():\n",
        "    #print(line)\n",
        "   # fields = line.strip().split(\" \")\n",
        "  #  word = fields[0]\n",
        " #   embeddings[word] = [float(x) for x in fields[1:]]\n",
        "#  return embeddings\n",
        "\n",
        "#GloVe = read_GloVe(\"glove.840B.300d.conll_filtered.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "Px2P_7BtFsds",
        "outputId": "ffa9cd6c-b6ef-4cc0-a363-0d4c46778177"
      },
      "source": [
        "#import re\n",
        "#paragraphs = []\n",
        "#for i in range(len(data)):\n",
        " # t = data.loc[0][6]\n",
        " # t = t.lower()\n",
        "  #t = re.findall(r\"[\\w']+|[.,!?;]\", t)\n",
        "  #paragraphs.append(t)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a8dd9ea81f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk1bUX1mDQvj"
      },
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSCyajSuRWBC"
      },
      "source": [
        "tokens = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX3gf2HQIZfR"
      },
      "source": [
        "#filtered = []\n",
        "#for d in range(len(data)):\n",
        " # sample = data.loc[d][6]\n",
        "#  t = tokens(sample)[\"input_ids\"][1:-1]\n",
        "  #chunks = []\n",
        "#  for i in range(0, len(t), 500):\n",
        "   # chunks.append(tokens.decode(t[i:i+500]))\n",
        " # j = 0\n",
        " # for i in range(len(chunks)):\n",
        "  #  if data.loc[d][5] in chunks[i]:\n",
        "   #   j = i\n",
        "   #   break\n",
        "  #filtered.append(chunks[j])\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyEkFiZ2KPqA"
      },
      "source": [
        "#data[\"filtered\"] = filtered\n",
        "#data.to_csv(\"data_filtered.csv\")\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a_Qe-7EN2WV"
      },
      "source": [
        "data = pd.read_csv(\"data_filtered.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccP2IkBgUSIX"
      },
      "source": [
        "Crashes here trying to find hidden states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjjOLCy1KjLv",
        "outputId": "ca2c4ec8-ab9a-4f7a-cd28-bf27f856c9a8"
      },
      "source": [
        "states = []\n",
        "for i in range(len(data)):  \n",
        "  if i /50 == 0:\n",
        "    print(i)\n",
        "\n",
        "  t = tokens(data.loc[i][-1], return_tensors=\"pt\")\n",
        "\n",
        "  states.append(model(**t).last_hidden_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEbcGwD-DQvk",
        "outputId": "bb5a408e-a00f-40c3-bf47-b33dbe41eff4"
      },
      "source": [
        "sample = data.loc[0][6]\n",
        "count = 0\n",
        "print(len(t))\n",
        "token_col = []\n",
        "len_col = []\n",
        "for i in range(len(data)):\n",
        "  t = tokens(data.loc[i][6], return_tensors=\"pt\")\n",
        "  token_col.append(t)\n",
        "  len_col.append(t[\"input_ids\"].shape)\n",
        "  if t[\"input_ids\"].shape[1] > 512:\n",
        "    count +=1\n",
        "print(count)\n",
        "print(sample)\n",
        "t = tokens(sample, return_tensors=\"pt\", max_length=512)\n",
        "print(t[\"input_ids\"].shape[1])\n",
        "outputs = model(**t)\n",
        "last_hidden_layers = outputs.last_hidden_state\n",
        "print(last_hidden_layers.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1886\n",
            " The existence of gender differences in labor market outcomes, such as wages, has gained ample attention in economics and the social sciences. Gender differences in wages have been researched and documented, and frequently debated in the literature. It is an established fact that males earn substantially higher wages than females. 0049 There is some empirical evidence, however, that while the gender gap is decreasing over time due to women's increased labor force participation and experience, it remains strong across the entire wage distribution.\n",
            "The quality of the empirical evidence on gender differences in wages has not always been very strong for two main reasons. First, typically, the samples of various studies on gender differences in wages are not representative of a well-defined population. Many studies have used convenient samples, and it is plausible that the results obtained from such selected samples are biased (positively or negatively), and hence very different from their ''true'' population parameters. Second, various previous studies on gender differences in wages have typically examined and reported group differences in means (the central tendency of the distribution of wages). Gender differences in the extremes (e.g., upper and lower tails) of the wage distribution are only recently documented in the literature. Since it is likely that gender differences in the tails of the wage distribution may be different qualitatively than differences in the middle of the distribution, examining gender differences across the entire distribution of wages is important and provides a more accurate picture of the gender gap. For example, males may be over-represented in the top 10 percent of the wage distribution compared to females, a byproduct of over-concentration of men in highly paid jobs. This difference may not necessarily be similar to gender differences observed in the middle or the lower tail of the wage distribution.\n",
            "In this study we employ base years and follow-up data of national probability samples of high school students in the US, namely the National Longitudinal Study (NLS) of the High School Class of 1972 (base year, fourth and fifth follow-up) and the National Education Longitudinal Study (NELS) of the Eighth Grade Class of 1988 (second and fourth follow-up). The main advantage of these datasets is that we can link important characteristics of high schools attended in the base year to wages from follow-up years, and thus, examine how high school characteristics affect wage differentials. In addition, these rich data allow us to examine the labor market performance of similarly aged individuals 7, 8, and 14 years after high school graduation, and hence, likely avoiding transitional labor market effects. Because of the use of national probability samples our results are more likely to have higher external validity, and be more resilient to threats of selection bias. We examine gender differences in hourly wages for young adults in the late 1970s, mid 1980s, and 2000 across the entire distribution of wages, thus covering three important time spells. Our estimation technique is to employ quantile regressions while adjusting for selection biases in the labor force. Because the gender gap may be declining on average, but may be remaining strong in the upper or lower tails, affecting women disproportionately, we examine the following quantiles of the wage distribution: 10th, 25th, 50th, 75th, and 90th quantile. We conduct separate analyses for three major race/ethnic groups in the US: Whites, Blacks, and Hispanics. This permits us to determine whether the gender gap differs by race/ethnic group and whether it is changing over time.\n",
            "An equally important objective of the present study is to investigate the link between high school characteristics and hourly wages. Gauging the effects of high school characteristics on the wage gender gap is of great importance because school effects have differential and enduring effects on the earnings of individuals who attend different schools net of individual and family background characteristics (Constant and Konstantopoulos, 2003) . Previous work on school effects has yielded mixed and inconsistent findings with respect to the importance of schooling on school outputs such as academic achievement. Some researchers have concluded that there is little or no evidence of school effects (Hanushek, 1986) , while others report that the impact of school factors may be substantial (Greenwald et al., 1996) . In this study, we examine school effects on labor market outcomes and ask the question, can high school characteristics predict future wages of young adults, net of the effects of individual characteristics? If so, then which school characteristics matter more for the economic performance of young workers, and for which ethnic groups?\n",
            "512\n",
            "torch.Size([1, 512, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zhbGSiIt-3l9",
        "outputId": "7c32e3d5-77e5-4eec-c619-9d983143e83a"
      },
      "source": [
        "\n",
        "changed_text = []\n",
        "t = tokens(data.loc[0][6])\n",
        "print(tokens.decode(t[\"input_ids\"]))\n",
        "print(data.loc[0][6])\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6].split(' ')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/allenai/longformer.git\n",
            "  Cloning https://github.com/allenai/longformer.git to /tmp/pip-req-build-gru9ntrz\n",
            "  Running command git clone -q https://github.com/allenai/longformer.git /tmp/pip-req-build-gru9ntrz\n",
            "Requirement already satisfied: transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers from git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers in /usr/local/lib/python3.7/dist-packages (from longformer==0.1) (4.5.1)\n",
            "Collecting pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning\n",
            "  Cloning http://github.com/ibeltagy/pytorch-lightning.git (to revision v0.8.5_fixes) to /tmp/pip-install-5393am4w/pytorch-lightning\n",
            "  Running command git clone -q http://github.com/ibeltagy/pytorch-lightning.git /tmp/pip-install-5393am4w/pytorch-lightning\n",
            "  Running command git checkout -b v0.8.5_fixes --track origin/v0.8.5_fixes\n",
            "  Switched to a new branch 'v0.8.5_fixes'\n",
            "  Branch 'v0.8.5_fixes' set up to track remote branch 'v0.8.5_fixes' from 'origin'.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 20kB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 48.8MB/s \n",
            "\u001b[?25hCollecting test-tube==0.7.5\n",
            "  Downloading https://files.pythonhosted.org/packages/91/f0/5c32f2fbd824f32354f7f4632c957163071597bb2c6a4105f507bc9af7c0/test_tube-0.7.5.tar.gz\n",
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 41.6MB/s \n",
            "\u001b[?25hCollecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (0.10.2)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (2.4.1)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 39.4MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->longformer==0.1) (3.12.4)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from test-tube==0.7.5->longformer==0.1) (1.1.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from test-tube==0.7.5->longformer==0.1) (2.4.1)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp->longformer==0.1) (3.0.0)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp->longformer==0.1) (0.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score->longformer==0.1) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score->longformer==0.1) (0.12.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score->longformer==0.1) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers@ git+http://github.com/ibeltagy/transformers.git@longformer_encoder_decoder#egg=transformers->longformer==0.1) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (56.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (1.28.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->test-tube==0.7.5->longformer==0.1) (2.8.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio>=2.3.0->test-tube==0.7.5->longformer==0.1) (7.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning@ git+http://github.com/ibeltagy/pytorch-lightning.git@v0.8.5_fixes#egg=pytorch-lightning->longformer==0.1) (0.4.8)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.8.5-cp37-none-any.whl size=313138 sha256=183d88522cdefed322644aa40f2e117952319853acb54d1ea4a2e482451620a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lip9n83f/wheels/52/3e/bb/0ca91e975dc17933cd9b698e5ba340c4bf283e8405936da0d1\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: longformer, test-tube, future\n",
            "  Building wheel for longformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for longformer: filename=longformer-0.1-cp37-none-any.whl size=548877 sha256=e3df3c0f3ecb752fbe4fd90a430822a741a7053e084cb044e14fe0f978e64398\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lip9n83f/wheels/be/82/60/69b954e03d633f9db8c184922d93643719f549ac5a23dc8bf1\n",
            "  Building wheel for test-tube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for test-tube: filename=test_tube-0.7.5-cp37-none-any.whl size=25357 sha256=2441c4677b0ec00e40c7f25142eecfaf4e62774697e91841b98d954c8c459338\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/52/96/c8b6f3c345cfd3284845ef50818c6996a5658006fe50e40e98\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=31e50dd05cfa1a4303c5f2f1926ce9cb0c6baa6a2caa7c785f4eb94871ceba88\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built longformer test-tube future\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: PyYAML, future, torch, pytorch-lightning, tensorboardX, test-tube, xxhash, nlp, rouge-score, longformer\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed PyYAML-5.4.1 future-0.18.2 longformer-0.1 nlp-0.4.0 pytorch-lightning-0.8.5 rouge-score-0.0.4 tensorboardX-2.2 test-tube-0.7.5 torch-1.6.0 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-1ded4a4fa423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+https://github.com/allenai/longformer.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchanged_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/longformer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongformerForMaskedLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongformerConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongformer_encoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongformerEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongformer_encoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongformerEncoderDecoderForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/longformer/longformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_chunks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msliding_chunks_matmul_qk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliding_chunks_matmul_pv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlongformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_chunks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msliding_chunks_no_overlap_matmul_qk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliding_chunks_no_overlap_matmul_pv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_roberta'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx-syYeLDQvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99087c0-3664-4198-8252-b572309815c6"
      },
      "source": [
        "print(len_col)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[909, 1064, 1486, 518, 863, 986, 1995, 1500, 854, 1468, 528, 409, 1008, 1299, 976, 1235, 1945, 2704, 2294, 603, 1557, 2217, 2010, 1159, 3555, 1568, 1357, 1550, 643, 2813, 756, 1545, 1521, 495, 863, 1619, 518, 1601, 1373, 1214, 2390, 2205, 888, 678, 872, 833, 261, 610, 944, 781, 668, 515, 852, 785, 810, 1439, 533, 804, 399, 869, 825, 385, 487, 438, 648, 575, 1329, 717, 481, 742, 333, 859, 695, 875, 585, 710, 885, 844, 460, 820, 568, 765, 1194, 1012, 750, 368, 1516, 1393, 803, 924, 313, 1000, 1242, 813, 740, 1877, 1334, 2091, 2837, 756, 2288, 558, 413, 956, 1974, 2131, 574, 517, 1101, 1239, 573, 1036, 1723, 1247, 627, 456, 1595, 955, 1000, 1107, 1395, 1041, 980, 1008, 2145, 716, 499, 581, 1056, 789, 962, 961, 2287, 777, 1209, 1652, 429, 1551, 1304, 516, 1371, 2017, 1116, 666, 1285, 684, 994, 2026, 2766, 3043, 1847, 1551, 1483, 479, 522, 429, 1304, 1215, 785, 1214, 1544, 944, 781, 668, 869, 888, 825, 648, 678, 872, 833, 261, 610, 515, 852, 785, 810, 1439, 533, 804, 399, 385, 487, 438, 575, 1329, 717, 481, 742, 333, 859, 695, 875, 585, 710, 885, 844, 460, 820, 568, 765, 1194, 1012, 750, 368, 1516, 1393, 803, 924, 313, 1000, 1242, 813, 740, 1877, 1334, 2091, 2837, 2010, 189, 853, 498, 624, 620, 725, 1492, 1051, 1246, 1094, 520, 451, 1685, 583, 785, 819, 1327, 1732, 771, 366, 1280, 1450, 849, 446, 772, 1907, 723, 1071, 1147, 1076, 365, 2130, 784, 635, 804, 613, 699, 1416, 1635, 621, 975, 1564, 2179, 592, 1699, 832, 1653, 759, 858, 1600, 1490, 1930, 2205, 1231, 818, 437, 1011, 1490, 1567, 1926, 663, 2078, 1945, 1398, 639, 1768, 1542, 462, 243, 1407, 2362, 1156, 1044, 1682, 1856, 3302, 2685, 2684, 834, 1031, 732, 486, 350, 1285, 1947, 835, 645, 986, 566, 727, 1732, 1019, 1169, 1164, 1450, 1027, 774, 1095, 1160, 1164, 1749, 807, 268, 554, 1087, 649, 1032, 732, 1269, 1013, 636, 635, 671, 735, 608, 403, 741, 809, 552, 481, 355, 1408, 2023, 834, 619, 910, 888, 567, 1797, 1272, 938, 430, 2323, 867, 754, 1017, 495, 1278, 621, 656, 1061, 493, 803, 1472, 468, 1354, 2504, 1277, 1075, 816, 1126, 926, 664, 909, 1676, 440, 1572, 1214, 1593, 1128, 819, 582, 1355, 1401, 601, 818, 837, 986, 825, 1094, 760, 824, 885, 894, 863, 1108, 286, 1042, 1185, 625, 585, 1686, 1794, 832, 3151, 1649, 1017, 1235, 1163, 1078, 1590, 542, 1060, 718, 1613, 672, 964, 971, 921, 2405, 1500, 1048, 1147, 924, 1236, 450, 688, 663, 1281, 2037, 1032, 622, 1660, 1284, 1163, 1131, 523, 477, 1197, 1104, 1106, 804, 600, 730, 1056, 1125, 1125, 518, 532, 1877, 1226, 922, 886, 683, 767, 400, 1099, 809, 935, 1015, 583, 791, 902, 1766, 612, 1426, 1324, 1152, 1118, 963, 1638, 1504, 903, 1870, 783, 1086, 1174, 1511, 1619, 1401, 1051, 1044, 1013, 1388, 1052, 871, 1344, 1946, 1575, 965, 1166, 1452, 809, 860, 1143, 922, 1909, 1539, 944, 1513, 1674, 1300, 2192, 2132, 2837, 563, 548, 1018, 1366, 830, 1458, 1244, 3424, 1373, 1437, 1708, 1248, 725, 1573, 1573, 1619, 1232, 1569, 1550, 118, 1070, 2404, 448, 498, 2137, 2151, 2107, 1082, 908, 717, 1156, 956, 628, 1556, 1354, 1084, 1049, 1243, 757, 1043, 952, 426, 1202, 1450, 704, 808, 2826, 807, 1195, 883, 1143, 1513, 1190, 2221, 1582, 834, 563, 698, 1579, 233, 1253, 1409, 1015, 586, 806, 1752, 1566, 1133, 820, 466, 1703, 1416, 1051, 2205, 1092, 1939, 585, 1810, 1184, 1005, 678, 822, 538, 1135, 550, 1444, 1168, 1049, 1475, 846, 530, 930, 1513, 2688, 919, 785, 1399, 1828, 1847, 959, 925, 1320, 3286, 1450, 699, 672, 1663, 1278, 1148, 1362, 271, 730, 750, 1325, 1084, 2212, 809, 1445, 557, 1223, 572, 1542, 1100, 697, 1196, 644, 2511, 3087, 1201, 923, 1029, 1079, 881, 529, 847, 904, 964, 1145, 1717, 2574, 1212, 1253, 1192, 2744, 1174, 1448, 586, 839, 2137, 1129, 894, 838, 818, 2220, 893, 1203, 659, 3442, 985, 1237, 328, 1159, 1887, 1049, 629, 746, 590, 1066, 575, 663, 978, 1224, 1012, 800, 544, 1552, 845, 1883, 786, 847, 784, 1282, 1561, 1909, 1694, 1450, 269, 1131, 867, 649, 610, 814, 1795, 311, 2338, 38, 927, 745, 369, 823, 2106, 571, 1002, 1001, 686, 501, 185, 655, 1060, 1903, 1285, 308, 1380, 311, 656, 971, 1114, 433, 550, 827, 587, 1550, 168, 1358, 639, 737, 298, 1373, 1013, 927, 611, 672, 257, 838, 1229, 987, 1903, 991, 1870, 1273, 1395, 1287, 690, 722, 1660, 1820, 1488, 1369, 624, 414, 1095, 898, 672, 1043, 910, 1585, 553, 633, 1630, 439, 1074, 1208, 929, 1738, 1294, 629, 1127, 592, 918, 1128, 498, 799, 461, 422, 353, 1496, 528, 1057, 688, 1119, 1222, 639, 837, 1573, 950, 715, 914, 881, 916, 1182, 1494, 1947, 936, 1278, 1456, 585, 2310, 984, 562, 769, 631, 1367, 1406, 1826, 338, 831, 690, 617, 2712, 1113, 1174, 650, 1641, 1083, 526, 1609, 476, 1757, 1068, 1234, 990, 876, 644, 912, 1576, 1564, 808, 673, 659, 1197, 898, 552, 399, 638, 966, 2361, 843, 754, 697, 735, 1392, 1227, 514, 312, 1469, 2299, 517, 1871, 1077, 401, 575, 509, 930, 2079, 755, 1405, 833, 2112, 473, 431, 830, 1252, 827, 784, 1237, 915, 454, 693, 1179, 1045, 769, 1043, 1986, 267, 1075, 1360, 1463, 1113, 911, 779, 270, 857, 456, 346, 1018, 676, 2088, 414, 1207, 1304, 2185, 764, 1014, 4142, 784, 1094, 701, 1150, 724, 3954, 1439, 378, 1248, 589, 1880, 127, 1665, 1498, 1994, 1073, 1581, 797, 1681, 1244, 1994, 1322, 521, 583, 1670, 1015, 1328, 2064, 1939, 1507, 936, 1766, 1708, 851, 2742, 602, 1585, 1255, 1215, 1847, 514, 909, 1064, 1486, 1008, 518, 863, 1945, 986, 1798, 1995, 1500, 854, 629, 883, 1468, 528, 501, 409, 1299, 976, 1762, 1235, 826, 520, 2704, 618, 2294, 1136, 603, 1557, 2217, 2010, 1159, 3555, 1568, 1398, 1016, 1218, 4793, 1884, 1710, 1581, 1884, 355, 241, 550, 735, 1061, 1226, 268, 1163, 400, 466, 477, 819, 807, 619, 971, 562, 529, 493, 448, 723, 548, 876, 358, 636, 1450, 1458, 440, 628, 846, 738, 532, 1104, 880, 1569, 625, 1573, 542, 804, 708, 1359, 1388, 1052, 1125, 1125, 466, 697, 903, 608, 808, 807, 1075, 1201, 286, 965, 838, 732, 1075, 1017, 935, 660, 2826, 1106, 1408, 867, 1649, 450, 552, 566, 910, 629, 986, 672, 939, 894, 1143, 774, 468, 783, 1019, 723, 1269, 1243, 1355, 1013, 938, 2221, 1766, 1383, 495, 1619, 834, 1947, 2223, 1573, 1278, 1126, 1148, 1513, 785, 986, 825, 1196, 2037, 706, 1362, 523, 1270, 3286, 328, 585, 1013, 1078, 1099, 1300, 1821, 1810, 2192, 1732, 1472, 339, 498, 847, 233, 922, 904, 723, 1887, 886, 538, 824, 672, 1032, 1164, 1344, 1049, 924, 1129, 2151, 2205, 908, 271, 554, 1049, 563, 663, 730, 820, 530, 750, 635, 548, 1686, 1450, 1069, 1084, 1870, 1797, 910, 1428, 3181, 481, 1195, 883, 809, 629, 846, 585, 809, 1511, 1601, 1277, 3424, 1373, 1086, 1877, 1214, 2390, 767, 1174, 703, 678, 834, 1018, 1060, 350, 1100, 1048, 822, 1366, 1732, 1444, 1500, 1582, 1183, 1281, 2504, 2023, 482, 791, 746, 590, 704, 1128, 1042, 276, 699, 586, 1031, 1185, 1070, 814, 1066, 566, 1717, 741, 1426, 1409, 1108, 839, 908, 923, 1324, 671, 1278, 698, 430, 830, 1056, 839, 1029, 2137, 575, 2323, 1703, 2613, 1939, 1354, 1416, 1593, 978, 612, 1224, 1012, 860, 586, 919, 800, 1143, 1087, 881, 922, 806, 894, 118, 656, 1184, 803, 582, 1043, 863, 688, 834, 567, 818, 583, 925, 824, 1051, 885, 1445, 1032, 1168, 557, 964, 816, 2220, 964, 1166, 544, 893, 2137, 426, 1196, 1169, 2132, 1579, 2405, 1027, 1094, 760, 1556, 1909, 1552, 1092, 403, 1160, 809, 1145, 1049, 930, 486, 1164, 1232, 845, 832, 1883, 786, 888, 1284, 757, 1550, 649, 664, 754, 1203, 1174, 909, 847, 1017, 2837, 2404, 1135, 683, 804, 1262, 1539, 784, 1401, 563, 944, 1253, 1282, 621, 1152, 600, 1272, 1619, 1051, 1118, 835, 1399, 730, 644, 2212, 1064, 1244, 1044, 1163, 902, 1079, 1248, 818, 1674, 1223, 1257, 1590, 1513, 963, 1156, 1638, 956, 572, 926, 1133, 725, 871, 1749, 1159, 518, 1561, 837, 1448, 1909, 809, 1676, 1828, 1847, 1015, 959, 1437, 1946, 1708, 622, 1147, 1401, 717, 1575, 1572, 1794, 1202, 1694, 1253, 1912, 838, 2511, 1082, 965, 1280, 1192, 1475, 1660, 921, 727, 3151, 1450, 718, 2744, 952, 1663, 1354, 1452, 1752, 659, 1131, 1450, 1325, 1566, 269, 1104, 1190, 3087, 3442, 2574, 1131, 645, 1504, 1212, 663, 732, 867, 985, 2107, 2164, 1613, 649, 1542, 1237, 601, 1084, 1577, 1005, 1513, 1320, 1235, 2688, 1285, 814, 1236, 1095, 610, 413, 1974, 2017, 1382, 1371, 1995, 962, 1209, 777, 1652, 872, 1422, 2287, 2179, 1127, 961, 1119, 950, 1042, 1545, 1807, 488, 839, 1216, 977, 1202, 1042, 1838, 633, 1521, 1795, 1494, 1233, 839, 1410, 2728, 1532, 1410, 1640, 1565, 293, 408, 615, 1207, 799, 1483, 3338, 1779, 839, 1594, 838, 465, 829, 886, 1120, 2234, 1001, 1360, 1822, 910, 1710, 1581, 1174, 213, 2023, 2288, 558, 2440, 1257, 1257, 729, 1056, 7446, 618, 727, 460, 1135, 883, 618, 1540, 691, 634, 585, 524, 1293, 1605, 490, 793, 1318, 263, 1011, 464, 469, 650, 2010, 572, 407, 753, 797, 1026, 1296, 457, 1144, 1816, 1816, 1816, 609, 609, 705, 705, 705, 206, 206, 986, 986, 685, 685, 1085, 1085, 374, 374, 753, 753, 753, 187, 187, 962, 962, 962, 182, 182, 645, 645, 130, 130, 1072, 1072, 154, 154, 154, 1370, 1370, 1370, 1370, 1827, 1827, 1827, 1830, 1830, 1830, 1830, 746, 746, 204, 204, 204, 204, 204, 1198, 1198, 234, 234, 206, 206, 1148, 721, 721, 2387, 336, 336, 432, 432, 986, 986, 886, 886, 886, 886, 951, 951, 951, 951, 951, 723, 2902, 2902, 1680, 1680, 856, 856, 647, 647, 581, 581, 419, 419, 1200, 1200, 420, 420, 817, 817, 169, 169, 641, 329, 2133, 2133, 743, 503, 497, 497, 497, 497, 497, 63, 63, 753, 723, 753, 987, 1246, 937, 918, 470, 609, 609, 365, 482, 1598, 1598, 243, 598, 3588, 3588, 892, 695, 699, 599, 589, 2166, 2166, 1409, 538, 1435, 483, 538, 144, 144, 513, 513, 1796, 1796, 268, 268, 470, 470, 749, 749, 354, 354, 1286, 1286, 79, 79, 511, 511, 447, 447, 2440, 841, 1912, 1912, 1912, 679, 893, 932, 974, 503, 504, 214, 1027, 218, 2249, 975, 658, 635, 1726, 1726, 1458, 2663, 378, 1137, 1460, 879, 532, 702, 761, 594, 614, 521, 819, 417, 840, 676, 1134, 538, 1069, 749, 4731, 1128, 597, 207, 523, 523, 900, 413, 833, 624, 320, 308, 1156, 2203, 1326, 987, 2089, 1571, 973, 973, 335, 335, 1738, 597, 222, 222, 421, 1519, 1310, 1366, 1366, 949, 949, 1805, 2664, 2664, 5505, 5505, 1233, 685, 685, 839, 862, 862, 860, 860, 860, 1017, 2236, 2236, 2236, 471, 591, 639, 639, 635, 1202, 1202, 1202, 407, 1290, 1290, 1646, 1646, 1646, 293, 293, 1265, 1265, 1229, 1417, 661, 1642, 1642, 1689, 1689, 1689, 1689, 1265, 1376, 1720, 1920, 370, 1462, 3257, 1252, 2681, 914, 943, 226, 937, 215, 373, 365, 1529, 851, 735, 926, 523, 1082, 265, 926, 2359, 1044, 6271, 1190, 3411, 3411, 3606, 2744, 4001, 4001, 4045, 4045, 1967, 381, 1385, 1723, 810, 1018, 910, 1965, 1965, 477, 324, 537, 1073, 810, 986, 564, 1572, 2186, 4162, 1054, 1054, 489, 992, 507, 806, 1084, 2784, 600, 425, 2066, 1215, 423, 423, 1376, 919, 919, 949, 1774, 1774, 1094, 330, 92, 693, 627, 918, 501, 1266, 1294, 537, 575, 750, 1358, 429, 761, 867, 917, 758, 1260, 485, 625, 1617, 238, 507, 1191, 506, 357, 787, 875, 586, 571, 168, 460, 1084, 566, 559, 1271, 1172, 868, 1105, 1234, 180, 1172, 568, 805, 156, 838, 2208, 2159, 270, 2626, 784, 3136, 534, 794, 1926, 1128, 1240, 685, 685, 1526, 832, 460, 879, 863, 749, 742, 277, 554, 347, 1559, 2098, 612, 1286, 1286, 7446, 706, 1033, 215, 215, 1473, 1473, 1548, 2658, 1939, 655, 593, 524, 524, 952, 952, 2125, 2125, 372, 1171, 577, 1272, 425, 728, 1005, 674, 573, 573, 868, 868, 1682, 2216, 553, 424, 508, 508, 517, 1180, 1214, 642, 1670, 1670, 261, 1127, 1255, 933, 933, 838, 1109, 1022, 1022, 2753, 955, 965, 1869, 857, 1059, 1422, 963, 3036, 353, 1300, 1055, 1574, 1414, 643, 759, 1885, 1213, 1292, 1292, 543, 391, 1436, 1436, 1633, 1633, 1056, 1056, 902, 902, 2100, 2100, 894, 894, 964, 964, 964, 1823, 1823, 1554, 1554, 1355, 1355, 455, 455, 529, 529, 902, 902, 2939, 2939, 1332, 880, 1332, 880, 932, 1408, 744, 1157, 807, 649, 401, 1202, 1408, 1085, 728, 1111, 719, 741, 702, 699, 1041, 741, 1111, 1085, 728, 719, 702, 699, 1041, 673, 879, 917, 727, 530, 372, 403, 935, 1181, 906, 423, 618, 3957, 792, 458, 1305, 656, 1612, 600, 1334, 576, 701, 618, 1008, 754, 1391, 620, 399, 565, 1185, 450, 673, 676, 696, 976, 531, 703, 2992, 1149, 1707, 433, 572, 333, 534, 1916, 1175, 1035, 879, 1391, 673, 696, 676, 653, 1357, 853, 1567]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVBkFkZ6DQvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c0c4d8-6908-4aca-9e75-47cc25f114fc"
      },
      "source": [
        "data = data[data[\"token_len\"] < 512]\n",
        "print(len(data))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8Mu59xODQvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2352da03-1aa6-4de2-e6b2-12315f9ef757"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "spacy_sent = spacy.load('en_core_web_sm')\n",
        "sentences = spacy_sent(data.loc[0][6]).sents\n",
        "base = torch.zeros((1,0,768))\n",
        "print(data.loc[0][6])\n",
        "for i in sentences:\n",
        "   print(i.text)\n",
        "   #t = tokens(i.text, return_tensors=\"pt\")\n",
        "   #outputs = model(**t)\n",
        "   #last_hidden_layers = outputs.last_hidden_state\n",
        "   #base = torch.cat((base, last_hidden_layers), dim=1)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The existence of gender differences in labor market outcomes, such as wages, has gained ample attention in economics and the social sciences. Gender differences in wages have been researched and documented, and frequently debated in the literature. It is an established fact that males earn substantially higher wages than females. 0049 There is some empirical evidence, however, that while the gender gap is decreasing over time due to women's increased labor force participation and experience, it remains strong across the entire wage distribution.\n",
            "The quality of the empirical evidence on gender differences in wages has not always been very strong for two main reasons. First, typically, the samples of various studies on gender differences in wages are not representative of a well-defined population. Many studies have used convenient samples, and it is plausible that the results obtained from such selected samples are biased (positively or negatively), and hence very different from their ''true'' population parameters. Second, various previous studies on gender differences in wages have typically examined and reported group differences in means (the central tendency of the distribution of wages). Gender differences in the extremes (e.g., upper and lower tails) of the wage distribution are only recently documented in the literature. Since it is likely that gender differences in the tails of the wage distribution may be different qualitatively than differences in the middle of the distribution, examining gender differences across the entire distribution of wages is important and provides a more accurate picture of the gender gap. For example, males may be over-represented in the top 10 percent of the wage distribution compared to females, a byproduct of over-concentration of men in highly paid jobs. This difference may not necessarily be similar to gender differences observed in the middle or the lower tail of the wage distribution.\n",
            "In this study we employ base years and follow-up data of national probability samples of high school students in the US, namely the National Longitudinal Study (NLS) of the High School Class of 1972 (base year, fourth and fifth follow-up) and the National Education Longitudinal Study (NELS) of the Eighth Grade Class of 1988 (second and fourth follow-up). The main advantage of these datasets is that we can link important characteristics of high schools attended in the base year to wages from follow-up years, and thus, examine how high school characteristics affect wage differentials. In addition, these rich data allow us to examine the labor market performance of similarly aged individuals 7, 8, and 14 years after high school graduation, and hence, likely avoiding transitional labor market effects. Because of the use of national probability samples our results are more likely to have higher external validity, and be more resilient to threats of selection bias. We examine gender differences in hourly wages for young adults in the late 1970s, mid 1980s, and 2000 across the entire distribution of wages, thus covering three important time spells. Our estimation technique is to employ quantile regressions while adjusting for selection biases in the labor force. Because the gender gap may be declining on average, but may be remaining strong in the upper or lower tails, affecting women disproportionately, we examine the following quantiles of the wage distribution: 10th, 25th, 50th, 75th, and 90th quantile. We conduct separate analyses for three major race/ethnic groups in the US: Whites, Blacks, and Hispanics. This permits us to determine whether the gender gap differs by race/ethnic group and whether it is changing over time.\n",
            "An equally important objective of the present study is to investigate the link between high school characteristics and hourly wages. Gauging the effects of high school characteristics on the wage gender gap is of great importance because school effects have differential and enduring effects on the earnings of individuals who attend different schools net of individual and family background characteristics (Constant and Konstantopoulos, 2003) . Previous work on school effects has yielded mixed and inconsistent findings with respect to the importance of schooling on school outputs such as academic achievement. Some researchers have concluded that there is little or no evidence of school effects (Hanushek, 1986) , while others report that the impact of school factors may be substantial (Greenwald et al., 1996) . In this study, we examine school effects on labor market outcomes and ask the question, can high school characteristics predict future wages of young adults, net of the effects of individual characteristics? If so, then which school characteristics matter more for the economic performance of young workers, and for which ethnic groups?\n",
            " The existence of gender differences in labor market outcomes, such as wages, has gained ample attention in economics and the social sciences.\n",
            "Gender differences in wages have been researched and documented, and frequently debated in the literature.\n",
            "It is an established fact that males earn substantially higher wages than females.\n",
            "0049\n",
            "There is some empirical evidence, however, that while the gender gap is decreasing over time due to women's increased labor force participation and experience, it remains strong across the entire wage distribution.\n",
            "\n",
            "The quality of the empirical evidence on gender differences in wages has not always been very strong for two main reasons.\n",
            "First, typically, the samples of various studies on gender differences in wages are not representative of a well-defined population.\n",
            "Many studies have used convenient samples, and it is plausible that the results obtained from such selected samples are biased (positively or negatively), and hence very different from their ''true'' population parameters.\n",
            "Second, various previous studies on gender differences in wages have typically examined and reported group differences in means (the central tendency of the distribution of wages).\n",
            "Gender differences in the extremes (e.g., upper and lower tails) of the wage distribution are only recently documented in the literature.\n",
            "Since it is likely that gender differences in the tails of the wage distribution may be different qualitatively than differences in the middle of the distribution, examining gender differences across the entire distribution of wages is important and provides a more accurate picture of the gender gap.\n",
            "For example, males may be over-represented in the top 10 percent of the wage distribution compared to females, a byproduct of over-concentration of men in highly paid jobs.\n",
            "This difference may not necessarily be similar to gender differences observed in the middle or the lower tail of the wage distribution.\n",
            "\n",
            "In this study we employ base years and follow-up data of national probability samples of high school students in the US, namely the National Longitudinal Study (NLS) of the High School Class of 1972 (\n",
            "base year, fourth and fifth follow-up) and the National Education Longitudinal Study (NELS) of the Eighth Grade Class of 1988 (second and fourth follow-up).\n",
            "The main advantage of these datasets is that we can link important characteristics of high schools attended in the base year to wages from follow-up years, and thus, examine how high school characteristics affect wage differentials.\n",
            "In addition, these rich data allow us to examine the labor market performance of similarly aged individuals 7, 8, and 14 years after high school graduation, and hence, likely avoiding transitional labor market effects.\n",
            "Because of the use of national probability samples our results are more likely to have higher external validity, and be more resilient to threats of selection bias.\n",
            "We examine gender differences in hourly wages for young adults in the late 1970s, mid 1980s, and 2000 across the entire distribution of wages, thus covering three important time spells.\n",
            "Our estimation technique is to employ quantile regressions while adjusting for selection biases in the labor force.\n",
            "Because the gender gap may be declining on average, but may be remaining strong in the upper or lower tails, affecting women disproportionately, we examine the following quantiles of the wage distribution:\n",
            "10th, 25th, 50th, 75th, and 90th quantile.\n",
            "We conduct separate analyses for three major race/ethnic groups in the US: Whites, Blacks, and Hispanics.\n",
            "This permits us to determine whether the gender gap differs by race/ethnic group and whether it is changing over time.\n",
            "\n",
            "An equally important objective of the present study is to investigate the link between high school characteristics and hourly wages.\n",
            "Gauging the effects of high school characteristics on the wage gender gap is of great importance because school effects have differential and enduring effects on the earnings of individuals who attend different schools net of individual and family background characteristics (Constant and Konstantopoulos, 2003) .\n",
            "Previous work on school effects has yielded mixed and inconsistent findings with respect to the importance of schooling on school outputs such as academic achievement.\n",
            "Some researchers have concluded that there is little or no evidence of school effects (Hanushek, 1986) , while others report that the impact of school factors may be substantial (Greenwald et al., 1996) .\n",
            "In this study, we examine school effects on labor market outcomes and ask the question, can high school characteristics predict future wages of young adults, net of the effects of individual characteristics?\n",
            "If so, then which school characteristics matter more for the economic performance of young workers, and for which ethnic groups?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpfpUJwNDQvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f608d8-f951-401f-fbe9-687a93759c02"
      },
      "source": [
        "print(base.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 967, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZuuSg2XDQvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58a5485-2b9b-4e9f-b3eb-53ff36db2e79"
      },
      "source": [
        "t = tokens(data.loc[0][6], return_tensors=\"pt\")\n",
        "print(t[\"input_ids\"].shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 909])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "jXsGzzWxDUj3",
        "outputId": "0003dc1e-df8b-4b0c-8ff7-61f7936ccfb1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2c7e3bfa8b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}