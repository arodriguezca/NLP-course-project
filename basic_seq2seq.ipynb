{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DataCleaning_(2) (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3648d55802de4d9697db4018695ce068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82c89513b63f45c2bb646dd3532d8cd0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d56c7314534e40eaaaaeb38920b331a3",
              "IPY_MODEL_e8d90bb848b84c58a5002fd9f623aaa2"
            ]
          }
        },
        "82c89513b63f45c2bb646dd3532d8cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d56c7314534e40eaaaaeb38920b331a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba078f36bbc94285909501abc0b1d8b6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c8c3550f24c4b189474706b2096ad5b"
          }
        },
        "e8d90bb848b84c58a5002fd9f623aaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c21c7a2719dc43d98ab7ef517005537b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:11&lt;00:00,  2.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a046fc3f05e420bb8f14e064b03a970"
          }
        },
        "ba078f36bbc94285909501abc0b1d8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c8c3550f24c4b189474706b2096ad5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c21c7a2719dc43d98ab7ef517005537b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a046fc3f05e420bb8f14e064b03a970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64be3b0cebde4ab89c8eac7a110dc167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed5c666268bd4430bddcec55f171f879",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e0910ee6cc94d47893adef3f346a285",
              "IPY_MODEL_082b6cf5ebd1456bb18807931374022a"
            ]
          }
        },
        "ed5c666268bd4430bddcec55f171f879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e0910ee6cc94d47893adef3f346a285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_03a5c82916df40bfb0cb83a1a16c14fc",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a7c163c7a224cc9a8a00c25ce884a25"
          }
        },
        "082b6cf5ebd1456bb18807931374022a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b87042d2b06040d58811c084e6806207",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:15&lt;00:00,  1.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8acfaefcdcd44b0be02b722dd183385"
          }
        },
        "03a5c82916df40bfb0cb83a1a16c14fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a7c163c7a224cc9a8a00c25ce884a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b87042d2b06040d58811c084e6806207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8acfaefcdcd44b0be02b722dd183385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4593924c459424b8305bc6ba4c8567d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23122ff6ae644bf196c832c2b91685fc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f6763232743f4fb2a627b9c40e6a5bde",
              "IPY_MODEL_cc23cdb0ffcd4be2b15efbbf8d82a81c"
            ]
          }
        },
        "23122ff6ae644bf196c832c2b91685fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6763232743f4fb2a627b9c40e6a5bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5ca59b7393741a19a1596d001a4649b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4442da74138142ebbbc90d6614dd8370"
          }
        },
        "cc23cdb0ffcd4be2b15efbbf8d82a81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_284e8aba73e04d6c9390022b2f232f68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:18&lt;00:00,  2.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2768b728fcca472e956700ff021b5848"
          }
        },
        "e5ca59b7393741a19a1596d001a4649b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4442da74138142ebbbc90d6614dd8370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "284e8aba73e04d6c9390022b2f232f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2768b728fcca472e956700ff021b5848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ba5a6e4f10a441b9e531588b19fac96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_840e638c798c4bb59aabcd7975c2b377",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f558752a1ee4a568543b59fa5b4828a",
              "IPY_MODEL_093315b3ee954fefb847b248f9e4a571"
            ]
          }
        },
        "840e638c798c4bb59aabcd7975c2b377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f558752a1ee4a568543b59fa5b4828a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_236dad0222684cb4a9959fe144dcb1eb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_508a52d701fc4e5eba94148bf683ead5"
          }
        },
        "093315b3ee954fefb847b248f9e4a571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3818308e7f2f442991c2ff3fd46aaee3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:17&lt;00:00,  2.86it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c82c752d629a470fafbd59ac08f5fb64"
          }
        },
        "236dad0222684cb4a9959fe144dcb1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "508a52d701fc4e5eba94148bf683ead5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3818308e7f2f442991c2ff3fd46aaee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c82c752d629a470fafbd59ac08f5fb64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "467a7652376147d6b131f9f028243349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f2567213608344ef93fad6dda286baa4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4233f2fd104a4c8eaa2bf0e1436bf13b",
              "IPY_MODEL_08d4b4f5edd54692b29f9115e26e01f5"
            ]
          }
        },
        "f2567213608344ef93fad6dda286baa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4233f2fd104a4c8eaa2bf0e1436bf13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a02bb26f3b8a49c290c3b4de4fc0cfd9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc5dd56960784d07914924119412c66c"
          }
        },
        "08d4b4f5edd54692b29f9115e26e01f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40dbd52d81fe46a994ab7997cd1e8c1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:15&lt;00:00,  2.18it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5042470d3c3b494bb771550b38b83cf5"
          }
        },
        "a02bb26f3b8a49c290c3b4de4fc0cfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc5dd56960784d07914924119412c66c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40dbd52d81fe46a994ab7997cd1e8c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5042470d3c3b494bb771550b38b83cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0628d11e578420a9945e6a5390d241c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e7b02e0aeed417db626ea39af54afd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c7fd661a36d4a6eaf978c05af9b749f",
              "IPY_MODEL_98e1b8690b7045409255cc0b9813e557"
            ]
          }
        },
        "0e7b02e0aeed417db626ea39af54afd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c7fd661a36d4a6eaf978c05af9b749f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b67ef296d594fef88310a3995643980",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ee9620d4eb84abbbe5a6c9f4399155a"
          }
        },
        "98e1b8690b7045409255cc0b9813e557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4546b1ef9834766bdfbc3682641247e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:15&lt;00:00,  3.12it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df536805435e47babb9df58add8117d5"
          }
        },
        "0b67ef296d594fef88310a3995643980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ee9620d4eb84abbbe5a6c9f4399155a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4546b1ef9834766bdfbc3682641247e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df536805435e47babb9df58add8117d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9694f2b93df04f0a8437b2ccdf47fb01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7963ab83642c4c7a8ad0d26d32b557e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5b32fba956bd448ca48670db26efc31c",
              "IPY_MODEL_cf73d0b840d24d1c80b86f0444ec6cb9"
            ]
          }
        },
        "7963ab83642c4c7a8ad0d26d32b557e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b32fba956bd448ca48670db26efc31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2bc85f55c9b04cb095a9e690aca237f6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 290,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 290,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac0da1e6543f48e385fba73cba6acabf"
          }
        },
        "cf73d0b840d24d1c80b86f0444ec6cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7869df865a4843e5b746d2b7ca89e5f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 290/290 [02:14&lt;00:00,  1.92it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b39a19f9384d49b990a8bdb91a7b2f84"
          }
        },
        "2bc85f55c9b04cb095a9e690aca237f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac0da1e6543f48e385fba73cba6acabf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7869df865a4843e5b746d2b7ca89e5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b39a19f9384d49b990a8bdb91a7b2f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a0feddd6747414e8572c1eff721328a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d416ae261a99422684b3a49123edea5e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b755f66966bb41ccaeba031f3d47ea7e",
              "IPY_MODEL_d04b8b2df6c34403a9c3d97dd3ed6d73"
            ]
          }
        },
        "d416ae261a99422684b3a49123edea5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b755f66966bb41ccaeba031f3d47ea7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_11022d0803a148f5a90df1f077c44b90",
            "_dom_classes": [],
            "description": "training: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_374ea2ff10114be988fc493be600bebb"
          }
        },
        "d04b8b2df6c34403a9c3d97dd3ed6d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c5e9e057fc3b44a082339f1c65643bba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:31&lt;00:00, 31.84s/epoch]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c208a10fbaf14da283b832b2fbe2191c"
          }
        },
        "11022d0803a148f5a90df1f077c44b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "374ea2ff10114be988fc493be600bebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5e9e057fc3b44a082339f1c65643bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c208a10fbaf14da283b832b2fbe2191c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37e351c02c8744ca920550a01dddb368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_720ae00932c54266a1b4253b6b78660b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2da511e144f740069ca53f3ae0457bd5",
              "IPY_MODEL_b2153bdd674942c7a6c1cb4f42579af8"
            ]
          }
        },
        "720ae00932c54266a1b4253b6b78660b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2da511e144f740069ca53f3ae0457bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a745b13b98249b0b7426a7bfa01483d",
            "_dom_classes": [],
            "description": "epoch 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_604b6f56ecec496f8b3a6d23d290bf79"
          }
        },
        "b2153bdd674942c7a6c1cb4f42579af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1cc6d5714a94b0396d60f36787c2d00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:31&lt;00:00, 31.82s/batch, current_loss=9.44, mean_loss=9.44]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec47be82a380401baddc14b098bfb1d3"
          }
        },
        "7a745b13b98249b0b7426a7bfa01483d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "604b6f56ecec496f8b3a6d23d290bf79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1cc6d5714a94b0396d60f36787c2d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec47be82a380401baddc14b098bfb1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMUa4s5DQvc"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.autonotebook import tqdm\n",
        "from functools import partial\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "#!pip install transformers\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "#import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeX5_0w9SBLL",
        "outputId": "a912e20d-b279-4e5e-dc4c-c5216c9080d9"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 29 17:57:57 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NcQtlQS5EHb",
        "outputId": "a35ced73-a99f-4168-8274-e2e930a17aae"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9mUxiWiEh7F",
        "outputId": "b4e0408a-a3be-4dbe-e44d-fdc221f4a495"
      },
      "source": [
        "#data = pd.read_csv(\"data.csv\")\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "data.head()\n",
        "print(len(data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L05dkowDbrW"
      },
      "source": [
        "def read_GloVe(filename):\n",
        " embeddings = {}\n",
        " for line in open(filename).readlines():\n",
        "\n",
        "    fields = line.strip().split(\" \")\n",
        "    word = fields[0]\n",
        "    embeddings[word] = [float(x) for x in fields[1:]]\n",
        " return embeddings\n",
        "\n",
        "GloVe = read_GloVe(\"glove.840B.300d.conll_filtered.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-HeyoODFVL1V",
        "outputId": "3421aa2e-b507-405a-c688-6275f21cc4c3"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "      <th>text</th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This article investigates an important factor...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>81b5f874-9b42-4d4f-8c50-b4bab24f8aed</td>\n",
              "      <td>The Gender Gap Reloaded: Are School Characteri...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This study examines the wage gender gap of yo...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>f35fdfa6-a3f9-4ef3-a858-c8e2efcd2349</td>\n",
              "      <td>The High School Environment and the Gender Gap...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Despite the striking reversal of the gender g...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>e5a4c6af-0854-4f7e-b088-06f03c18a01a</td>\n",
              "      <td>Locus of Control and Peer Relationships Among ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Abstract Past research has shown that locus o...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>d5ff0ecd-ccf5-4f8a-a09c-182376eed4d0</td>\n",
              "      <td>Beyond the Laboratory: Evaluating the Survey E...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>The black-white gap in achievement, as measur...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  mask\n",
              "0           4  ...  True\n",
              "1           5  ...  True\n",
              "2           6  ...  True\n",
              "3           8  ...  True\n",
              "4          12  ...  True\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px2P_7BtFsds"
      },
      "source": [
        "import re\n",
        "paragraphs = []\n",
        "copy = []\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6]\n",
        "  t = t.lower()\n",
        "  t = re.findall(r\"[\\w']+|[.,!?;]\", t)\n",
        "  paragraphs.append(t)\n",
        "  copy.append((t, data.loc[i][5]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_8hG0yXVHym",
        "outputId": "eeb9e067-aecc-4e87-c36f-74314078eec3"
      },
      "source": [
        "#Create mappings between tokens and indices.\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "#Will need this later to remove 50% of words that only appear once in the training data from the vocabulary (and don't have GloVe embeddings).\n",
        "wordCounts = Counter([w for l in paragraphs for w in l])\n",
        "singletons = set([w for (w,c) in wordCounts.items() if c == 1 and not w in GloVe.keys()])\n",
        "\n",
        "\n",
        "#Build dictionaries to map from words, characters to indices and vice versa.\n",
        "#Save first two words in the vocabulary for padding and \"UNK\" token.\n",
        "word2i = {w:i+2 for i,w in enumerate(set([w for l in paragraphs for w in l] + list(GloVe.keys())))}\n",
        "\n",
        "i2word = {i:w for w,i in word2i.items()}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#When training, randomly replace singletons with UNK tokens sometimes to simulate situation at test time.\n",
        "def getDictionaryRandomUnk(w, dictionary, train=False):\n",
        "  if train and (w in singletons and random.random() > 0.5):\n",
        "    return 1\n",
        "  else:\n",
        "    return dictionary.get(w, 1)\n",
        "\n",
        "#Map a list of sentences from words to indices.\n",
        "def sentences2indices(words, dictionary, train=False):\n",
        "  #1.0 => UNK\n",
        "  return [[getDictionaryRandomUnk(w,dictionary, train=train) for w in l] for l in words]\n",
        "\n",
        "X       = sentences2indices(paragraphs, word2i, train=True)\n",
        "\n",
        "\n",
        "print(i2word[253])\n",
        "\n",
        "#Print out some examples of what the dev inputs will look like\n",
        "for i in range(10):\n",
        "  print(\" \".join([i2word.get(w,'UNK') for w in X[i]]))\n",
        "def prepare_input(X_list):\n",
        "  X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X_list], batch_first=True).type(torch.LongTensor)\n",
        "  X_mask   = torch.nn.utils.rnn.pad_sequence([torch.as_tensor([1.0] * len(l)) for l in X_list], batch_first=True).type(torch.FloatTensor)\n",
        "  return (X_padded, X_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dpi\n",
            "this article investigates an important factor in student achievement parental involvement . using data from the national education longitudinal study nels , we estimate a value added education production function that includes parental effort as an input . parental effort equations are also estimated as a function of child , parent , household , and school characteristics . our results suggest that parental effort has a strong positive effect on achievement that is large relative to the effect of school resources and is not captured by family background variables . parents appear to reduce their effort in response to increased school resources , suggesting potential ''crowding out'' of school resources .\n",
            "this study examines the wage gender gap of young adults in the 1970s , 1980s , and 2000 in the us . using quantile regression we estimate the gender gap across the entire wage distribution . we also study the importance of high school characteristics in predicting future labor market performance . we conduct analyses for three major racial ethnic groups in the us whites , blacks , and hispanics , employing data from two rich longitudinal studies nls and nels . our results indicate that while some school characteristics are positive and significant predictors of future wages for whites , they are less so for the two minority groups . we find significant wage gender disparities favoring men across all three surveys in the 1970s , 1980s , and 2000 . the wage gender gap is more pronounced in higher paid jobs 90th quantile for all groups , indicating the presence of a persistent and alarming ''glass ceiling . '' Ã³ 2007 elsevier inc . all rights reserved . jel codes j16 ; j24 ; j31 the existence of gender differences in labor market outcomes , such as wages , has gained ample attention in economics and the social sciences . gender differences in wages have been researched and documented , and frequently debated in the literature . it is an established fact that males earn substantially higher wages than females . 0049 there is some empirical evidence , however , that while the gender gap is decreasing over time due to women's increased labor force participation and experience , it remains strong across the entire wage distribution . the quality of the empirical evidence on gender differences in wages has not always been very strong for two main reasons . first , typically , the samples of various studies on gender differences in wages are not representative of a well defined population . many studies have used convenient samples , and it is plausible that the results obtained from such selected samples are biased positively or negatively , and hence very different from their ''true'' population parameters . second , various previous studies on gender differences in wages have typically examined and reported group differences in means the central tendency of the distribution of wages . gender differences in the extremes e . g . , upper and lower tails of the wage distribution are only recently documented in the literature . since it is likely that gender differences in the tails of the wage distribution may be different qualitatively than differences in the middle of the distribution , examining gender differences across the entire distribution of wages is important and provides a more accurate picture of the gender gap . for example , males may be over represented in the top 10 percent of the wage distribution compared to females , a byproduct of over concentration of men in highly paid jobs . this difference may not necessarily be similar to gender differences observed in the middle or the lower tail of the wage distribution . in this study we employ base years and follow up data of national probability samples of high school students in the us , namely the national longitudinal study nls of the high school class of 1972 base year , fourth and fifth follow up and the national education longitudinal study nels of the eighth grade class of 1988 second and fourth follow up . the main advantage of these datasets is that we can link important characteristics of high schools attended in the base year to wages from follow up years , and thus , examine how high school characteristics affect wage differentials . in addition , these rich data allow us to examine the labor market performance of similarly aged individuals 7 , 8 , and 14 years after high school graduation , and hence , likely avoiding transitional labor market effects . because of the use of national probability samples our results are more likely to have higher external validity , and be more resilient to threats of selection bias . we examine gender differences in hourly wages for young adults in the late 1970s , mid 1980s , and 2000 across the entire distribution of wages , thus covering three important time spells . our estimation technique is to employ quantile regressions while adjusting for selection biases in the labor force . because the gender gap may be declining on average , but may be remaining strong in the upper or lower tails , affecting women disproportionately , we examine the following quantiles of the wage distribution 10th , 25th , 50th , 75th , and 90th quantile . we conduct separate analyses for three major race ethnic groups in the us whites , blacks , and hispanics . this permits us to determine whether the gender gap differs by race ethnic group and whether it is changing over time . an equally important objective of the present study is to investigate the link between high school characteristics and hourly wages . gauging the effects of high school characteristics on the wage gender gap is of great importance because school effects have differential and enduring effects on the earnings of individuals who attend different schools net of individual and family background characteristics constant and konstantopoulos , 2003 . previous work on school effects has yielded mixed and inconsistent findings with respect to the importance of schooling on school outputs such as academic achievement . some researchers have concluded that there is little or no evidence of school effects hanushek , 1986 , while others report that the impact of school factors may be substantial greenwald et al . , 1996 . in this study , we examine school effects on labor market outcomes and ask the question , can high school characteristics predict future wages of young adults , net of the effects of individual characteristics ? if so , then which school characteristics matter more for the economic performance of young workers , and for which ethnic groups ?\n",
            "despite the striking reversal of the gender gap in education , women pursue science , technology , engineering , and mathematics stem degrees at much lower rates than those of their male peers . this study extends existing explanations for these gender differences and examines the role of the high school context for plans to major in stem fields . building on recent gender theories , we argue that widely shared and hegemonic gender beliefs manifest differently across schools so that the gender specific formation of study plans is shaped by the local environment of high schools . using the national education longitudinal study , we first show large variations between high schools in the ability to attract students to stem fields conditional on a large set of pre high school measures . schools that are successful in attracting students to these fields reduce the gender gap by 25 percent or more . as a first step toward understanding what matters about schools , we then estimate the effect of two concrete high school characteristics on plans to major in stem fields in college a high school's curriculum in stem and gender segregation of extracurricular activities . these factors have a substantial effect on the gender gap in plans to major in stem a finding that is reaffirmed in a number of sensitivity analyses . our focus on the high school context opens concrete avenues for policy intervention and is of central theoretical importance to understand the gender gap in orientations toward stem fields .\n",
            "abstract past research has shown that locus of control plays an important role in a wide range of behaviors , such as academic achievement and positive social behaviors . however , little is known about whether locus of control plays the same role in minority adolescents' peer relationships . the current study examined ethnic differences in the associations between locus of control and peer relationships in early adolescence using samples from the early childhood longitudinal study ecls k 5 , 612 caucasian , 1 , 562 hispanic , 507 asian , and 908 africanamerican adolescents and the national education longitudinal study nels 8 , 484 caucasian , 1 , 604 hispanic , and 860 asian , and 1 , 228 african american adolescents . gender was approximately evenly split in both samples . the results from the two datasets were highly consistent . significant interactions between ethnicity and locus of control indicated that having a more internal locus of control was particularly important for caucasian students' peer relationships ecls k and social status nels , but less so for asian , hispanic , and african american students . our findings suggest that the role of locus of control in peer relationship is contingent upon culture . locus of control , a concept developed by rotter 1954 , focuses on the degree to which individuals generally believe that they , rather than other people or uncontrollable factors such as ''fate , '' are responsible for the outcomes of events in their lives . rotter 1954 conceptualized locus of control as occurring on a continuum from internal to external control , rather than as a dichotomous variable . among the most replicable findings , internal locus of control has been associated with better academic performance e . g . , chang et al . 2007 ; strayhorn 2010 , healthrelated outcomes e . g . , jose and weir 2013 ; sturmer et al . 2006 , job performance see review by ng et al . 2006 , and athletic performance e . g . , denny and steiner 2009 , whereas lacking a sense of control has been associated with negative outcomes , such as anxiety weems et al . 2003 and depression muris et al . 2004 . in short , decades of research have shown that locus of control plays an important role in a wide range of behaviors . most relevant to the current study , several studies also have shown that locus of control is associated with social relationships . specifically , studies with adult subjects demonstrated that those individuals who reported greater internal locus of control were more willing to communicate with out groups lam and mizerski 2005 and characterized themselves as being more sociable and open to new experiences mÃ¼hlig versen et al . 2012 ; rubin 1993 . similarly , research has shown that children and adolescents with a more internal locus of control are more engaged in their classrooms you and sharkey 2009 , are less shy crozier 2011 , and display greater social maturity nelson and mathia 1995 . it also appears that having an external locus of control orientation is associated with aggressive tendencies and bullying behaviors osterman et al . 1999 , which can contribute to difficulties developing or maintaining friendships . for example , in a study of 5 7 year old australian elementary school children , slee 1993 found that children who were identified as bullies through self and teacher reports showed a more external locus of control . students who were identified as victims did not differ in locus of control and students who were not identified as either bullies or victims reported a more internal locus of control . in sum , there is considerable evidence for a significant role of locus of control in peer relationships . the above conclusion , however , is based on studies of mostly caucasian participants . little is known about whether locus of control plays the same role in minority adolescents' peer relationships . thus far , cross cultural and cross ethnic research on locus of control has focused mostly on either general locus of control or in the domain of academic achievement and the results have been inconsistent . a few earlier studies documented that asians , asian americans , and mexicanamericans had a more internal locus of control than their caucasian counterparts brown et al . 2007 ; hamilton et al . 1989 ; si et al . 1995 . a number of studies , however , showed that ethnic minority groups such as african , asian , and mexican americans tended to hold a more external locus of control leung 2001 ; o'hea et al . 2009 ; okeke et al . 1999 ; ramirez 1988 than european americans . an orientation towards external locus of control seems consistent with fatalistic beliefs that prevail in many asian cultures norenzayan and lee 2010 ; yeh et al . 2006 and in mexican and other cultures with a strong catholic tradition mccabe et al . 2008 . in african american cultures , having a more external locus of control orientation may be a way to deal with the disadvantages of poverty , unemployment , and racial discrimination threlfall et al . 2013 . in the domain of academic achievement , asians and asian americans report greater internal control than caucasian americans regarding their academic performance . asians and asian americans have been found to exceed caucasian americans in linking their successes and failures to their own efforts rather than to external circumstances e . g . , si et al . 1995 ; stevenson and stigler 1992 . bartal et al . 1980 found that perceived locus of control was positively associated with academic achievement , but more so for students of asian or african origin than for students of european , american , or israeli origin . however , gardner 2007 found that african american students tended to blame poor academic performance on external factors . in sum , previous research has shown cross cultural differences although not always consistently in general locus of control orientation as well as in the domain of academic achievement . cultural values can also influence locus of control in the domain of peer relationships , although little research has been conducted in this area . european american culture is characterized by an emphasis on the personal responsibility of individuals or ''agency'' to enter and to maintain relationships shweder et al . 2006 . children are encouraged to retain their sense of autonomy and freedom , even when interacting with peers rubin et al . 2002 . in contrast , asian and latino cultures emphasize cooperation and interdependence in the establishment and maintenance of peer relationships chen 2000 ; shweder et al . 2006 ; way 2006 . for example , one must be flexible when interacting with others to avoid embarrassing or hurting others' feelings . as a result , these groups' cultural emphasis on interdependence and maintenance of social harmony may lead them to see peers' agentic attitudes and behavior as a threat to positive relationships in the peer group . to put it succinctly , an internal locus of control in peer relationships may conflict with their cultural value of collectivism . given these differing cultural values in peer relationships , internal locus of control should be more congruent with european american children's peer relationships than with those of ethnic minority children .\n",
            "the black white gap in achievement , as measured by performance on standardized tests , has received considerable attention from researchers in the past five years . claude steele's stereotype threat and disidentification mechanism is perhaps the most heralded of the new explanations for residual racial differences that persist after adjustments for social background are performed . analyzing data from the national education longitudinal study , we found qualified support for portions of the disidentification explanation . black students' academic selfevaluations are more weakly associated with their measured academic performances , a difference that could stem from stereotype threat or a belief that the evaluations are racially biased . but this discounting of performance evaluations does not seem to provoke a more complete disidentification with the schooling process or with academic achievement in general . the findings suggest that there is no clear path from being stereotyped to disidentifying , and in conclusion we discuss alternative explanations for why it may be so .\n",
            "this study examines two important and related dimensions of the persisting gender gap in science , technology , engineering , and mathematics stem bachelor degrees first , the life course timing of a stable gender gap in stem orientation , and second , variations in the gender gap across high schools . we build on existing psychological and sociological gender theories to develop a theoretical argument about the development of stem orientations during adolescence and the potential influence of the local high school environment on the formation of stem orientations by females and males . using the national education longitudinal study nels , we then decompose the gender gap in stem bachelor degrees and show that the solidification of the gender gap in stem orientations is largely a process that occurs during the high school years . far from being a fixed attribute of adolescent development , however , we find that the size of the gender gap in stem orientation is quite sensitive to local high school influences ; going to school at a high school that is supportive of a positive orientation by females towards math and science can reduce the gender gap in stem bachelor degrees by 25 or more . when then harvard president lawrence summers pointed at innate differences between men and women as a possible explanation for women's under representation in high level science positions , he sparked an intense public controversy that mirrors a continuing debate in the scientific community . despite the striking reversal of the gender gap in educational attainment buchmann and diprete 2006 and the near gender parity in math performance hyde et al . 2008 , women still pursue science , technology , engineering , and mathematics stem degrees at much lower rates than their male peers do . figure 1 illustrates these trends . it shows , on the one hand , how women have made impressive gains in college attainment compared to men and now clearly outnumber men among college graduates in recent decades . on the other hand , women continue to lag behind in terms of bachelor degrees awarded in the 'quantitative' sciences illustrated in the graph for different stem sub fields . 1 this persistent pattern of gender inequality in college science majors and the implications for later career choices and labor market earnings has been a major concern for scientist and policy makers alike . such differences are not only relevant for the representation of women in high income and prestigious jobs and as such for gender equality in general but also for the supply of qualified labor in science oriented jobs , which is often regarded as a linchpin for the future of the us economy in an increasingly competitive global environment . in this paper , we explore two important and related dimensions of the persisting gender gap in stem degrees . the first dimension is the timing of the emergence of a gender gap in orientation towards stem fields . our analysis of the middle school through college phase of the educational life course reveals that the high school years play a major role in shaping gendered orientations toward science and engineering . the second dimension concerns the impact of the high school environment on the development of these orientations during the decisive high school years . in particular , we use multilevel models to document how the gender gap in stem orientation in twelfth grade varies across high schools , and we estimate the causal effect of the high school curriculum on the gender gap in stem orientation . far from being a fixed attribute of adolescent development , we find that the size of the gender gap in stem orientation is quite sensitive to local high school influences ; going to school in a high school that is supportive of a positive orientation by females towards math and science can reduce the gender gap in stem bachelor degrees by 25 or more . together these two dimensions of timing and local environmental influence extend existing theories and open concrete avenues for policy intervention . 1 exceptions to this trend are the biological , biomedical and life sciences , in which women today outnumber men . table 268 , 299 , 303 , 305 , 312 and 313 we begin by developing a theoretical argument about the development of stem orientations during adolescence , and the potential influence of the local environment on the formation of stem orientations by females and males . while acknowledging the importance of the global environment , we argue that local environments centrally affect the strength and salience of gender stereotypes about math and science . their influence operates mainly through two important processes . first , peers , parents and teachers all important actors in the local environment support and encourage certain career paths for boys and girls while disparaging others . second , the influence of gender stereotypes about stem occupations depends on the level of exposure to stem academic courses and to information about stem fields and stem occupations . both arguments imply that the size of the gender gap in stem bachelor degrees is shaped to a considerable extent by the character of the high school environment . variations in this environment will therefore affect the size of the gender gap in stem orientations . we then turn to an empirical examination of the timing and the local variation across high schools . first , we decompose the gender gap in stem bachelor degrees into various pathways to examine the emergence and solidification of gender differences in the orientation towards science and engineering in the adolescent life course . in particular , we use the national education longitudinal study nels to follow the 1988 cohort of eighth grade students through adolescence and young adulthood , and observe how orientations towards stem fields emerge and change from eighth grade through college . we find that the substantial gender gap in eighth grade orientation is relatively inconsequential for the persisting gender gap in stem degrees at the completion of college . instead , the high school years play a major role in shaping gendered orientations towards science and engineering . second , we examine the role of the high school for this decisive period . in particular , we use multilevel models to document how the gender gap in stem orientation at the end of high school varies across high schools , and we estimate the causal effect of high school curriculum on the gender gap in stem orientation . the results show substantial variation in the gender gap in stem orientation across schools , which supports our argument that the local environment plays a major role in shaping these orientations among boys and girls . the significant effect of high school curriculum on the gender gap in stem orientation provides the beginning of an understanding about the source of the high school effect . our results imply that the gender gap could be considerably reduced if high school environments were reshaped to model those high schools that are most supportive of female interest and competency in science and mathematics .\n",
            "abstract this paper analyses the impact of teacher gender towards students' test results in a blinded math test administered to students in catalonia spain . the data for this analysis are drawn from a sample of secondary school students who participated in an international blind test known as the mathematical kangaroo in 2008 . the estimation considers a two stage procedure since participation on the test leads to the presence of sample selection . results show a correlation between female teacher gender and student results . moreover , students with female teachers have a higher probability of participating in the kangaroo test in this case , the effect being more marked among male students . jel codes i28 , j16 a number of countries , including australia , canada , finland , new zealand , united kingdom and the united states have implemented policy initiatives to increase the number of male teachers in primary education where they have been in a clear minority to improve the results of male pupils , since it is assumed that , among other reasons , they are related to the lower presence of male teachers see a review in klein , 2004 ; carrington et al . , 2007 ; younger and warrington , 2008 ; skelton , 2009 . these policies have been implemented although there is ongoing debate as to whether students' results can be correlated with the gender of their teachers . thus , some studies report that students perform better if they have a same gender teacher , others point out that it is better to have a female teacher , while the third group of studies indicate that there is no gender effect at all . the reasons to support the first type of results relate to the fact that teachers might prefer teaching students of their own gender or that gender stereotypes may influence teacher evaluations of their students . in addition , teachers may act as role models for their students see , among others , carrington and skelton , 2003 ; gray and leith , 2004 ; holmlund and sund , 2008 . in regards to the second kind of results , students assigned to female teachers perform better since female teachers tend to be more supportive , provide a more positive classroom atmosphere , and tend to use a more student oriented style of teaching stressing the importance of motivation see stake and katz , 1982 ; singer , 1996 ; krieg , 2005 ; nelson laird , 2011 . below we summarize some of the existing evidence for primary and secondary education middle and high school . in primary education , the studies reviewed conclude that teacher gender is either irrelevant or that female teachers improve both girls' and boys' performance . thus , in a large scale analysis 8 , 978 eleven year old pupils and 413 teachers in 113 primary schools year 6 in england , during the 1997 98 academic year , carrington et al . 2008 conclude that having a same gender teacher has no impact on student performance measured through tests , either that of males or females , in mathematics , reading and science . moreover , with a sample of students and teachers in 19 primary and secondary schools in australia , lingard et al . 2002 show that teacher gender is not a significant factor in determining positive outcomes for students in literacy and mathematical tests . this result is also found in sokal et al . 2005 in a study with 6 to 8 yearold children in a school in canada regards to reading performance , as well as in driessen 2007 with a large scale sample of dutch primary schools , in relation to language and maths tests' results , which included 5 , 181 grade eight pupils , 251 teachers and 163 schools . krieg 2005 also concludes that having a teacher of the same gender is not relevant . however , the study shows that male and female students in grade three 8 year olds assigned to female teachers obtain higher marks on a standardized test the washington assessment of student learning . controlling for student ability as well as for school and district fixed effects , krieg shows that the students of female teachers are more likely to score well on the maths , reading and writing sections of the test i . e . , they obtain higher results and present a higher probability of passing . in developing countries , chudgar and sankar 2008 consider a sample of grade four and six students ages 9 to 11 , in 300 public schools in india , and conclude that being in a female teacher's classroom is advantageous for language learning but teacher gender has no effect on mathematics learning . in secondary education we find numerous evidence teacher gender is irrelevant regards to students' results ; female teachers increase students' outcomes ; and or students assigned to a same gender teacher have better results . thus , in an analysis of upper secondary education students 16 to 18 year olds in 69 schools in stockholm sweden , holmlund and sund 2008 find no evidence to show that teacher gender improves student outcomes . in this study , for each student and school year , they are able to identify both the student outcome the final overall grade as well as the grades obtained in several individual subjects and the gender of the students and teachers . the authors argue that the gender performance differential in favor of female students is greater in subjects in which the share of female teachers is higher but this effect is not causal , since it is not observed when the analysis controls for teacher turnover and student mobility nor when the assumption of random student teacher matching within a subject holds . the same kind of evidence is provided by ehrenberg et al . 1995 for the usa . using data from the national education longitudinal study of 1988 , which comprises nearly 25 , 000 eighth graders lower secondary education as well as two of each of the student's teachers , they find that a teacher's gender is not correlated with the achievement test scores in mathematics and science of students although , in some cases , teachers' subjective evaluations about their students are . however , dee 2007 shows that that assignment to a same gender teacher significantly improves the achievement of both girls and boys in terms of their test scores in mathematics , science , reading and history . data comes from the national education longitudinal study of 1988 and he controls for student traits , classroom and teacher characteristics as well as including student , class and teacher fixed effects . finally , considering 3 , 446 pupils from 110 public schools in israel , klein 2004 concludes that male and female pupils get higher scores , defined as end of the year grades in subjects related to humanities , such as literature and history , and science mathematics , chemistry and physics with female teachers . thus , the available empirical evidence does not allow an accurate determination of the correlation between teacher's gender and student's academic performance in primary and secondary education . in this context , this paper analyses the impact of teacher gender on student marks , considering student results in a mathematical blind test named kangaroo . the issue is of importance since , as holmlund and sund 2008 point out , the gender gap may have both educational and economic consequences in terms of an efficiency loss whereby students with higher ability might obtain lower marks and face greater difficulties in accessing higher levels of education and furthering their professional careers . in addition , as mentioned before , educational policies have been implemented in a number of countries to regulate this issue with labor consequences for the teachers . three specific aspects of this paper are worth highlighting . first , in this study we include the characteristics of the agents that might have an impact on student results , such as pupils , teachers , and schools . second , we include fixed effects to control for student and school characteristics not observed in the analysis unobservables , as the recent literature recommends dee , 2007 ; hoffman and oreopoulos , 2009 ; holmlund and sund , 2008 . finally , to the best of our knowledge , this is the first analysis conducted in spain or in any similar country in southern europe .\n",
            "today , social network is very important to many people particularly for the youths in their everyday lives , integrating their online and offline experience and becoming one of the primary means of connecting and socially interacting worldwide , whether among individuals , students , business people and even governments . the purpose of this study is to propose a conceptual framework on the relationship between social network usage especially facebook with academic performance . according to hussain 2005 , facebook is the most popular social media worldwide , and its trend indicated that university students used social media for self enjoyment and friendship . however , they preferred to share their study experiences research projects , educational events , information , and developing networks . social networking sites such as facebook , twitter , youtube , skype and blog are widely used in the exchange of different information and in communicating with various cultures worldwide o'reilly , 2007 . in the current 2000s , online social networking sites have gained popularity among college students . facebook is one of the highly popular social media worldwide . facebook provides an opportunity for users particularly college students to create their profiles , make personal information known to other users , and upload and display pictures . moreover , users are able to look at the profiles of other users , gather and make more friends online , and connect with those friends via messages , gifts , and photo tagging kalpidou , dan costin , jessica , 2011 . the increasing connection of college students to these worldwide online communities has constituted an anxiety to both parents and educators to the extent of stimulating research in this area to investigate the phenomenon . some researchers have pointed out that students using social media learn in new ways and for this reason educators should welcome these new developments ito et al . , 2009 ; jenkins , 2006 . park , kee and valenzuela 2009 , found that facebook could negatively affect the overall academic performance of the students . according to khan 2009 , facebook users often experience poor performance academically . moreover , many researchers were focused on various demographic variables such as age and gender on academic performance . the result of these researches found the significance of gender in student's academic achievement for example ; they found a significant difference in gpa grade between male and female students in a class jabor et al . , 2011 . similar with an analysis of data from the national education longitudinal study nels ; lee and burka 1996 , found a large advantage for males on the physical sciences and the modest advantage for females on the life science . however , coley 2001 , studied gender difference within ethnic groups of varying ages , and it revealed more similarities than differences . in conclusion , more research on age and gender differences in academic performance is needed to make conclusive implication of the effect of age , gender , and facebook intensity on the academic performance of university students . this study will investigate whether students' age , gender , and facebook intensity may make a difference in their academic performance .\n",
            "a study investigated the college application behaviors of students from different racial ethnic groups whites , african americans , asian americans , hispanic americans to understand differences in the college search and choice process . data we . re drawn from two large national longitudinal studies , the national education longitudinal study 1988 and the beginning postsecondary studentn longitudinal study . analysis revealed significant group differentes in college application behavior number of colleges applied to , time of submission of application , first choice of institution , and tuition cost . substantial data tables showing analyses are included . asian americans were most likely to follow assumptions underlying traditional college choice models . latino students were the least prepared regarding knowledge about college and least likely to fit traditional college choice models . it is concluded that the findings suggest a need for campuses to evaluate the potential effects of policy decisions that may affect student choice for different applicant populations . implications for institutional research needs are also noted . contains 7 tables , 5 appendixes , and 18 references . mse access and equity have long been central goals of american higher education , as reflections of both egalitarian and pragmatic interests . most often , measures of enrollment and persistence have been used to track overall participation rates and to gauge the success of various groups in securing equal levels of opportunity . there is fairly wide agreement that throughout the 1960s and '70s , minority men and women of all ethnic groups achieved ever increasing levels of representation at american two and four year institutions , and that disparities between socioeconomic , racial ethnic , and gender groups decreased alexander , pallas holupka , 1987 ; orfield , 1990 ; paul , 1990 . there is less agreement regarding the cease at these gains some researchers credit the vast increases in public and private student aid expenditures during this period astin , 1982 cited in nora and horvath , 1989 , and others claim no evidence exists to suggest that financial aid improves access hanson , 1980 cited in nora and horvath , 1989 ; zemsky , 1988 cited in orf eld , 1990 . there is also deep disagreement over whether racial and ethnic groups and those of lower socioeconomic status have gained or lost ground since the retrenchment of the 1980s . alexander , et al . 1987 found that for a cohort of 1980 high school seniors , within individual socio economic status ses levels , minority youth consistently showed higher participation rates than white students , yet low ses was nonetheless strongly associated with less participation . paul 1990 cites the failure of some researchers to take into account the increasing number of minority high school graduates when they claim advances in higher education representation of minorities . instead , she contends that when minority enrollment in higher education is considered as a percentage of minority high school graduates , both african americans and latinos lost considerable ground between the mid 1970s and the mid 1980s . at the same time , however , critics of affirmative action in admissions suggest that such programs and policies are either no longer necessary or that they provide an unfair advantage to choice among racial ethnic groups p . 2 racial ethnic groups over white applicants to college . these differing points of view suggest that it is time to reexamine the progress and barriers to progress in terms of access to higher education for different racial ethnic groups . yet united states higher education is not a monolith of similar institutions evenly dispersed throughout the land . there is a great variety of institution types , from large , prestigious research institutions producing bachelor's degrees through doctorates , to small two year community colleges offering associates degrees and vocational training . cost , availability of financial support , and entrance requirements all differ among institutions , affecting access in a number of ways pascarella terenzini , 1991 . likewise , degrees confer individual benefits of economic opportunity and prestige and increase human resources to society in amounts which differ from one institution to the next . therefore , it is important when judging equality of access to higher education and return on investment to consider the distribution of students among institutions of different types . using the theoretical model established by hossler and associates 1984 , we investigated the college application behaviors of various racial ethnic groups in order to understand differences in the college search and choice processes . hossler and gallagher 1987 posit three phases of the college choice process the predisposition , search , and choice phase when students' backgrounds , attributes , activities , and institutional characteristics interact to influence the decision making process . the first stage is the predisposition phase when family background , ability , and students' early preferences predispose students to aspire to specific degree attainments and seek information about colleges . during the next phase , both the student and institutions engage in search activities . while students seek information about and make decisions concerning the types of institutions they will consider applying to , institutions typically also provide information to students they are interested in recruiting . in the third and last phase of the college choice process , students narrow the range of schools they are considering to a choice set composed of two or more schools , and colleges engage in courtship activities ranging from invitations for campus visits to the offering of financial aid packages . choice among racial' fanic groups p . 3 , because college pricing , financial aid , and other factors are critical to understanding this process , we set out to explore continuing differences in groups both at the senior year of high school and once in college . student demographics , preferences , academic ability , and income levels were taken into account in our analyses . erdman 1983 examined factors that influenced high school seniors' applications to specific colleges and found traditional age students rank the following factors from . most influential to least academic programs , reputation , location , size , parent recommendation , counselor recommendation , cost , and alumni contact . erdman concludes that the reputation of a particular institution in the mind of students , the location of that institution , and its size are powerful forces in the selection process , outweighing other factors examined , including cost p . 6 . in contrast , other work on nontraditional students consisting mainly of adults students suggest that these students are more sensitive to tuition cost than recent high school graduates bishop van dyk , 1977 . moreover , recent studies have shown that the typical models for college choice are less effective in predicting nontraditional or delayed entry students' search and choice processes than they are of traditional aged students bers and smith , 1987 ; hurtado , kurotsuchi , and sharp , 1996 . we examine these issues across racial ethnic groups in order to determine key differences in college choice and access .\n",
            "in this paper , we develop a new direct measure of state anti smoking sentiment and merge it with micro data on youth smoking in 1992 and 2000 . the empirical results from the cross sectional models show two consistent patterns after controlling for differences in state anti smoking sentiment , the price of cigarettes has a weak and statistically insignificant influence on smoking participation ; and state anti smoking sentiment appears to be a potentially important influence on youth smoking participation . the cross sectional results are corroborated by results from discrete time hazard models of smoking initiation that include state fixed effects . however , there is evidence of price responsiveness in the conditional cigarette demand by youth and young adult smokers . despite mainly encouraging trends , public health policy makers continue to be concerned about youth smoking . although the prevalence of smoking among high school seniors in the u . s . has fallen from its 1997 peak of 36 . 5 percent to about 23 percent , it still has a ways to go to reach the healthy people 2010 national health objective of 16 percent johnston 2005 , usdhss 2004 . and the most recent data suggest the rate of decline in smoking among u . s . youth may have stopped or even reversed johnston 2005 , cdc 2006 . the most recent trend data led the president of the american legacy foundation the anti smoking organization established by the 1998 legal settlement with the tobacco industry to conclude that today's news from the cdc is a warning sign act now to support tobacco prevention work across our nation , and we can drive down youth smoking rates . if we wait , the percentage of youth smokers will continue to rise . . . . american legacy foundation 2006 . higher cigarette prices are widely seen as one of the most effective ways to reduce youth smoking usdhss 2000 . in fact , another healthy people 2010 objective is for combined federal and state excise taxes to average 2 . 00 per pack . as of november 1 , 2005 , combined federal and state excise taxes reached or almost reached 2 . 00 per pack in only 13 states orzechowski and walker 2005 . some research calls into doubt the policy prescription that higher prices are an effective way to reduce youth smoking wasserman et al . 1991 , douglas 1994 , douglas and hariharan 1998 , decicca , kenkel and mathios 2001 , 2002 . most research that supports the policy prescription uses variation across states in cigarette taxes as a natural experiment to identify the price responsiveness of youth smoking e . g . , lewit , coate , and grossman 1981 , chaloupka and grossman 1996 , harris and chan 1999 , ross and chaloupka 2003 . however , tax rates are not randomly set , but result from the political process which reflects public sentiment towards smoking . warner 1982 , p . 483 concludes that the fluctuations in new cigarette tax activity do not appear to have occurred randomly . to the contrary , they correspond closely to the evolution of public concerns about the link between cigarette smoking and illness . . . . as evidence supporting this hypothesis , he points out that from 1921 1952 tobacco producing states and other states taxed cigarettes similarly ; but as public awareness of the smoking andhealth issue grew , other states were much more likely to increase cigarette taxes than were the tobacco states . similarly , based on their econometric analysis of 1946 1989 data , hunter and nelson 1992 conclude that public policy concerns . . . contribute positively to the level of tobacco excise taxation in a state . these findings suggest that in the political process taxes are seen as an anti smoking policy and hence probably reflect anti smoking sentiment . if antismoking sentiment is itself an important determinant of youth smoking , failing to control for differences in anti smoking sentiment across states will yield biased estimates of the priceresponsiveness of youth smoking . in this paper , we extend previous work decicca , kenkel , and mathios 2002 to explore in much greater depth the role of state anti smoking sentiment in empirical models of youth and young adult smoking . in particular , in section 2 we analyze data from the tobacco use supplements of the current population survey tus cps to develop a new direct measure of state anti smoking sentiment during the 1990s . section 3 describes the national education longitudinal study nels , a nationally representative micro data set that contains youth smoking information . in section 4 we use the 1992 and 2000 waves of nels to re examine the impact of prices in models of youth smoking that include the new measure of anti smoking sentiment . section 5 explores whether our main results are robust to several sensitivity checks . in section 6 we explore other approaches to control for anti smoking sentiment in cross sectional data , and we also estimate hazard models of smoking initiation over time that include state fixed effects . section 7 concludes .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3287QTBVxEJ"
      },
      "source": [
        "import torch.nn as nn\n",
        "num_words = len(word2i) + 2\n",
        "weights = torch.Tensor(np.zeros((num_words, 300)))\n",
        "E = nn.Embedding(num_words, 300)\n",
        "for i in range(0,num_words):\n",
        "  if i in i2word.keys():\n",
        "    if i2word[i] in GloVe.keys():\n",
        "      weights[i] = torch.Tensor(GloVe[i2word[i]])\n",
        "    else:\n",
        "      \n",
        "      weights[i] = E.weight[i]\n",
        "  else:\n",
        "    weights[i] = E.weight[i]\n",
        "E.weight.data = weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUEsPrB0WlKA"
      },
      "source": [
        "gold_seq = []\n",
        "count = 0\n",
        "for i in range(len(data)):\n",
        "  words = paragraphs[i]\n",
        "  label = data.loc[i][4]\n",
        "  t = label.lower()\n",
        "  t = re.findall(r\"[\\w']+|[.,!?;]\", t)\n",
        "  seq = np.zeros(len(words))\n",
        "  for j in range(len(words)):\n",
        "    if words[j:j+len(t)] == t:\n",
        "      index = (j, j+len(t))\n",
        "      break\n",
        "    else:\n",
        "      index = False\n",
        "  seq = np.zeros(len(words))\n",
        "  if index:\n",
        "    for j in range(index[0], index[1]):\n",
        "      seq[j] = 1\n",
        "  if all(v == 0 for v in seq):\n",
        "    count+= 1\n",
        "    for j in range(len(words)):\n",
        "      for z in t:\n",
        "        if words[j] in z:\n",
        "          seq[j] = 1\n",
        "          t.remove(z)\n",
        "          break   \n",
        "  gold_seq.append(seq)\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQEH7zrm9Az"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND_AbL7Ic7uH"
      },
      "source": [
        "copy, test_x, gold_seq, test_y = train_test_split(copy, gold_seq, test_size=0.3, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPe6RNJJmouJ"
      },
      "source": [
        "paragraphs = []\n",
        "for i in copy:\n",
        "  paragraphs.append(i[0])\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFe4kC2UZ7dx",
        "outputId": "12fe62f4-d7d7-46c1-f61e-ca651d7f8749"
      },
      "source": [
        "\n",
        "class BasicLSTMtagger(nn.Module):\n",
        "    def __init__(self, DIM_EMB=10, DIM_HID=10):\n",
        "        super(BasicLSTMtagger, self).__init__()\n",
        "\n",
        "        self.E = nn.Embedding(num_words, 300)\n",
        "        self.init_glove(GloVe)\n",
        "        self.L = nn.LSTM(DIM_EMB, DIM_HID, bidirectional=True, batch_first=True)\n",
        "        self.W = nn.Linear(DIM_HID*2, 2)\n",
        "        self.logSoftmax = nn.LogSoftmax(dim=2)\n",
        "        #TODO: initialize parameters - embedding layer, nn.LSTM, nn.Linear and nn.LogSoftmax\n",
        "\n",
        "    def forward(self, X, train=False):\n",
        "        #TODO: Implement the forward computation.\n",
        "        X = X.cuda()\n",
        "        \n",
        "        x = self.E(X)\n",
        "\n",
        "        x, hidden = self.L(x)\n",
        "\n",
        "        x = self.W(x)\n",
        "        return self.logSoftmax(x)  \n",
        "\n",
        "    def init_glove(self, GloVe):\n",
        "      weights = torch.Tensor(np.zeros((num_words, 300)))\n",
        "      for i in range(0,num_words):\n",
        "        if i in i2word.keys():\n",
        "          if i2word[i] in GloVe.keys():\n",
        "            weights[i] = torch.Tensor(GloVe[i2word[i]])\n",
        "          else:\n",
        "            \n",
        "            weights[i] = self.E.weight[i]\n",
        "        else:\n",
        "          weights[i] = self.E.weight[i]\n",
        "      self.E.weight.data = weights\n",
        "    def inference(self, sentences):\n",
        "\n",
        "      pred = self.forward(sentences).argmax(dim=2)\n",
        "      return pred\n",
        "\n",
        "\n",
        "#The following code will initialize a model and test that your forward computation runs without errors.\n",
        "lstm   = BasicLSTMtagger(DIM_HID=7, DIM_EMB=300)\n",
        "lstm = lstm.cuda()\n",
        "print(X[0])\n",
        "lstm_output = lstm.forward(torch.LongTensor(X[0]).reshape((1,len(X[0])))).cuda()\n",
        "\n",
        "\n",
        "#Check the shape of the lstm_output and one-hot label tensors.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22304, 70801, 10987, 46533, 49881, 8148, 28775, 46650, 22371, 34336, 48116, 115, 71540, 50692, 8676, 1450, 27121, 66660, 5725, 71325, 20336, 29803, 41077, 18785, 33494, 40908, 6808, 66660, 17576, 36377, 46344, 48878, 34336, 60169, 61498, 46533, 59185, 115, 34336, 60169, 60864, 60502, 12370, 17112, 61498, 33494, 36377, 30349, 66645, 29803, 12217, 29803, 41769, 29803, 69392, 6061, 63323, 115, 33001, 67624, 39832, 46344, 34336, 60169, 32165, 33494, 3870, 65359, 29084, 35443, 22371, 46344, 35547, 46207, 20583, 64832, 1450, 29084, 30349, 6061, 27078, 69392, 35547, 12034, 51590, 42516, 17779, 11402, 36810, 115, 26607, 61234, 64832, 54159, 18669, 60169, 28775, 34969, 64832, 53527, 6061, 27078, 29803, 37326, 56813, 37033, 22120, 30349, 6061, 27078, 115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "3648d55802de4d9697db4018695ce068",
            "82c89513b63f45c2bb646dd3532d8cd0",
            "d56c7314534e40eaaaaeb38920b331a3",
            "e8d90bb848b84c58a5002fd9f623aaa2",
            "ba078f36bbc94285909501abc0b1d8b6",
            "6c8c3550f24c4b189474706b2096ad5b",
            "c21c7a2719dc43d98ab7ef517005537b",
            "2a046fc3f05e420bb8f14e064b03a970",
            "64be3b0cebde4ab89c8eac7a110dc167",
            "ed5c666268bd4430bddcec55f171f879",
            "7e0910ee6cc94d47893adef3f346a285",
            "082b6cf5ebd1456bb18807931374022a",
            "03a5c82916df40bfb0cb83a1a16c14fc",
            "1a7c163c7a224cc9a8a00c25ce884a25",
            "b87042d2b06040d58811c084e6806207",
            "f8acfaefcdcd44b0be02b722dd183385",
            "b4593924c459424b8305bc6ba4c8567d",
            "23122ff6ae644bf196c832c2b91685fc",
            "f6763232743f4fb2a627b9c40e6a5bde",
            "cc23cdb0ffcd4be2b15efbbf8d82a81c",
            "e5ca59b7393741a19a1596d001a4649b",
            "4442da74138142ebbbc90d6614dd8370",
            "284e8aba73e04d6c9390022b2f232f68",
            "2768b728fcca472e956700ff021b5848",
            "5ba5a6e4f10a441b9e531588b19fac96",
            "840e638c798c4bb59aabcd7975c2b377",
            "7f558752a1ee4a568543b59fa5b4828a",
            "093315b3ee954fefb847b248f9e4a571",
            "236dad0222684cb4a9959fe144dcb1eb",
            "508a52d701fc4e5eba94148bf683ead5",
            "3818308e7f2f442991c2ff3fd46aaee3",
            "c82c752d629a470fafbd59ac08f5fb64",
            "467a7652376147d6b131f9f028243349",
            "f2567213608344ef93fad6dda286baa4",
            "4233f2fd104a4c8eaa2bf0e1436bf13b",
            "08d4b4f5edd54692b29f9115e26e01f5",
            "a02bb26f3b8a49c290c3b4de4fc0cfd9",
            "bc5dd56960784d07914924119412c66c",
            "40dbd52d81fe46a994ab7997cd1e8c1a",
            "5042470d3c3b494bb771550b38b83cf5",
            "f0628d11e578420a9945e6a5390d241c",
            "0e7b02e0aeed417db626ea39af54afd9",
            "1c7fd661a36d4a6eaf978c05af9b749f",
            "98e1b8690b7045409255cc0b9813e557",
            "0b67ef296d594fef88310a3995643980",
            "7ee9620d4eb84abbbe5a6c9f4399155a",
            "a4546b1ef9834766bdfbc3682641247e",
            "df536805435e47babb9df58add8117d5",
            "9694f2b93df04f0a8437b2ccdf47fb01",
            "7963ab83642c4c7a8ad0d26d32b557e8",
            "5b32fba956bd448ca48670db26efc31c",
            "cf73d0b840d24d1c80b86f0444ec6cb9",
            "2bc85f55c9b04cb095a9e690aca237f6",
            "ac0da1e6543f48e385fba73cba6acabf",
            "7869df865a4843e5b746d2b7ca89e5f9",
            "b39a19f9384d49b990a8bdb91a7b2f84"
          ]
        },
        "id": "22KK1EkJc6pf",
        "outputId": "0b161afc-162b-4a63-d8d6-859da6668aa5"
      },
      "source": [
        "#Training\n",
        "\n",
        "from random import sample\n",
        "import tqdm\n",
        "import os\n",
        "import subprocess\n",
        "import random\n",
        "import torch.optim as optim\n",
        "def shuffle_sentences(sentences, tags):\n",
        "  shuffled_sentences = []\n",
        "  shuffled_tags      = []\n",
        "  indices = list(range(len(sentences)))\n",
        "  random.shuffle(indices)\n",
        "  for i in indices:\n",
        "    #print(len(sentences[i]), len(tags[i]))\n",
        "    shuffled_sentences.append(sentences[i])\n",
        "    shuffled_tags.append(tags[i])\n",
        "  return (shuffled_sentences, shuffled_tags)\n",
        "nEpochs = 7\n",
        "\n",
        "def train_basic_lstm(sentences, tags, lstm):\n",
        "  optimizer = optim.Adadelta(lstm.parameters(), lr=0.1)\n",
        "  #TODO: initialize optimizer\n",
        "  \n",
        "  batchSize = 10\n",
        "  for epoch in range(nEpochs):\n",
        "    totalLoss = 0.0\n",
        "    l = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=10).cuda()\n",
        "    (shuffled_sentences, shuffled_tags) = shuffle_sentences(sentences, tags)\n",
        "    X       = sentences2indices(shuffled_sentences, word2i, train=True)\n",
        "    for batch in tqdm.notebook.tqdm(range(0, len(sentences) - batchSize, batchSize), leave=False):\n",
        "      lstm.zero_grad()\n",
        "      #TODO: Impelement gradient update.\n",
        "      \n",
        "      input = prepare_input(X[batch:batch+batchSize-1])[0]\n",
        "      Y = []\n",
        "      for j in range(batch, batch+batchSize-1):\n",
        "        Y.append(torch.LongTensor(shuffled_tags[j]))\n",
        "      Y_padded = torch.nn.utils.rnn.pad_sequence(Y, batch_first=True,padding_value=10)\n",
        "      lstm_output = lstm.forward(input)\n",
        "      loss = l(torch.flatten(lstm_output, start_dim=0, end_dim = 1), torch.flatten(Y_padded).cuda())\n",
        "      loss.backward()\n",
        "      totalLoss += loss\n",
        "      \n",
        "      optimizer.step()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "    \n",
        "lstm = BasicLSTMtagger(DIM_HID=500, DIM_EMB=300).cuda()\n",
        "train_basic_lstm(paragraphs, gold_seq, lstm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3648d55802de4d9697db4018695ce068",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 0 = 61518.8125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64be3b0cebde4ab89c8eac7a110dc167",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 1 = 14917.763671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4593924c459424b8305bc6ba4c8567d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 2 = 12598.5439453125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba5a6e4f10a441b9e531588b19fac96",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 3 = 11577.103515625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "467a7652376147d6b131f9f028243349",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 4 = 10791.9296875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0628d11e578420a9945e6a5390d241c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 5 = 9637.771484375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9694f2b93df04f0a8437b2ccdf47fb01",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=290.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss on epoch 6 = 8817.748046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyOP07Bwyskq"
      },
      "source": [
        "torch.save(lstm.state_dict(), \"lstm.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a8jNNsOBk8gQ",
        "outputId": "e1262492-7f86-4752-e72b-76f648bc1032"
      },
      "source": [
        "for j in range(0,len(data), 50):\n",
        "  z = torch.LongTensor(X[j])\n",
        "\n",
        "  result = lstm.inference(z.reshape((1,len(X[j])))).tolist()\n",
        "  result = np.array(result[0])\n",
        "  index = np.where(result == 1)[0]\n",
        "  print(index)\n",
        "  for i in index:\n",
        "    print(i2word[X[j][i]])\n",
        "  print(data.loc[j])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16 17 18 19]\n",
            "national\n",
            "education\n",
            "longitudinal\n",
            "study\n",
            "Unnamed: 0                                                       4\n",
            "Id                            c754dec7-c5a3-4337-9892-c02158475064\n",
            "pub_title        Parental Effort, School Resources, and Student...\n",
            "dataset_title                National Education Longitudinal Study\n",
            "dataset_label                National Education Longitudinal Study\n",
            "cleaned_label                national education longitudinal study\n",
            "text              This article investigates an important factor...\n",
            "mask                                                          True\n",
            "Name: 0, dtype: object\n",
            "[ 817  905  906 1090 1091]\n",
            "slosh\n",
            "slosh\n",
            "model\n",
            "slosh\n",
            "model\n",
            "Unnamed: 0                                                     347\n",
            "Id                            07ce992d-e2f3-49cb-bedb-49616a19405f\n",
            "pub_title        U.S. IOOS coastal and ocean modeling testbed: ...\n",
            "dataset_title       Sea, Lake, and Overland Surges from Hurricanes\n",
            "dataset_label                                          SLOSH model\n",
            "cleaned_label                                          slosh model\n",
            "text              [1] A Gulf of Mexico performance evaluation a...\n",
            "mask                                                          True\n",
            "Name: 50, dtype: object\n",
            "[202 203 204 205 206 207]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "blsa\n",
            "Unnamed: 0                                                     556\n",
            "Id                            12b7418c-3114-4ffa-92bf-3468187ee885\n",
            "pub_title        Stride variability measures derived from wrist...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "cleaned_label          baltimore longitudinal study of aging blsa \n",
            "text              Many epidemiological and clinical studies use...\n",
            "mask                                                          True\n",
            "Name: 100, dtype: object\n",
            "[401 402 403 404 405 406]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "blsa\n",
            "Unnamed: 0                                                     740\n",
            "Id                            ae17c19b-811f-4a21-b39a-5d580809a4b1\n",
            "pub_title        Absence of Age-Related Increase in Central Art...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "cleaned_label          baltimore longitudinal study of aging blsa \n",
            "text              Abstract-Increased arterial stiffness is thou...\n",
            "mask                                                          True\n",
            "Name: 150, dtype: object\n",
            "[122 123 124 125]\n",
            "agricultural\n",
            "resource\n",
            "management\n",
            "survey\n",
            "Unnamed: 0                                                    1211\n",
            "Id                            c5b3794e-3577-44a0-ba69-0787765746be\n",
            "pub_title        A Comparison of Data Collected through Farm Ma...\n",
            "dataset_title              Agricultural Resource Management Survey\n",
            "dataset_label              Agricultural Resource Management Survey\n",
            "cleaned_label              agricultural resource management survey\n",
            "text              Purpose: This study compares the characterist...\n",
            "mask                                                          True\n",
            "Name: 200, dtype: object\n",
            "[726 727 728 729 730 731 732 733]\n",
            "international\n",
            "best\n",
            "track\n",
            "archive\n",
            "for\n",
            "climate\n",
            "stewardship\n",
            "ibtracs\n",
            "Unnamed: 0                                                    1407\n",
            "Id                            642e187d-44dc-4ea7-92dd-64f3d73c206d\n",
            "pub_title        An Analysis of the Effect of Global Warming on...\n",
            "dataset_title    International Best Track Archive for Climate S...\n",
            "dataset_label    International Best Track Archive for Climate S...\n",
            "cleaned_label    international best track archive for climate s...\n",
            "text              A statistical intensity adjustment is utilize...\n",
            "mask                                                          True\n",
            "Name: 250, dtype: object\n",
            "[67 68 69 70 71]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    1727\n",
            "Id                            bcbc988a-086f-47ce-aa0c-d468b0f318ab\n",
            "pub_title        Glucose and Insulin Measurements From the Oral...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              Background. Diabetes is associated with decre...\n",
            "mask                                                          True\n",
            "Name: 300, dtype: object\n",
            "[141 142 143 144 145]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    1842\n",
            "Id                            afd0a2dc-4cc3-4578-91b6-41a9e05f28fd\n",
            "pub_title        Association of periodontitis and metabolic syn...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              Background and aims: Metabolic syndrome (MetS...\n",
            "mask                                                          True\n",
            "Name: 350, dtype: object\n",
            "[58 59 60 61 62]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    2012\n",
            "Id                            91b42587-e2d7-4cdf-853d-0406423fcf00\n",
            "pub_title        Association Between Non-Iron-Deficient Anemia ...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              Background: Anemia is associated with poorer ...\n",
            "mask                                                          True\n",
            "Name: 400, dtype: object\n",
            "[650 651 652 653 654]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    2218\n",
            "Id                            70a3b684-ddfb-4ca1-b5ba-27ca42218c7c\n",
            "pub_title        Age and gender affect ventricular-vascular cou...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              The goal of this study was to examine the age...\n",
            "mask                                                          True\n",
            "Name: 450, dtype: object\n",
            "[75 76 77 78 79]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    2387\n",
            "Id                            f36f2051-878f-4336-ad06-548d7f7657fa\n",
            "pub_title        Basal body temperature as a biomarker of healt...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              Abstract Scattered evidence indicates that a ...\n",
            "mask                                                          True\n",
            "Name: 500, dtype: object\n",
            "[83 84 85 86 87]\n",
            "baltimore\n",
            "longitudinal\n",
            "study\n",
            "of\n",
            "aging\n",
            "Unnamed: 0                                                    2536\n",
            "Id                            e80d9f54-df77-4e2a-9492-6edfb6d3e280\n",
            "pub_title        Impact of age on the cardiovascular response t...\n",
            "dataset_title         Baltimore Longitudinal Study of Aging (BLSA)\n",
            "dataset_label                Baltimore Longitudinal Study of Aging\n",
            "cleaned_label                baltimore longitudinal study of aging\n",
            "text              Impact of age on the cardiovascular response ...\n",
            "mask                                                          True\n",
            "Name: 550, dtype: object\n",
            "[41 42 43 44]\n",
            "early\n",
            "childhood\n",
            "longitudinal\n",
            "study\n",
            "Unnamed: 0                                                    2824\n",
            "Id                            bc1ad101-6fc7-415a-b8c4-66ec47716016\n",
            "pub_title        Etiological Subgroups of Small-for-Gestational...\n",
            "dataset_title                   Early Childhood Longitudinal Study\n",
            "dataset_label                   Early Childhood Longitudinal Study\n",
            "cleaned_label                   early childhood longitudinal study\n",
            "text              Abstract\\nIt remains unclear why substantial ...\n",
            "mask                                                          True\n",
            "Name: 600, dtype: object\n",
            "[39 40 41 42]\n",
            "early\n",
            "childhood\n",
            "longitudinal\n",
            "study\n",
            "Unnamed: 0                                                    2967\n",
            "Id                            9fc40e2a-0d43-4a14-a75e-02326be7be10\n",
            "pub_title        Associations between maternal prepregnancy bod...\n",
            "dataset_title                   Early Childhood Longitudinal Study\n",
            "dataset_label                   Early Childhood Longitudinal Study\n",
            "cleaned_label                   early childhood longitudinal study\n",
            "text              OBJECTIVE: Both underweight and obese mothers...\n",
            "mask                                                          True\n",
            "Name: 650, dtype: object\n",
            "[43 44 45 46]\n",
            "early\n",
            "childhood\n",
            "longitudinal\n",
            "study\n",
            "Unnamed: 0                                                    3118\n",
            "Id                            ec5170c4-d184-4ab5-bbde-7114eebf5799\n",
            "pub_title        Concurrent Trajectories of Female Drinking and...\n",
            "dataset_title                   Early Childhood Longitudinal Study\n",
            "dataset_label                   Early Childhood Longitudinal Study\n",
            "cleaned_label                   early childhood longitudinal study\n",
            "text              Abstract The purpose of this longitudinal stu...\n",
            "mask                                                          True\n",
            "Name: 700, dtype: object\n",
            "[66 67 68 69]\n",
            "early\n",
            "childhood\n",
            "longitudinal\n",
            "study\n",
            "Unnamed: 0                                                    3316\n",
            "Id                            19093f76-a34f-410f-9c2e-a3dbb53ddee6\n",
            "pub_title        Family Complexity, Siblings, and Childrenâ€™s Ag...\n",
            "dataset_title                   Early Childhood Longitudinal Study\n",
            "dataset_label                   Early Childhood Longitudinal Study\n",
            "cleaned_label                   early childhood longitudinal study\n",
            "text              Abstract As family structure in the United St...\n",
            "mask                                                          True\n",
            "Name: 750, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    3458\n",
            "Id                            9e3af1ac-ae73-4145-a8d5-fecc0263b854\n",
            "pub_title        Common folate gene variant, MTHFR C677T, is as...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              A commonly carried C677T polymorphism in a fo...\n",
            "mask                                                          True\n",
            "Name: 800, dtype: object\n",
            "[843]\n",
            "adni\n",
            "Unnamed: 0                                                    3517\n",
            "Id                            88d12a7d-03ed-44fd-adc4-a060cbbecb24\n",
            "pub_title        Identifying amyloid pathologyâ€“related cerebros...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Introduction: The dynamic range of cerebrospi...\n",
            "mask                                                          True\n",
            "Name: 850, dtype: object\n",
            "[80]\n",
            "adni\n",
            "Unnamed: 0                                                    3584\n",
            "Id                            30c0f4eb-e24a-434e-baca-27306565cef6\n",
            "pub_title        Effect of EPHA1 Genetic Variation on Cerebrosp...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Ephrin type-A receptor 1 (EPHA1) (11771145) w...\n",
            "mask                                                          True\n",
            "Name: 900, dtype: object\n",
            "[43]\n",
            "adni\n",
            "Unnamed: 0                                                    3657\n",
            "Id                            22382206-5c16-47f6-9d61-e1bdabb24248\n",
            "pub_title        Treatment With Cholinesterase Inhibitors and M...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              To assess the clinical characteristics and co...\n",
            "mask                                                          True\n",
            "Name: 950, dtype: object\n",
            "[5]\n",
            "adni\n",
            "Unnamed: 0                                                    3742\n",
            "Id                            7060dd90-b569-41c0-ad0f-c1cc9ac0e4de\n",
            "pub_title        Does MRI scan acceleration affect power to tra...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              The Alzheimer's Disease Neuroimaging Initiati...\n",
            "mask                                                          True\n",
            "Name: 1000, dtype: object\n",
            "[640]\n",
            "adni\n",
            "Unnamed: 0                                                    3811\n",
            "Id                            ee2c450b-ff76-4356-be60-f09b870e815a\n",
            "pub_title        Prediction of Conversion from Mild Cognitive I...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Prediction of disease progress is of great im...\n",
            "mask                                                          True\n",
            "Name: 1050, dtype: object\n",
            "[93]\n",
            "adni\n",
            "Unnamed: 0                                                    3879\n",
            "Id                            052d8f0d-be4c-425e-93a8-ea6a494ed9e3\n",
            "pub_title        Benchmarking machine learning models for late-...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Background: Late-Onset Alzheimer's Disease (L...\n",
            "mask                                                          True\n",
            "Name: 1100, dtype: object\n",
            "[191]\n",
            "adni\n",
            "Unnamed: 0                                                    3944\n",
            "Id                            c8cb7cbe-28f8-4dac-abdd-9411c00735dc\n",
            "pub_title        TREML2 Mutation Mediate Alzheimerâ€™s Disease Ri...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              A coding missense mutation (rs3747742) in tri...\n",
            "mask                                                          True\n",
            "Name: 1150, dtype: object\n",
            "[1131]\n",
            "adni\n",
            "Unnamed: 0                                                    4020\n",
            "Id                            2729433a-91b0-4198-ad89-851f1e4273f4\n",
            "pub_title        A genome-wide scan for common variants affecti...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Age-related cognitive decline is likely promo...\n",
            "mask                                                          True\n",
            "Name: 1200, dtype: object\n",
            "[264]\n",
            "adni\n",
            "Unnamed: 0                                                    4108\n",
            "Id                            75b35fd6-a326-4cf6-8b8d-f3c64fa45eb5\n",
            "pub_title        Multi-Modal MultiTask Learning for Joint Predi...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Many machine learning and pattern classificat...\n",
            "mask                                                          True\n",
            "Name: 1250, dtype: object\n",
            "[84]\n",
            "adni\n",
            "Unnamed: 0                                                    4200\n",
            "Id                            d29a0494-7270-45db-b2ad-edfd79df96b2\n",
            "pub_title        Measurement of Longitudinal -Amyloid Change wi...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              The accurate measurement of Î²-amyloid (AÎ²) ch...\n",
            "mask                                                          True\n",
            "Name: 1300, dtype: object\n",
            "[349 350]\n",
            "initiative\n",
            "adni\n",
            "Unnamed: 0                                                    4284\n",
            "Id                            3ac7b472-8357-44e1-a87d-8d8fe8af2115\n",
            "pub_title        A multivariate metabolic imaging marker for be...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Introduction: The heterogeneity of behavioral...\n",
            "mask                                                          True\n",
            "Name: 1350, dtype: object\n",
            "[  38 1959]\n",
            "adni\n",
            "adni\n",
            "Unnamed: 0                                                    4368\n",
            "Id                            0efe99c0-9b51-4060-986e-d62d3fc3eb9e\n",
            "pub_title        Testing for association with multiple traits i...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              There is an increasing need to develop and ap...\n",
            "mask                                                          True\n",
            "Name: 1400, dtype: object\n",
            "[106]\n",
            "adni\n",
            "Unnamed: 0                                                    4460\n",
            "Id                            600585a2-708f-4231-85f9-8284b524f441\n",
            "pub_title        Cortical thickness and semantic fluency in Alz...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              The hallmark of Alzheimer's disease (AD) is d...\n",
            "mask                                                          True\n",
            "Name: 1450, dtype: object\n",
            "[615]\n",
            "adni\n",
            "Unnamed: 0                                                    4544\n",
            "Id                            da859d6b-2c18-4f88-a4bd-8364d5b04b9e\n",
            "pub_title        A Longitudinal Imaging Genetics Study of Neuro...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Background-Neuroanatomical asymmetries have r...\n",
            "mask                                                          True\n",
            "Name: 1500, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    4650\n",
            "Id                            b5582deb-3e3c-4977-b206-07b2d5399b11\n",
            "pub_title        Network-Guided Sparse Learning for Predicting ...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Alzheimer's disease (AD) is characterized by ...\n",
            "mask                                                          True\n",
            "Name: 1550, dtype: object\n",
            "[44]\n",
            "adni\n",
            "Unnamed: 0                                                    4761\n",
            "Id                            44846ee2-7530-4370-b8f1-917829d81888\n",
            "pub_title            A Bayesian Spatial Model for Imaging Genetics\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              In this paper we develop a Bayesian bivariate...\n",
            "mask                                                          True\n",
            "Name: 1600, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    4887\n",
            "Id                            779388a3-3523-42dc-8767-9a055215151b\n",
            "pub_title        Structural Similarity based Anatomical and Fun...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Abstract. Multimodal medical image fusion hel...\n",
            "mask                                                          True\n",
            "Name: 1650, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    5011\n",
            "Id                            a2914670-0692-497c-a238-3bb064b80269\n",
            "pub_title        Combining regional metrics for disease-related...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              In this paper, we present a new metric combin...\n",
            "mask                                                          True\n",
            "Name: 1700, dtype: object\n",
            "[130]\n",
            "adni\n",
            "Unnamed: 0                                                    5168\n",
            "Id                            3e7c19cd-e15d-4aea-b64f-31f5bb875c7b\n",
            "pub_title        Simultaneous segmentation and grading of anato...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              In this paper, we propose an innovative appro...\n",
            "mask                                                          True\n",
            "Name: 1750, dtype: object\n",
            "[235]\n",
            "adni\n",
            "Unnamed: 0                                                    5300\n",
            "Id                            5ff6a7a6-ae78-49eb-8a4c-caaaa9bb605a\n",
            "pub_title        A learning-based wrapper method to correct sys...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              We propose a simple but generally applicable ...\n",
            "mask                                                          True\n",
            "Name: 1800, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    5444\n",
            "Id                            9faf1abd-6474-4d8f-b4d3-af5fae754542\n",
            "pub_title        Can measuring hippocampal atrophy with a fully...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              To quantify the \"segmentation noise\" of sever...\n",
            "mask                                                          True\n",
            "Name: 1850, dtype: object\n",
            "[499]\n",
            "adni\n",
            "Unnamed: 0                                                    5645\n",
            "Id                            5e5ce4c3-4a7d-4dff-8a85-2ac3ee6a7cdd\n",
            "pub_title        Subjective Cognitive Impairment Is a Predomina...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Subjective cognitive impairment Â· Subjective ...\n",
            "mask                                                          True\n",
            "Name: 1900, dtype: object\n",
            "[352]\n",
            "adni\n",
            "Unnamed: 0                                                    5886\n",
            "Id                            180dae50-baa4-4707-a21d-a0202c8fed05\n",
            "pub_title        Quantitative Amyloid Imaging in Autosomal Domi...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Amyloid imaging plays an important role in th...\n",
            "mask                                                          True\n",
            "Name: 1950, dtype: object\n",
            "[553]\n",
            "adni\n",
            "Unnamed: 0                                                    6194\n",
            "Id                            b3900cd6-6a1f-436a-89a4-85791932b83c\n",
            "pub_title        A Review on Image- and Network-based Brain Dat...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              Abstract. Unveiling pathological brain change...\n",
            "mask                                                          True\n",
            "Name: 2000, dtype: object\n",
            "[617]\n",
            "adni\n",
            "Unnamed: 0                                                    6723\n",
            "Id                            94cac787-fcdb-43c1-ab65-33a3784c8965\n",
            "pub_title        Analysis of biased PET images caused by inaccu...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label                                                 ADNI\n",
            "cleaned_label                                                 adni\n",
            "text              PET scanners with an elongated axial field of...\n",
            "mask                                                          True\n",
            "Name: 2050, dtype: object\n",
            "[192 193 194 195 196 197 198]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    7137\n",
            "Id                            b6051caa-9eea-4569-b3e8-90ff50668cf7\n",
            "pub_title        The interplay of g and mathematical abilities ...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              This study investigates the interplay between...\n",
            "mask                                                          True\n",
            "Name: 2100, dtype: object\n",
            "[240 241 242 243 244 245 246]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    7290\n",
            "Id                            8baa76b3-06e8-40a4-b6e9-cef667bb7854\n",
            "pub_title        LEADERSHIP PRACTICES AMONGST HEADMASTERS OF C ...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              Abstract: This research intended to investiga...\n",
            "mask                                                          True\n",
            "Name: 2150, dtype: object\n",
            "[262 263 264 265 266 267 268]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    7439\n",
            "Id                            53e978f6-6a75-40eb-9281-3c3235f7c6ed\n",
            "pub_title        Early Learning Experiences, School Entry Skill...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              Early childhood is an opportune period during...\n",
            "mask                                                          True\n",
            "Name: 2200, dtype: object\n",
            "[208 209 210 211 212 213 214]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    7599\n",
            "Id                            163ef835-2dcf-4d14-b34c-9210851771cf\n",
            "pub_title        What happens when econometrics and psychometri...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              International large-scale assessments such as...\n",
            "mask                                                          True\n",
            "Name: 2250, dtype: object\n",
            "[338 339 340 341 342 343 344]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    7801\n",
            "Id                            ef03f007-1bbd-47b2-b29f-51f5b3c0b89d\n",
            "pub_title        Gendered study choice: a literature review. A ...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              Female participation in higher tertiary educa...\n",
            "mask                                                          True\n",
            "Name: 2300, dtype: object\n",
            "[151 152 153 154 155 156 157]\n",
            "trends\n",
            "in\n",
            "international\n",
            "mathematics\n",
            "and\n",
            "science\n",
            "study\n",
            "Unnamed: 0                                                    8027\n",
            "Id                            bef5b889-616b-4d8d-ae6c-a8e57e172d42\n",
            "pub_title        Qatarâ€™s Educational Reform: The Experiences an...\n",
            "dataset_title    Trends in International Mathematics and Scienc...\n",
            "dataset_label    Trends in International Mathematics and Scienc...\n",
            "cleaned_label    trends in international mathematics and scienc...\n",
            "text              This study describes the implications of the ...\n",
            "mask                                                          True\n",
            "Name: 2350, dtype: object\n",
            "[]\n",
            "Unnamed: 0                                                    8233\n",
            "Id                            c0f3aec2-4635-402a-8613-f85c1a1bc2a5\n",
            "pub_title        Special Education Services Received by Student...\n",
            "dataset_title                         Education Longitudinal Study\n",
            "dataset_label                         Education Longitudinal Study\n",
            "cleaned_label                         education longitudinal study\n",
            "text              Little is known about how special education s...\n",
            "mask                                                          True\n",
            "Name: 2400, dtype: object\n",
            "[283]\n",
            "adni\n",
            "Unnamed: 0                                                    8665\n",
            "Id                            16d394ca-1064-4e48-a600-1097423e8118\n",
            "pub_title        Modeling disease progression via fused sparse ...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Alzheimer's Disease (AD) is the most common n...\n",
            "mask                                                          True\n",
            "Name: 2450, dtype: object\n",
            "[118]\n",
            "adni\n",
            "Unnamed: 0                                                    8782\n",
            "Id                            6b4c82f4-f74b-4e7d-8aa4-a56653a5afaf\n",
            "pub_title        Quantification of Structural Brain Connectivit...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Connectomics has proved promising in quantify...\n",
            "mask                                                          True\n",
            "Name: 2500, dtype: object\n",
            "[67]\n",
            "adni\n",
            "Unnamed: 0                                                    8892\n",
            "Id                            bd7bc179-b246-4dd8-b6a9-e3a27b083a48\n",
            "pub_title        ABCA7 genotype altered AÎ² levels in cerebrospi...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Background: ATP-binding cassette transporter ...\n",
            "mask                                                          True\n",
            "Name: 2550, dtype: object\n",
            "[336]\n",
            "adni\n",
            "Unnamed: 0                                                    9030\n",
            "Id                            1a768bde-fe42-4091-90fe-39e88601eb08\n",
            "pub_title        Altered regional brain volumes in elderly carr...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Abstract Dopamine D2 receptors mediate the re...\n",
            "mask                                                          True\n",
            "Name: 2600, dtype: object\n",
            "[326]\n",
            "adni\n",
            "Unnamed: 0                                                    9174\n",
            "Id                            ea9dcd2f-6394-4b90-b7c3-57b21957fe97\n",
            "pub_title        Effect of apolipoprotein E phenotype on the as...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Introduction: The plasma concentration of bet...\n",
            "mask                                                          True\n",
            "Name: 2650, dtype: object\n",
            "[499]\n",
            "adni\n",
            "Unnamed: 0                                                    9336\n",
            "Id                            f9dc2971-3075-4705-9a9d-9ee3fa22b780\n",
            "pub_title        Feature Selection and Classification for High-...\n",
            "dataset_title    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "dataset_label    Alzheimer's Disease Neuroimaging Initiative (A...\n",
            "cleaned_label    alzheimer s disease neuroimaging initiative adni \n",
            "text              Due to missing values, incomplete dataset is ...\n",
            "mask                                                          True\n",
            "Name: 2700, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-2bbd5ca1165c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-181-208e02e60faa>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-181-208e02e60faa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, train)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 662\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ARGbPTtKuhR",
        "outputId": "7d647fb7-d4a7-4157-bdd3-114ca8cb552e"
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "x = []\n",
        "y = []\n",
        "for i in test_x:\n",
        "  x.append(i[0])\n",
        "  y.append(i[1])\n",
        "X       = sentences2indices(x, word2i, train=True)\n",
        "score = 0\n",
        "for j in range(0,len(X)):\n",
        "  z = torch.LongTensor(X[j])\n",
        "\n",
        "  result = lstm.inference(z.reshape((1,len(X[j])))).tolist()\n",
        "  result = np.array(result[0])\n",
        "  index = np.where(result == 1)[0]\n",
        "  a = []\n",
        "  for i in index:\n",
        "    a.append(i2word[X[j][i]])\n",
        "  if jaccard(a, y[j]) >= 0.5:\n",
        "    score += 1\n",
        "print(score/len(X))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7249398556535686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLvURgM-tE0d",
        "outputId": "6238bcd3-7511-43b4-a020-58342c8495dc"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "for i in copy:\n",
        "  x.append(i[0])\n",
        "  y.append(i[1])\n",
        "X       = sentences2indices(x, word2i, train=True)\n",
        "score = 0\n",
        "for j in range(0,len(X)):\n",
        "  z = torch.LongTensor(X[j])\n",
        "\n",
        "  result = lstm.inference(z.reshape((1,len(X[j])))).tolist()\n",
        "  result = np.array(result[0])\n",
        "  index = np.where(result == 1)[0]\n",
        "  a = []\n",
        "  for i in index:\n",
        "    a.append(i2word[X[j][i]])\n",
        "  if jaccard(a, y[j]) >= 0.5:\n",
        "    score += 1\n",
        "print(score/len(X))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7455295735900963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ue3UPfTeKsip",
        "outputId": "5241de0e-a0f7-4b5a-b2e0-bfa465292f5d"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"lstm.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_222836b1-41bf-4c5c-98dc-9fadc89caab2\", \"lstm.pt\", 91512147)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbBZ1pv1oEv"
      },
      "source": [
        "text = []\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6]\n",
        "\n",
        "\n",
        "  text.append((t, data.loc[i][5]))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxs0JLGXn-9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf20805-a005-4ee4-f4c6-d23233e1778d"
      },
      "source": [
        "pad_word = \"<pad>\"\n",
        "bos_word = \"<s>\"\n",
        "eos_word = \"</s>\"\n",
        "unk_word = \"<unk>\"\n",
        "pad_id = 0\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "unk_id = 3\n",
        "    \n",
        "def normalize_sentence(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
        "        self.word_count = {}\n",
        "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
        "        self.num_words = 4\n",
        "    \n",
        "    def get_ids_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
        "                               else unk_id for word in sentence.split()] + \\\n",
        "                               [eos_id]\n",
        "        return sent_ids\n",
        "    \n",
        "    def tokenized_sentence(self, sentence):\n",
        "        sent_ids = self.get_ids_from_sentence(sentence)\n",
        "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
        "\n",
        "    def decode_sentence_from_ids(self, sent_ids):\n",
        "        words = list()\n",
        "        for i, word_id in enumerate(sent_ids):\n",
        "            if word_id in [bos_id, eos_id, pad_id]:\n",
        "                # Skip these words\n",
        "                continue\n",
        "            else:\n",
        "                words.append(self.id_to_word[word_id])\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def add_words_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word_to_id:\n",
        "                # add this word to the vocabulary\n",
        "                self.word_to_id[word] = self.num_words\n",
        "                self.id_to_word[self.num_words] = word\n",
        "                self.word_count[word] = 1\n",
        "                self.num_words += 1\n",
        "            else:\n",
        "                # update the word count\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for src, tgt in text:\n",
        "    vocab.add_words_from_sentence(src)\n",
        "    vocab.add_words_from_sentence(tgt)\n",
        "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in the vocabulary = 56347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7n2N3wn_QF"
      },
      "source": [
        "Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5V-1Yj72sf3",
        "outputId": "fbcc3f7c-fe74-43a6-adc5-5d8d77e14c9a"
      },
      "source": [
        "for src, tgt in text[:3]:\n",
        "    sentence = src\n",
        "    word_tokens = vocab.tokenized_sentence(sentence)\n",
        "    # Automatically adds bos_id and eos_id before and after sentence ids respectively\n",
        "    word_ids = vocab.get_ids_from_sentence(sentence)\n",
        "    print(sentence)\n",
        "    print(word_tokens)\n",
        "    print(word_ids)\n",
        "    print(vocab.decode_sentence_from_ids(word_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Introduction: Characterizing progression in Alzheimer's disease is critically important for early detection and targeted treatment. The objective was to develop a prognostic model, based on multivariate longitudinal markers, for predicting progression-free survival in patients with mild cognitive impairment. Methods: The information contained in multiple longitudinal markers was extracted using multivariate functional principal components analysis and used as predictors in the Cox regression models. Cross-validation was used for selecting the best model based on Alzheimer's Disease Neuroimaging Initiative-1. External validation was conducted on Alzheimer's Disease Neuroimaging Initiative-2. Results: Model comparison yielded a prognostic index computed as the weighted combination of historical information of five neurocognitive longitudinal markers that are routinely collected in observational studies. The comprehensive validity analysis provided solid evidence of the usefulness of the model for predicting Alzheimer's disease progression. Discussion: The prognostic model was improved by incorporating multiple longitudinal markers. It is useful for monitoring disease and identifying patients for clinical trial recruitment. The growing public health threat posed by Alzheimer's disease (AD) has raised the urgency to discover and assess prognostic markers for the early detection of the disease. In this regard, a great deal of effort has been dedicated to building models for predicting AD based on a single marker, or a combination of multiple markers, which captures the heterogeneity among subjects and detects the disease progression of subjects at risk [1] .\n",
            "Because mild cognitive impairment (MCI) is a risk state for AD, existing research has identified a number of biomarkers that predict clinical changes of MCI patients [2] [3] [4] [5] [6] [7] , including neurocognitive markers, neuroimaging markers, genetics, and cerebrospinal fluid (CSF)-based markers. Although these studies showed an enhancement of the prognostic value when multivariate markers are considered, the prediction was either qualitative change (i.e., conversion from MCI to AD as a binary response) or quantitative change (i.e., cognitive scores) in the next few years. These studies did not examine the prediction of time from MCI to AD, which is usually the primary outcome in prevention clinical studies of AD [8].\n",
            "The authors have declared that no conflict of interest exists. 1 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at http://adni.loni.usc.edu/wp-content/ uploads/how to apply/ADNI Acknowledgement List.pdf. *Corresponding author. Relatively few recent studies have investigated the timing from MCI to AD over the duration of follow-up based on Cox regression models [9] [10] [11] . These studies assessed the predictive utility of various candidate prognostic markers independently or in combination but only focused on the baseline measurements of the markers. Li et al. [12] developed a joint modeling of longitudinal and time-to-event data technique to examine the comparative utility of the longitudinal markers in determining the risk of incident AD conversion at future time points. However, they analyzed each longitudinal marker independently because of the limitation of the current state-of-the-art joint modeling software. To our knowledge, no prior study has leveraged multiple longitudinal markers and time-to-event information jointly to investigate the prognosis of AD.\n",
            "The overarching goal of this study is to develop a prognostic model, which relies on serial measurements of multiple markers, for predicting progression-free survival in patients with MCI. The prognostic model uses several cutting-edge statistical methods, which enable it to facilitate clinical decision making based on all the information collected. We evaluated the model using data from the Alzheimer's Disease Neuroimaging Initiative 1 (ADNI-1), a public data set that is well suitable for this task because of its large sample size, breadth of markers, and prospective structure. The combined prognostic value of longitudinal neurocognitive tests, neuroimaging, genetics, and CSF markers was assessed using the prognostic model. External validation of the model is carried out on ADNI-2 to demonstrate the usefulness of the model across studies. The main output of the prognostic model is a prognostic index which can be updated over time as new measurements are available. Such an index is useful for monitoring disease progression for MCI patients and to enrich clinical trials with subjects likely to develop AD in the time frame of the trial.\n",
            "['<s>', 'Introduction', 'Characterizing', 'progression', 'in', 'Alzheimer', 's', 'disease', 'is', 'critically', 'important', 'for', 'early', 'detection', 'and', 'targeted', 'treatment', '.', 'The', 'objective', 'was', 'to', 'develop', 'a', 'prognostic', 'model', 'based', 'on', 'multivariate', 'longitudinal', 'markers', 'for', 'predicting', 'progression', 'free', 'survival', 'in', 'patients', 'with', 'mild', 'cognitive', 'impairment', '.', 'Methods', 'The', 'information', 'contained', 'in', 'multiple', 'longitudinal', 'markers', 'was', 'extracted', 'using', 'multivariate', 'functional', 'principal', 'components', 'analysis', 'and', 'used', 'as', 'predictors', 'in', 'the', 'Cox', 'regression', 'models', '.', 'Cross', 'validation', 'was', 'used', 'for', 'selecting', 'the', 'best', 'model', 'based', 'on', 'Alzheimer', 's', 'Disease', 'Neuroimaging', 'Initiative', '.', 'External', 'validation', 'was', 'conducted', 'on', 'Alzheimer', 's', 'Disease', 'Neuroimaging', 'Initiative', '.', 'Results', 'Model', 'comparison', 'yielded', 'a', 'prognostic', 'index', 'computed', 'as', 'the', 'weighted', 'combination', 'of', 'historical', 'information', 'of', 'five', 'neurocognitive', 'longitudinal', 'markers', 'that', 'are', 'routinely', 'collected', 'in', 'observational', 'studies', '.', 'The', 'comprehensive', 'validity', 'analysis', 'provided', 'solid', 'evidence', 'of', 'the', 'usefulness', 'of', 'the', 'model', 'for', 'predicting', 'Alzheimer', 's', 'disease', 'progression', '.', 'Discussion', 'The', 'prognostic', 'model', 'was', 'improved', 'by', 'incorporating', 'multiple', 'longitudinal', 'markers', '.', 'It', 'is', 'useful', 'for', 'monitoring', 'disease', 'and', 'identifying', 'patients', 'for', 'clinical', 'trial', 'recruitment', '.', 'The', 'growing', 'public', 'health', 'threat', 'posed', 'by', 'Alzheimer', 's', 'disease', 'AD', 'has', 'raised', 'the', 'urgency', 'to', 'discover', 'and', 'assess', 'prognostic', 'markers', 'for', 'the', 'early', 'detection', 'of', 'the', 'disease', '.', 'In', 'this', 'regard', 'a', 'great', 'deal', 'of', 'effort', 'has', 'been', 'dedicated', 'to', 'building', 'models', 'for', 'predicting', 'AD', 'based', 'on', 'a', 'single', 'marker', 'or', 'a', 'combination', 'of', 'multiple', 'markers', 'which', 'captures', 'the', 'heterogeneity', 'among', 'subjects', 'and', 'detects', 'the', 'disease', 'progression', 'of', 'subjects', 'at', 'risk', '.', 'Because', 'mild', 'cognitive', 'impairment', 'MCI', 'is', 'a', 'risk', 'state', 'for', 'AD', 'existing', 'research', 'has', 'identified', 'a', 'number', 'of', 'biomarkers', 'that', 'predict', 'clinical', 'changes', 'of', 'MCI', 'patients', 'including', 'neurocognitive', 'markers', 'neuroimaging', 'markers', 'genetics', 'and', 'cerebrospinal', 'fluid', 'CSF', 'based', 'markers', '.', 'Although', 'these', 'studies', 'showed', 'an', 'enhancement', 'of', 'the', 'prognostic', 'value', 'when', 'multivariate', 'markers', 'are', 'considered', 'the', 'prediction', 'was', 'either', 'qualitative', 'change', 'i', '.e', '.', 'conversion', 'from', 'MCI', 'to', 'AD', 'as', 'a', 'binary', 'response', 'or', 'quantitative', 'change', 'i', '.e', '.', 'cognitive', 'scores', 'in', 'the', 'next', 'few', 'years', '.', 'These', 'studies', 'did', 'not', 'examine', 'the', 'prediction', 'of', 'time', 'from', 'MCI', 'to', 'AD', 'which', 'is', 'usually', 'the', 'primary', 'outcome', 'in', 'prevention', 'clinical', 'studies', 'of', 'AD', '.', 'The', 'authors', 'have', 'declared', 'that', 'no', 'conflict', 'of', 'interest', 'exists', '.', 'Data', 'used', 'in', 'preparation', 'of', 'this', 'article', 'were', 'obtained', 'from', 'the', 'Alzheimer', 's', 'Disease', 'Neuroimaging', 'Initiative', 'ADNI', 'database', 'http', 'adni', '.loni', '.usc', '.edu', '.', 'As', 'such', 'the', 'investigators', 'within', 'the', 'ADNI', 'contributed', 'to', 'the', 'design', 'and', 'implementation', 'of', 'ADNI', 'and', 'or', 'provided', 'data', 'but', 'did', 'not', 'participate', 'in', 'analysis', 'or', 'writing', 'of', 'this', 'report', '.', 'A', 'complete', 'listing', 'of', 'ADNI', 'investigators', 'can', 'be', 'found', 'at', 'http', 'adni', '.loni', '.usc', '.edu', 'wp', 'content', 'uploads', 'how', 'to', 'apply', 'ADNI', 'Acknowledgement', 'List', '.pdf', '.', 'Corresponding', 'author', '.', 'Relatively', 'few', 'recent', 'studies', 'have', 'investigated', 'the', 'timing', 'from', 'MCI', 'to', 'AD', 'over', 'the', 'duration', 'of', 'follow', 'up', 'based', 'on', 'Cox', 'regression', 'models', '.', 'These', 'studies', 'assessed', 'the', 'predictive', 'utility', 'of', 'various', 'candidate', 'prognostic', 'markers', 'independently', 'or', 'in', 'combination', 'but', 'only', 'focused', 'on', 'the', 'baseline', 'measurements', 'of', 'the', 'markers', '.', 'Li', 'et', 'al', '.', 'developed', 'a', 'joint', 'modeling', 'of', 'longitudinal', 'and', 'time', 'to', 'event', 'data', 'technique', 'to', 'examine', 'the', 'comparative', 'utility', 'of', 'the', 'longitudinal', 'markers', 'in', 'determining', 'the', 'risk', 'of', 'incident', 'AD', 'conversion', 'at', 'future', 'time', 'points', '.', 'However', 'they', 'analyzed', 'each', 'longitudinal', 'marker', 'independently', 'because', 'of', 'the', 'limitation', 'of', 'the', 'current', 'state', 'of', 'the', 'art', 'joint', 'modeling', 'software', '.', 'To', 'our', 'knowledge', 'no', 'prior', 'study', 'has', 'leveraged', 'multiple', 'longitudinal', 'markers', 'and', 'time', 'to', 'event', 'information', 'jointly', 'to', 'investigate', 'the', 'prognosis', 'of', 'AD', '.', 'The', 'overarching', 'goal', 'of', 'this', 'study', 'is', 'to', 'develop', 'a', 'prognostic', 'model', 'which', 'relies', 'on', 'serial', 'measurements', 'of', 'multiple', 'markers', 'for', 'predicting', 'progression', 'free', 'survival', 'in', 'patients', 'with', 'MCI', '.', 'The', 'prognostic', 'model', 'uses', 'several', 'cutting', 'edge', 'statistical', 'methods', 'which', 'enable', 'it', 'to', 'facilitate', 'clinical', 'decision', 'making', 'based', 'on', 'all', 'the', 'information', 'collected', '.', 'We', 'evaluated', 'the', 'model', 'using', 'data', 'from', 'the', 'Alzheimer', 's', 'Disease', 'Neuroimaging', 'Initiative', 'ADNI', 'a', 'public', 'data', 'set', 'that', 'is', 'well', 'suitable', 'for', 'this', 'task', 'because', 'of', 'its', 'large', 'sample', 'size', 'breadth', 'of', 'markers', 'and', 'prospective', 'structure', '.', 'The', 'combined', 'prognostic', 'value', 'of', 'longitudinal', 'neurocognitive', 'tests', 'neuroimaging', 'genetics', 'and', 'CSF', 'markers', 'was', 'assessed', 'using', 'the', 'prognostic', 'model', '.', 'External', 'validation', 'of', 'the', 'model', 'is', 'carried', 'out', 'on', 'ADNI', 'to', 'demonstrate', 'the', 'usefulness', 'of', 'the', 'model', 'across', 'studies', '.', 'The', 'main', 'output', 'of', 'the', 'prognostic', 'model', 'is', 'a', 'prognostic', 'index', 'which', 'can', 'be', 'updated', 'over', 'time', 'as', 'new', 'measurements', 'are', 'available', '.', 'Such', 'an', 'index', 'is', 'useful', 'for', 'monitoring', 'disease', 'progression', 'for', 'MCI', 'patients', 'and', 'to', 'enrich', 'clinical', 'trials', 'with', 'subjects', 'likely', 'to', 'develop', 'AD', 'in', 'the', 'time', 'frame', 'of', 'the', 'trial', '.', '</s>']\n",
            "[1, 3176, 24189, 5537, 10, 8015, 88, 8003, 58, 3145, 8, 105, 487, 8938, 47, 1773, 3036, 15, 135, 341, 500, 61, 815, 27, 6730, 999, 635, 57, 2612, 80, 8294, 105, 98, 5537, 4550, 9659, 10, 8108, 363, 8166, 1859, 8296, 15, 3184, 135, 966, 2164, 10, 3679, 80, 8294, 500, 8146, 489, 2612, 5003, 7509, 2066, 916, 47, 210, 36, 123, 10, 19, 3666, 91, 926, 15, 10093, 4474, 500, 210, 105, 5194, 19, 1207, 999, 635, 57, 8015, 88, 8084, 8085, 2811, 15, 19754, 4474, 500, 707, 57, 8015, 88, 8084, 8085, 2811, 15, 1023, 6041, 2601, 359, 27, 6730, 2128, 6415, 36, 19, 2054, 5427, 43, 6733, 966, 43, 764, 12509, 80, 8294, 33, 40, 2850, 2058, 10, 6890, 117, 15, 135, 2841, 309, 916, 1156, 9124, 184, 43, 19, 5035, 43, 19, 999, 105, 98, 8015, 88, 8003, 5537, 15, 2865, 135, 6730, 999, 500, 2794, 65, 3384, 3679, 80, 8294, 15, 174, 58, 1751, 105, 5854, 8003, 47, 3607, 8108, 105, 7979, 9535, 10857, 15, 135, 1778, 849, 1572, 747, 3348, 65, 8015, 88, 8003, 8016, 53, 4063, 19, 19669, 61, 7512, 47, 3089, 6730, 8294, 105, 19, 487, 8938, 43, 19, 8003, 15, 263, 264, 5661, 27, 346, 679, 43, 35, 53, 168, 16162, 61, 2431, 926, 105, 98, 8016, 635, 57, 27, 2770, 8546, 217, 27, 5427, 43, 3679, 8294, 389, 8935, 19, 4934, 876, 573, 47, 17926, 19, 8003, 5537, 43, 573, 403, 2191, 15, 305, 8166, 1859, 8296, 8107, 58, 27, 2191, 1548, 105, 8016, 410, 471, 53, 619, 27, 459, 43, 8024, 33, 385, 7979, 3311, 43, 8107, 8108, 1031, 12509, 8294, 8002, 8294, 9384, 47, 8764, 6470, 8430, 635, 8294, 15, 1574, 283, 117, 625, 7, 4317, 43, 19, 6730, 28, 722, 2612, 8294, 40, 1421, 19, 5421, 500, 630, 4245, 985, 1120, 1121, 15, 8130, 18, 8107, 61, 8016, 36, 27, 8098, 73, 217, 884, 985, 1120, 1121, 15, 1859, 1162, 10, 19, 1471, 647, 267, 15, 456, 117, 628, 63, 290, 19, 5421, 43, 188, 18, 8107, 61, 8016, 389, 58, 2207, 19, 1043, 1139, 10, 1603, 7979, 117, 43, 8016, 15, 135, 1143, 167, 3155, 33, 373, 751, 43, 1001, 1392, 15, 1169, 210, 10, 1764, 43, 264, 5, 505, 213, 18, 19, 8015, 88, 8084, 8085, 2811, 8086, 1966, 2814, 8087, 8088, 8089, 4171, 15, 443, 159, 19, 8134, 1153, 19, 8086, 7932, 61, 19, 1762, 47, 4445, 43, 8086, 47, 217, 1156, 17, 326, 628, 63, 1772, 10, 916, 217, 1118, 43, 264, 376, 15, 646, 795, 2126, 43, 8086, 8134, 285, 242, 618, 403, 2814, 8087, 8088, 8089, 4171, 8135, 4068, 8136, 291, 61, 3819, 8086, 8137, 8138, 8139, 15, 8239, 3147, 15, 11419, 647, 417, 117, 167, 1313, 19, 810, 18, 8107, 61, 8016, 187, 19, 2385, 43, 268, 269, 635, 57, 3666, 91, 926, 15, 456, 117, 2124, 19, 4407, 8904, 43, 204, 8527, 6730, 8294, 3635, 217, 10, 5427, 326, 237, 643, 57, 19, 5496, 6051, 43, 19, 8294, 15, 6661, 381, 382, 15, 521, 27, 2362, 3307, 43, 80, 47, 188, 61, 2390, 17, 320, 61, 290, 19, 4277, 8904, 43, 19, 80, 8294, 10, 1095, 19, 2191, 43, 10577, 8016, 8130, 403, 99, 188, 1439, 15, 479, 125, 2033, 1136, 80, 8546, 3635, 347, 43, 19, 5971, 43, 19, 485, 1548, 43, 19, 7446, 2362, 3307, 5896, 15, 748, 307, 1208, 373, 2050, 81, 53, 2851, 3679, 80, 8294, 47, 188, 61, 2390, 966, 5529, 61, 343, 19, 10330, 43, 8016, 15, 135, 16161, 2329, 43, 264, 81, 58, 61, 815, 27, 6730, 999, 389, 3500, 57, 8290, 6051, 43, 3679, 8294, 105, 98, 5537, 4550, 9659, 10, 8108, 363, 8107, 15, 135, 6730, 999, 1634, 570, 7152, 7057, 2035, 3160, 389, 3323, 194, 61, 3697, 7979, 1485, 1486, 635, 57, 133, 19, 966, 2058, 15, 95, 4623, 19, 999, 489, 17, 18, 19, 8015, 88, 8084, 8085, 2811, 8086, 27, 849, 17, 437, 33, 58, 206, 6765, 105, 264, 4447, 347, 43, 1237, 59, 1011, 827, 4228, 43, 8294, 47, 3661, 1886, 15, 135, 1613, 6730, 28, 43, 80, 12509, 761, 8002, 9384, 47, 8430, 8294, 500, 2124, 489, 19, 6730, 999, 15, 19754, 4474, 43, 19, 999, 58, 6977, 78, 57, 8086, 61, 3768, 19, 5035, 43, 19, 999, 92, 117, 15, 135, 199, 6547, 43, 19, 6730, 999, 58, 27, 6730, 2128, 389, 285, 242, 6944, 187, 188, 36, 585, 6051, 40, 1180, 15, 899, 7, 2128, 58, 1751, 105, 5854, 8003, 5537, 105, 8107, 8108, 47, 61, 25013, 7979, 8039, 363, 573, 240, 61, 815, 8016, 10, 19, 188, 8585, 43, 19, 9535, 15, 2]\n",
            "Introduction Characterizing progression in Alzheimer s disease is critically important for early detection and targeted treatment . The objective was to develop a prognostic model based on multivariate longitudinal markers for predicting progression free survival in patients with mild cognitive impairment . Methods The information contained in multiple longitudinal markers was extracted using multivariate functional principal components analysis and used as predictors in the Cox regression models . Cross validation was used for selecting the best model based on Alzheimer s Disease Neuroimaging Initiative . External validation was conducted on Alzheimer s Disease Neuroimaging Initiative . Results Model comparison yielded a prognostic index computed as the weighted combination of historical information of five neurocognitive longitudinal markers that are routinely collected in observational studies . The comprehensive validity analysis provided solid evidence of the usefulness of the model for predicting Alzheimer s disease progression . Discussion The prognostic model was improved by incorporating multiple longitudinal markers . It is useful for monitoring disease and identifying patients for clinical trial recruitment . The growing public health threat posed by Alzheimer s disease AD has raised the urgency to discover and assess prognostic markers for the early detection of the disease . In this regard a great deal of effort has been dedicated to building models for predicting AD based on a single marker or a combination of multiple markers which captures the heterogeneity among subjects and detects the disease progression of subjects at risk . Because mild cognitive impairment MCI is a risk state for AD existing research has identified a number of biomarkers that predict clinical changes of MCI patients including neurocognitive markers neuroimaging markers genetics and cerebrospinal fluid CSF based markers . Although these studies showed an enhancement of the prognostic value when multivariate markers are considered the prediction was either qualitative change i .e . conversion from MCI to AD as a binary response or quantitative change i .e . cognitive scores in the next few years . These studies did not examine the prediction of time from MCI to AD which is usually the primary outcome in prevention clinical studies of AD . The authors have declared that no conflict of interest exists . Data used in preparation of this article were obtained from the Alzheimer s Disease Neuroimaging Initiative ADNI database http adni .loni .usc .edu . As such the investigators within the ADNI contributed to the design and implementation of ADNI and or provided data but did not participate in analysis or writing of this report . A complete listing of ADNI investigators can be found at http adni .loni .usc .edu wp content uploads how to apply ADNI Acknowledgement List .pdf . Corresponding author . Relatively few recent studies have investigated the timing from MCI to AD over the duration of follow up based on Cox regression models . These studies assessed the predictive utility of various candidate prognostic markers independently or in combination but only focused on the baseline measurements of the markers . Li et al . developed a joint modeling of longitudinal and time to event data technique to examine the comparative utility of the longitudinal markers in determining the risk of incident AD conversion at future time points . However they analyzed each longitudinal marker independently because of the limitation of the current state of the art joint modeling software . To our knowledge no prior study has leveraged multiple longitudinal markers and time to event information jointly to investigate the prognosis of AD . The overarching goal of this study is to develop a prognostic model which relies on serial measurements of multiple markers for predicting progression free survival in patients with MCI . The prognostic model uses several cutting edge statistical methods which enable it to facilitate clinical decision making based on all the information collected . We evaluated the model using data from the Alzheimer s Disease Neuroimaging Initiative ADNI a public data set that is well suitable for this task because of its large sample size breadth of markers and prospective structure . The combined prognostic value of longitudinal neurocognitive tests neuroimaging genetics and CSF markers was assessed using the prognostic model . External validation of the model is carried out on ADNI to demonstrate the usefulness of the model across studies . The main output of the prognostic model is a prognostic index which can be updated over time as new measurements are available . Such an index is useful for monitoring disease progression for MCI patients and to enrich clinical trials with subjects likely to develop AD in the time frame of the trial .\n",
            " This paper uses data from the Program for the International Assessment of Adult Competencies (PIAAC) to analyze the relationship between self-reported health (SRH) and literacy and numeracy proficiency for immigrants compared to U.S.-born respondents and for Hispanic versus Asian immigrants. The research questions were: (1) Are literacy and numeracy scores associated with adults' SRH? (2) Are associations between SRH and literacy and numeracy proficiency moderated by immigrant status? (3) Among immigrants, are literacy and numeracy scores more strongly associated with SRH for Hispanics versus Asians? Immigrants had significantly lower literacy and numeracy scores, yet reported better health than U.S.-born respondents. Ordinal logistic regression analyses showed that literacy and numeracy were both positively related to SRH for immigrants and U.S.-born adults, and should therefore be viewed as part of the growing evidence that literacy is an independent and significant social determinant of health. Second, U.S.-born and immigrant adults accrued similarly positive health benefits from stronger literacy and numeracy skills. Third, although Hispanic immigrants were more disadvantaged than Asian immigrants on almost all socioeconomic characteristics and had significantly lower literacy and numeracy scores and worse SRH than Asian immigrants, both Hispanic and Asian immigrants experienced similar positive health returns from literacy and numeracy proficiency. These findings underscore the potential health benefits of providing adult basic education instruction, particularly for immigrants with the least formal schooling and fewest socioeconomic resources. This paper uses U.S. data from the Program for the International Assessment of Adult Competencies (PIAAC) to analyze the relationship between self-rated health (SRH) and proficiency in literacy and numeracy for immigrants and U.S.-born adults. Educational attainment is strongly related to health [1, 2] ; however, the health benefits of formal education do not accrue equally across racial/ethnic groups in the United States. In particular, blacks experience \"diminishing returns to education,\" meaning they derive fewer health rewards than whites from increasing levels of formal education [3] [4] [5] . Our previous PIAAC analyses suggest that literacy and numeracy proficiency are also associated with U.S. adults' self-rated health, but unlike with educational attainment, minority racial/ethnic groups benefit equally from stronger literacy and numeracy skills [6] . However, this previous research did not explore how the relationship between health and proficiency in literacy and numeracy varies by immigrant status, a particularly important topic given the large and growing immigrant population in the U.S. This is also an under-explored topic in the literature on basic skills and health.\n",
            "Immigrants to the U.S. tend to enjoy better health than their U.S.-born counterparts. This \"healthy immigrant effect\" primarily stems from selection effects-healthier people are more likely to migrate-and have better health behaviors, at least immediately following migration [7] [8] [9] . However, we do not know whether immigrants and U.S.-born adults accumulate similar health benefits from literacy and numeracy. On the one hand, inequitable access to health care and insurance [10, 11] , poor healthcare quality [10] , segregation and environmental hazards [12] , cultural dissonance and lack of familiarity with the healthcare system [13] , and limited English proficiency [10] may diminish immigrants' ability to convert literacy and numeracy proficiencies into health rewards. On the other hand, because people with more socioeconomic resources are more likely to migrate, immigrants may be better equipped to reap health benefits from literacy and numeracy.\n",
            "Furthermore, Asian and Hispanic immigrants' heterogeneous cultural, employment, and residential contexts might influence their ability to accumulate health rewards from basic skills. Specifically, Asian immigrants might be better able to do so because they have comparatively higher educational attainment and more highly educated parents and tend to work in professional occupations [14] .\n",
            "Our research explores these topics by answering the following questions: (1) Are literacy and numeracy scores associated with adults' SRH? (2) Are associations between SRH and proficiency in literacy and numeracy moderated by immigrant status? (3) Among immigrants, are literacy and numeracy skills more strongly associated with SRH for Hispanics versus Asians? We also examine the role of socioeconomic status (SES)-related human capital characteristics as a pathway through which literacy and numeracy may be associated with SRH. This study adds to scholarship on social determinants of health and adult basic skills [15] [16] [17] [18] [19] by examining how literacy and numeracy may differentially contribute to health for immigrants and native-born adults in the United States.\n",
            "['<s>', 'This', 'paper', 'uses', 'data', 'from', 'the', 'Program', 'for', 'the', 'International', 'Assessment', 'of', 'Adult', 'Competencies', 'PIAAC', 'to', 'analyze', 'the', 'relationship', 'between', 'self', 'reported', 'health', 'SRH', 'and', 'literacy', 'and', 'numeracy', 'proficiency', 'for', 'immigrants', 'compared', 'to', 'U', '.S', '.', 'born', 'respondents', 'and', 'for', 'Hispanic', 'versus', 'Asian', 'immigrants', '.', 'The', 'research', 'questions', 'were', 'Are', 'literacy', 'and', 'numeracy', 'scores', 'associated', 'with', 'adults', 'SRH', '?', 'Are', 'associations', 'between', 'SRH', 'and', 'literacy', 'and', 'numeracy', 'proficiency', 'moderated', 'by', 'immigrant', 'status', '?', 'Among', 'immigrants', 'are', 'literacy', 'and', 'numeracy', 'scores', 'more', 'strongly', 'associated', 'with', 'SRH', 'for', 'Hispanics', 'versus', 'Asians', '?', 'Immigrants', 'had', 'significantly', 'lower', 'literacy', 'and', 'numeracy', 'scores', 'yet', 'reported', 'better', 'health', 'than', 'U', '.S', '.', 'born', 'respondents', '.', 'Ordinal', 'logistic', 'regression', 'analyses', 'showed', 'that', 'literacy', 'and', 'numeracy', 'were', 'both', 'positively', 'related', 'to', 'SRH', 'for', 'immigrants', 'and', 'U', '.S', '.', 'born', 'adults', 'and', 'should', 'therefore', 'be', 'viewed', 'as', 'part', 'of', 'the', 'growing', 'evidence', 'that', 'literacy', 'is', 'an', 'independent', 'and', 'significant', 'social', 'determinant', 'of', 'health', '.', 'Second', 'U', '.S', '.', 'born', 'and', 'immigrant', 'adults', 'accrued', 'similarly', 'positive', 'health', 'benefits', 'from', 'stronger', 'literacy', 'and', 'numeracy', 'skills', '.', 'Third', 'although', 'Hispanic', 'immigrants', 'were', 'more', 'disadvantaged', 'than', 'Asian', 'immigrants', 'on', 'almost', 'all', 'socioeconomic', 'characteristics', 'and', 'had', 'significantly', 'lower', 'literacy', 'and', 'numeracy', 'scores', 'and', 'worse', 'SRH', 'than', 'Asian', 'immigrants', 'both', 'Hispanic', 'and', 'Asian', 'immigrants', 'experienced', 'similar', 'positive', 'health', 'returns', 'from', 'literacy', 'and', 'numeracy', 'proficiency', '.', 'These', 'findings', 'underscore', 'the', 'potential', 'health', 'benefits', 'of', 'providing', 'adult', 'basic', 'education', 'instruction', 'particularly', 'for', 'immigrants', 'with', 'the', 'least', 'formal', 'schooling', 'and', 'fewest', 'socioeconomic', 'resources', '.', 'This', 'paper', 'uses', 'U', '.S', '.', 'data', 'from', 'the', 'Program', 'for', 'the', 'International', 'Assessment', 'of', 'Adult', 'Competencies', 'PIAAC', 'to', 'analyze', 'the', 'relationship', 'between', 'self', 'rated', 'health', 'SRH', 'and', 'proficiency', 'in', 'literacy', 'and', 'numeracy', 'for', 'immigrants', 'and', 'U', '.S', '.', 'born', 'adults', '.', 'Educational', 'attainment', 'is', 'strongly', 'related', 'to', 'health', 'however', 'the', 'health', 'benefits', 'of', 'formal', 'education', 'do', 'not', 'accrue', 'equally', 'across', 'racial', 'ethnic', 'groups', 'in', 'the', 'United', 'States', '.', 'In', 'particular', 'blacks', 'experience', 'diminishing', 'returns', 'to', 'education', 'meaning', 'they', 'derive', 'fewer', 'health', 'rewards', 'than', 'whites', 'from', 'increasing', 'levels', 'of', 'formal', 'education', '.', 'Our', 'previous', 'PIAAC', 'analyses', 'suggest', 'that', 'literacy', 'and', 'numeracy', 'proficiency', 'are', 'also', 'associated', 'with', 'U', '.S', '.', 'adults', 'self', 'rated', 'health', 'but', 'unlike', 'with', 'educational', 'attainment', 'minority', 'racial', 'ethnic', 'groups', 'benefit', 'equally', 'from', 'stronger', 'literacy', 'and', 'numeracy', 'skills', '.', 'However', 'this', 'previous', 'research', 'did', 'not', 'explore', 'how', 'the', 'relationship', 'between', 'health', 'and', 'proficiency', 'in', 'literacy', 'and', 'numeracy', 'varies', 'by', 'immigrant', 'status', 'a', 'particularly', 'important', 'topic', 'given', 'the', 'large', 'and', 'growing', 'immigrant', 'population', 'in', 'the', 'U', '.S', '.', 'This', 'is', 'also', 'an', 'under', 'explored', 'topic', 'in', 'the', 'literature', 'on', 'basic', 'skills', 'and', 'health', '.', 'Immigrants', 'to', 'the', 'U', '.S', '.', 'tend', 'to', 'enjoy', 'better', 'health', 'than', 'their', 'U', '.S', '.', 'born', 'counterparts', '.', 'This', 'healthy', 'immigrant', 'effect', 'primarily', 'stems', 'from', 'selection', 'effects', 'healthier', 'people', 'are', 'more', 'likely', 'to', 'migrate', 'and', 'have', 'better', 'health', 'behaviors', 'at', 'least', 'immediately', 'following', 'migration', '.', 'However', 'we', 'do', 'not', 'know', 'whether', 'immigrants', 'and', 'U', '.S', '.', 'born', 'adults', 'accumulate', 'similar', 'health', 'benefits', 'from', 'literacy', 'and', 'numeracy', '.', 'On', 'the', 'one', 'hand', 'inequitable', 'access', 'to', 'health', 'care', 'and', 'insurance', 'poor', 'healthcare', 'quality', 'segregation', 'and', 'environmental', 'hazards', 'cultural', 'dissonance', 'and', 'lack', 'of', 'familiarity', 'with', 'the', 'healthcare', 'system', 'and', 'limited', 'English', 'proficiency', 'may', 'diminish', 'immigrants', 'ability', 'to', 'convert', 'literacy', 'and', 'numeracy', 'proficiencies', 'into', 'health', 'rewards', '.', 'On', 'the', 'other', 'hand', 'because', 'people', 'with', 'more', 'socioeconomic', 'resources', 'are', 'more', 'likely', 'to', 'migrate', 'immigrants', 'may', 'be', 'better', 'equipped', 'to', 'reap', 'health', 'benefits', 'from', 'literacy', 'and', 'numeracy', '.', 'Furthermore', 'Asian', 'and', 'Hispanic', 'immigrants', 'heterogeneous', 'cultural', 'employment', 'and', 'residential', 'contexts', 'might', 'influence', 'their', 'ability', 'to', 'accumulate', 'health', 'rewards', 'from', 'basic', 'skills', '.', 'Specifically', 'Asian', 'immigrants', 'might', 'be', 'better', 'able', 'to', 'do', 'so', 'because', 'they', 'have', 'comparatively', 'higher', 'educational', 'attainment', 'and', 'more', 'highly', 'educated', 'parents', 'and', 'tend', 'to', 'work', 'in', 'professional', 'occupations', '.', 'Our', 'research', 'explores', 'these', 'topics', 'by', 'answering', 'the', 'following', 'questions', 'Are', 'literacy', 'and', 'numeracy', 'scores', 'associated', 'with', 'adults', 'SRH', '?', 'Are', 'associations', 'between', 'SRH', 'and', 'proficiency', 'in', 'literacy', 'and', 'numeracy', 'moderated', 'by', 'immigrant', 'status', '?', 'Among', 'immigrants', 'are', 'literacy', 'and', 'numeracy', 'skills', 'more', 'strongly', 'associated', 'with', 'SRH', 'for', 'Hispanics', 'versus', 'Asians', '?', 'We', 'also', 'examine', 'the', 'role', 'of', 'socioeconomic', 'status', 'SES', 'related', 'human', 'capital', 'characteristics', 'as', 'a', 'pathway', 'through', 'which', 'literacy', 'and', 'numeracy', 'may', 'be', 'associated', 'with', 'SRH', '.', 'This', 'study', 'adds', 'to', 'scholarship', 'on', 'social', 'determinants', 'of', 'health', 'and', 'adult', 'basic', 'skills', 'by', 'examining', 'how', 'literacy', 'and', 'numeracy', 'may', 'differentially', 'contribute', 'to', 'health', 'for', 'immigrants', 'and', 'native', 'born', 'adults', 'in', 'the', 'United', 'States', '.', '</s>']\n",
            "[1, 4, 912, 1634, 17, 18, 19, 5880, 105, 19, 4212, 1112, 43, 3190, 16573, 16649, 61, 1677, 19, 515, 344, 622, 226, 1572, 16661, 47, 1096, 47, 5814, 3769, 105, 16662, 254, 61, 1576, 1577, 15, 4591, 2109, 47, 105, 495, 2736, 496, 16662, 15, 135, 471, 1757, 505, 7489, 1096, 47, 5814, 1162, 543, 363, 87, 16661, 386, 7489, 486, 344, 16661, 47, 1096, 47, 5814, 3769, 3676, 65, 3361, 514, 386, 540, 16662, 40, 1096, 47, 5814, 1162, 136, 1412, 543, 363, 16661, 105, 113, 2736, 649, 386, 16663, 652, 1167, 235, 1096, 47, 5814, 1162, 1409, 226, 544, 1572, 180, 1576, 1577, 15, 4591, 2109, 15, 16664, 2776, 91, 104, 625, 33, 1096, 47, 5814, 505, 504, 216, 804, 61, 16661, 105, 16662, 47, 1576, 1577, 15, 4591, 87, 47, 755, 974, 242, 2805, 36, 1735, 43, 19, 1778, 184, 33, 1096, 58, 7, 3571, 47, 122, 164, 1670, 43, 1572, 15, 223, 1576, 1577, 15, 4591, 47, 3361, 87, 16665, 297, 55, 1572, 1466, 18, 2298, 1096, 47, 5814, 1807, 15, 2025, 702, 495, 16662, 505, 136, 1699, 180, 496, 16662, 57, 1621, 133, 1373, 49, 47, 652, 1167, 235, 1096, 47, 5814, 1162, 47, 3578, 16661, 180, 496, 16662, 504, 495, 47, 496, 16662, 2912, 260, 55, 1572, 2595, 18, 1096, 47, 5814, 3769, 15, 456, 362, 14157, 19, 76, 1572, 1466, 43, 2315, 572, 4502, 30, 1798, 513, 105, 16662, 363, 19, 1335, 2403, 365, 47, 16666, 1373, 62, 15, 4, 912, 1634, 1576, 1577, 15, 17, 18, 19, 5880, 105, 19, 4212, 1112, 43, 3190, 16573, 16649, 61, 1677, 19, 515, 344, 622, 3964, 1572, 16661, 47, 3769, 10, 1096, 47, 5814, 105, 16662, 47, 1576, 1577, 15, 4591, 87, 15, 1968, 857, 58, 1412, 804, 61, 1572, 185, 19, 1572, 1466, 43, 2403, 30, 864, 63, 11376, 340, 92, 108, 109, 110, 10, 19, 1037, 1039, 15, 263, 924, 16667, 193, 9372, 2595, 61, 30, 2245, 125, 7047, 1836, 1572, 3303, 180, 2540, 18, 1275, 1190, 43, 2403, 30, 15, 50, 224, 16649, 104, 52, 33, 1096, 47, 5814, 3769, 40, 41, 543, 363, 1576, 1577, 15, 87, 622, 3964, 1572, 326, 3816, 363, 856, 857, 128, 108, 109, 110, 1755, 340, 18, 2298, 1096, 47, 5814, 1807, 15, 479, 264, 224, 471, 628, 63, 913, 291, 19, 515, 344, 1572, 47, 3769, 10, 1096, 47, 5814, 930, 65, 3361, 514, 27, 513, 8, 2338, 1800, 19, 59, 47, 1778, 3361, 208, 10, 19, 1576, 1577, 15, 4, 58, 41, 7, 842, 7155, 2338, 10, 19, 173, 57, 4502, 1807, 47, 1572, 15, 16663, 61, 19, 1576, 1577, 15, 1072, 61, 15444, 544, 1572, 180, 72, 1576, 1577, 15, 4591, 653, 15, 4, 3300, 3361, 56, 1906, 5040, 18, 312, 304, 11236, 529, 40, 136, 240, 61, 7399, 47, 167, 544, 1572, 478, 403, 1335, 2517, 330, 2670, 15, 479, 25, 864, 63, 4235, 336, 16662, 47, 1576, 1577, 15, 4591, 87, 2597, 260, 1572, 1466, 18, 1096, 47, 5814, 15, 878, 19, 733, 869, 16668, 1394, 61, 1572, 8235, 47, 7309, 701, 11801, 196, 453, 47, 933, 6897, 642, 16669, 47, 1734, 43, 16102, 363, 19, 11801, 2731, 47, 1704, 3817, 3769, 241, 12316, 16662, 434, 61, 7158, 1096, 47, 5814, 16670, 978, 1572, 3303, 15, 878, 19, 528, 869, 347, 529, 363, 136, 1373, 62, 40, 136, 240, 61, 7399, 16662, 241, 242, 544, 14496, 61, 16627, 1572, 1466, 18, 1096, 47, 5814, 15, 2830, 496, 47, 495, 16662, 8804, 642, 1746, 47, 14419, 16506, 1061, 706, 72, 434, 61, 2597, 1572, 3303, 18, 4502, 1807, 15, 571, 496, 16662, 1061, 242, 544, 1137, 61, 864, 127, 347, 125, 167, 14973, 138, 856, 857, 47, 136, 257, 2649, 953, 47, 1072, 61, 358, 10, 1192, 962, 15, 50, 471, 1692, 283, 4311, 65, 13392, 19, 330, 1757, 7489, 1096, 47, 5814, 1162, 543, 363, 87, 16661, 386, 7489, 486, 344, 16661, 47, 3769, 10, 1096, 47, 5814, 3676, 65, 3361, 514, 386, 540, 16662, 40, 1096, 47, 5814, 1807, 136, 1412, 543, 363, 16661, 105, 113, 2736, 649, 386, 95, 41, 290, 19, 412, 43, 1373, 514, 1406, 804, 1468, 2520, 49, 36, 27, 2633, 621, 389, 1096, 47, 5814, 241, 242, 543, 363, 16661, 15, 4, 81, 11192, 61, 13268, 57, 164, 2337, 43, 1572, 47, 572, 4502, 1807, 65, 245, 291, 1096, 47, 5814, 241, 3175, 609, 61, 1572, 105, 16662, 47, 15845, 4591, 87, 10, 19, 1037, 1039, 15, 2]\n",
            "This paper uses data from the Program for the International Assessment of Adult Competencies PIAAC to analyze the relationship between self reported health SRH and literacy and numeracy proficiency for immigrants compared to U .S . born respondents and for Hispanic versus Asian immigrants . The research questions were Are literacy and numeracy scores associated with adults SRH ? Are associations between SRH and literacy and numeracy proficiency moderated by immigrant status ? Among immigrants are literacy and numeracy scores more strongly associated with SRH for Hispanics versus Asians ? Immigrants had significantly lower literacy and numeracy scores yet reported better health than U .S . born respondents . Ordinal logistic regression analyses showed that literacy and numeracy were both positively related to SRH for immigrants and U .S . born adults and should therefore be viewed as part of the growing evidence that literacy is an independent and significant social determinant of health . Second U .S . born and immigrant adults accrued similarly positive health benefits from stronger literacy and numeracy skills . Third although Hispanic immigrants were more disadvantaged than Asian immigrants on almost all socioeconomic characteristics and had significantly lower literacy and numeracy scores and worse SRH than Asian immigrants both Hispanic and Asian immigrants experienced similar positive health returns from literacy and numeracy proficiency . These findings underscore the potential health benefits of providing adult basic education instruction particularly for immigrants with the least formal schooling and fewest socioeconomic resources . This paper uses U .S . data from the Program for the International Assessment of Adult Competencies PIAAC to analyze the relationship between self rated health SRH and proficiency in literacy and numeracy for immigrants and U .S . born adults . Educational attainment is strongly related to health however the health benefits of formal education do not accrue equally across racial ethnic groups in the United States . In particular blacks experience diminishing returns to education meaning they derive fewer health rewards than whites from increasing levels of formal education . Our previous PIAAC analyses suggest that literacy and numeracy proficiency are also associated with U .S . adults self rated health but unlike with educational attainment minority racial ethnic groups benefit equally from stronger literacy and numeracy skills . However this previous research did not explore how the relationship between health and proficiency in literacy and numeracy varies by immigrant status a particularly important topic given the large and growing immigrant population in the U .S . This is also an under explored topic in the literature on basic skills and health . Immigrants to the U .S . tend to enjoy better health than their U .S . born counterparts . This healthy immigrant effect primarily stems from selection effects healthier people are more likely to migrate and have better health behaviors at least immediately following migration . However we do not know whether immigrants and U .S . born adults accumulate similar health benefits from literacy and numeracy . On the one hand inequitable access to health care and insurance poor healthcare quality segregation and environmental hazards cultural dissonance and lack of familiarity with the healthcare system and limited English proficiency may diminish immigrants ability to convert literacy and numeracy proficiencies into health rewards . On the other hand because people with more socioeconomic resources are more likely to migrate immigrants may be better equipped to reap health benefits from literacy and numeracy . Furthermore Asian and Hispanic immigrants heterogeneous cultural employment and residential contexts might influence their ability to accumulate health rewards from basic skills . Specifically Asian immigrants might be better able to do so because they have comparatively higher educational attainment and more highly educated parents and tend to work in professional occupations . Our research explores these topics by answering the following questions Are literacy and numeracy scores associated with adults SRH ? Are associations between SRH and proficiency in literacy and numeracy moderated by immigrant status ? Among immigrants are literacy and numeracy skills more strongly associated with SRH for Hispanics versus Asians ? We also examine the role of socioeconomic status SES related human capital characteristics as a pathway through which literacy and numeracy may be associated with SRH . This study adds to scholarship on social determinants of health and adult basic skills by examining how literacy and numeracy may differentially contribute to health for immigrants and native born adults in the United States .\n",
            " Abstract: Reference intervals (RIs) for laboratory analyses by and large, are provided by analytical platform providers -the provenience and preanalytics of materials for the calculation of intervals often remain arcane particularly relating to the age group of donors. In an observational, prospective cohort study on 1467 healthy uniracial Caucasian residents >60 years of age, 105 frequently used lab tests were done on one blood sample. With a nonrestrictive definition of health, several pathological lab results pointing to occult disease have been found and published from SENIORLAB so far. The RIs found for hemoglobin in women went from 117.9 to 152.4 g/L (80-84 years) and in men from 124.9 to 170.6 g/L (90% confidence interval [CI]). This article lists RIs computed with SENIORLAB data for such frequently ordered analyses as platelet counts, vitamin B12 and folate, ferritin and analytes measured to estimate metabolic performance in glucose turnover. In fact, 64.5% of the cohort showed prediabetic fasting plasma glucose (FPG) and/or glycated hemoglobin (HbA 1c ); total serum folate levels but not red blood cell folate decreased with progressing age. As much as 66% of evaluable study participants had insufficient levels of 25(OH) vitamin D. Published reports from SENIORLAB are referenced in this article. Ten years ago, the SENIORLAB study was designed to establish reference intervals (RIs) of current medical lab assays suitable for validation performed in elderly patients. At the onset and upon immediate evaluation, the RIs were computed on cross-comparative data sets usually grouping ages from 60 to 69, 70 to 79 and above 79 years of age. The study was concluded with RIs of most lab analyses performed. Questionnaires were sent to surviving participants in an endeavor to relate quality of remaining life years to lab values determined at the onset of disease. Here, we report part of the results in the context of geriatric medicine.\n",
            "Before this paper goes into the data obtained, let us cite Georges Canguilhem (1904 Canguilhem ( -1995 [1] who tells us in his thesis on The Normal and the Pathological: 'MÃ©decins et chirurgiens ont une information clinique et utilisent aussi parfois des techniques de laboratoire qui leur permettent de savoir malades des gens qui ne se sentent pas tels' (English translation: \"With clinical information at hand, doctors and surgeons sometimes use lab techniques to identify persons who are sick without signs and symptoms\").\n",
            "With the extension of the human life span the laboratory medicine community began to scrutinize RIs applicable for validating assays ordered for older patients. There were already short lists of analytes available, the RIs of which substantially deviate from work force age groups. It has long been known that sex hormones decline and a number of parameters have joined an ever growing list, often immediately upon initial recognition of their importance in the improvement of diagnosis.\n",
            "Aging, senescence and maturation are denominations with slightly different meanings. Whereas senescence is interpreted as endogenous degenerative processes leading to death, aging encompasses a wide array of passive or nonregulated, degenerative steps determined, at least in part, by exogenous factors; gradual deterioration with aging can be looked at as a fail-safe system with collateral turnover that is prone to repair [2] . Involution such as with the thymus may feign senescence and age-related decline in T cell output is a risk factor for many cancer types and infectious diseases [3] , thymic atrophy which starts at puberty in all vertebrates. When we submitted the study protocol of SENIORLAB to the competent agencies back in 2008, many studies had already addressed the question of specific RIs in the elderly. The summary of the then screened literature can be listed as is shown in Table 1 .\n",
            "The increasing mean age of the world's population concerns all those survivors of accidental death unruffled by senescent biology. That there are differences in the causes of death among species and, ultimately among different human races and different individuals, was made evident from studies on the survivor patterns for cohorts of organisms. Pearl and Miner [4] described several possible types of survivor curves. In Figure 1 , curve A is characteristic of a population in which death is predominantly caused by disaster, predation or other random external selection forces.\n",
            "The constant rate of removal results in a survivor curve that drops exponentially, gradually approaching zero; mortality of 50% per unit of time. The nonaligned pattern in B denotes an initially strong surviving population with increased susceptibility to death with age in the absence of compromising influences of any type whereas the shape C shows a survival curve from a population with abrupt termination of all members by natural processes, like senescence and the role it plays as an endogenous control phenomenon [4] . The list of institutions who are trying to get to grips with senescence biology and its impact on lab assays is long, such as those from the Framingham Study, the Leiden Longevity cohort or the Baltimore Longitudinal Study of Aging (BLSA) and the Berlin Aging Study to name but a few are studied.\n",
            "['<s>', 'Abstract', 'Reference', 'intervals', 'RIs', 'for', 'laboratory', 'analyses', 'by', 'and', 'large', 'are', 'provided', 'by', 'analytical', 'platform', 'providers', 'the', 'provenience', 'and', 'preanalytics', 'of', 'materials', 'for', 'the', 'calculation', 'of', 'intervals', 'often', 'remain', 'arcane', 'particularly', 'relating', 'to', 'the', 'age', 'group', 'of', 'donors', '.', 'In', 'an', 'observational', 'prospective', 'cohort', 'study', 'on', 'healthy', 'uniracial', 'Caucasian', 'residents', 'years', 'of', 'age', 'frequently', 'used', 'lab', 'tests', 'were', 'done', 'on', 'one', 'blood', 'sample', '.', 'With', 'a', 'nonrestrictive', 'definition', 'of', 'health', 'several', 'pathological', 'lab', 'results', 'pointing', 'to', 'occult', 'disease', 'have', 'been', 'found', 'and', 'published', 'from', 'SENIORLAB', 'so', 'far', '.', 'The', 'RIs', 'found', 'for', 'hemoglobin', 'in', 'women', 'went', 'from', '.', 'to', '.', 'g', 'L', 'years', 'and', 'in', 'men', 'from', '.', 'to', '.', 'g', 'L', 'confidence', 'interval', 'CI', '.', 'This', 'article', 'lists', 'RIs', 'computed', 'with', 'SENIORLAB', 'data', 'for', 'such', 'frequently', 'ordered', 'analyses', 'as', 'platelet', 'counts', 'vitamin', 'B', 'and', 'folate', 'ferritin', 'and', 'analytes', 'measured', 'to', 'estimate', 'metabolic', 'performance', 'in', 'glucose', 'turnover', '.', 'In', 'fact', '.', 'of', 'the', 'cohort', 'showed', 'prediabetic', 'fasting', 'plasma', 'glucose', 'FPG', 'and', 'or', 'glycated', 'hemoglobin', 'HbA', 'c', 'total', 'serum', 'folate', 'levels', 'but', 'not', 'red', 'blood', 'cell', 'folate', 'decreased', 'with', 'progressing', 'age', '.', 'As', 'much', 'as', 'of', 'evaluable', 'study', 'participants', 'had', 'insufficient', 'levels', 'of', 'OH', 'vitamin', 'D', '.', 'Published', 'reports', 'from', 'SENIORLAB', 'are', 'referenced', 'in', 'this', 'article', '.', 'Ten', 'years', 'ago', 'the', 'SENIORLAB', 'study', 'was', 'designed', 'to', 'establish', 'reference', 'intervals', 'RIs', 'of', 'current', 'medical', 'lab', 'assays', 'suitable', 'for', 'validation', 'performed', 'in', 'elderly', 'patients', '.', 'At', 'the', 'onset', 'and', 'upon', 'immediate', 'evaluation', 'the', 'RIs', 'were', 'computed', 'on', 'cross', 'comparative', 'data', 'sets', 'usually', 'grouping', 'ages', 'from', 'to', 'to', 'and', 'above', 'years', 'of', 'age', '.', 'The', 'study', 'was', 'concluded', 'with', 'RIs', 'of', 'most', 'lab', 'analyses', 'performed', '.', 'Questionnaires', 'were', 'sent', 'to', 'surviving', 'participants', 'in', 'an', 'endeavor', 'to', 'relate', 'quality', 'of', 'remaining', 'life', 'years', 'to', 'lab', 'values', 'determined', 'at', 'the', 'onset', 'of', 'disease', '.', 'Here', 'we', 'report', 'part', 'of', 'the', 'results', 'in', 'the', 'context', 'of', 'geriatric', 'medicine', '.', 'Before', 'this', 'paper', 'goes', 'into', 'the', 'data', 'obtained', 'let', 'us', 'cite', 'Georges', 'Canguilhem', 'Canguilhem', 'who', 'tells', 'us', 'in', 'his', 'thesis', 'on', 'The', 'Normal', 'and', 'the', 'Pathological', 'M', 'decins', 'et', 'chirurgiens', 'ont', 'une', 'information', 'clinique', 'et', 'utilisent', 'aussi', 'parfois', 'des', 'techniques', 'de', 'laboratoire', 'qui', 'leur', 'permettent', 'de', 'savoir', 'malades', 'des', 'gens', 'qui', 'ne', 'se', 'sentent', 'pas', 'tels', 'English', 'translation', 'With', 'clinical', 'information', 'at', 'hand', 'doctors', 'and', 'surgeons', 'sometimes', 'use', 'lab', 'techniques', 'to', 'identify', 'persons', 'who', 'are', 'sick', 'without', 'signs', 'and', 'symptoms', '.', 'With', 'the', 'extension', 'of', 'the', 'human', 'life', 'span', 'the', 'laboratory', 'medicine', 'community', 'began', 'to', 'scrutinize', 'RIs', 'applicable', 'for', 'validating', 'assays', 'ordered', 'for', 'older', 'patients', '.', 'There', 'were', 'already', 'short', 'lists', 'of', 'analytes', 'available', 'the', 'RIs', 'of', 'which', 'substantially', 'deviate', 'from', 'work', 'force', 'age', 'groups', '.', 'It', 'has', 'long', 'been', 'known', 'that', 'sex', 'hormones', 'decline', 'and', 'a', 'number', 'of', 'parameters', 'have', 'joined', 'an', 'ever', 'growing', 'list', 'often', 'immediately', 'upon', 'initial', 'recognition', 'of', 'their', 'importance', 'in', 'the', 'improvement', 'of', 'diagnosis', '.', 'Aging', 'senescence', 'and', 'maturation', 'are', 'denominations', 'with', 'slightly', 'different', 'meanings', '.', 'Whereas', 'senescence', 'is', 'interpreted', 'as', 'endogenous', 'degenerative', 'processes', 'leading', 'to', 'death', 'aging', 'encompasses', 'a', 'wide', 'array', 'of', 'passive', 'or', 'nonregulated', 'degenerative', 'steps', 'determined', 'at', 'least', 'in', 'part', 'by', 'exogenous', 'factors', 'gradual', 'deterioration', 'with', 'aging', 'can', 'be', 'looked', 'at', 'as', 'a', 'fail', 'safe', 'system', 'with', 'collateral', 'turnover', 'that', 'is', 'prone', 'to', 'repair', '.', 'Involution', 'such', 'as', 'with', 'the', 'thymus', 'may', 'feign', 'senescence', 'and', 'age', 'related', 'decline', 'in', 'T', 'cell', 'output', 'is', 'a', 'risk', 'factor', 'for', 'many', 'cancer', 'types', 'and', 'infectious', 'diseases', 'thymic', 'atrophy', 'which', 'starts', 'at', 'puberty', 'in', 'all', 'vertebrates', '.', 'When', 'we', 'submitted', 'the', 'study', 'protocol', 'of', 'SENIORLAB', 'to', 'the', 'competent', 'agencies', 'back', 'in', 'many', 'studies', 'had', 'already', 'addressed', 'the', 'question', 'of', 'specific', 'RIs', 'in', 'the', 'elderly', '.', 'The', 'summary', 'of', 'the', 'then', 'screened', 'literature', 'can', 'be', 'listed', 'as', 'is', 'shown', 'in', 'Table', '.', 'The', 'increasing', 'mean', 'age', 'of', 'the', 'world', 's', 'population', 'concerns', 'all', 'those', 'survivors', 'of', 'accidental', 'death', 'unruffled', 'by', 'senescent', 'biology', '.', 'That', 'there', 'are', 'differences', 'in', 'the', 'causes', 'of', 'death', 'among', 'species', 'and', 'ultimately', 'among', 'different', 'human', 'races', 'and', 'different', 'individuals', 'was', 'made', 'evident', 'from', 'studies', 'on', 'the', 'survivor', 'patterns', 'for', 'cohorts', 'of', 'organisms', '.', 'Pearl', 'and', 'Miner', 'described', 'several', 'possible', 'types', 'of', 'survivor', 'curves', '.', 'In', 'Figure', 'curve', 'A', 'is', 'characteristic', 'of', 'a', 'population', 'in', 'which', 'death', 'is', 'predominantly', 'caused', 'by', 'disaster', 'predation', 'or', 'other', 'random', 'external', 'selection', 'forces', '.', 'The', 'constant', 'rate', 'of', 'removal', 'results', 'in', 'a', 'survivor', 'curve', 'that', 'drops', 'exponentially', 'gradually', 'approaching', 'zero', 'mortality', 'of', 'per', 'unit', 'of', 'time', '.', 'The', 'nonaligned', 'pattern', 'in', 'B', 'denotes', 'an', 'initially', 'strong', 'surviving', 'population', 'with', 'increased', 'susceptibility', 'to', 'death', 'with', 'age', 'in', 'the', 'absence', 'of', 'compromising', 'influences', 'of', 'any', 'type', 'whereas', 'the', 'shape', 'C', 'shows', 'a', 'survival', 'curve', 'from', 'a', 'population', 'with', 'abrupt', 'termination', 'of', 'all', 'members', 'by', 'natural', 'processes', 'like', 'senescence', 'and', 'the', 'role', 'it', 'plays', 'as', 'an', 'endogenous', 'control', 'phenomenon', '.', 'The', 'list', 'of', 'institutions', 'who', 'are', 'trying', 'to', 'get', 'to', 'grips', 'with', 'senescence', 'biology', 'and', 'its', 'impact', 'on', 'lab', 'assays', 'is', 'long', 'such', 'as', 'those', 'from', 'the', 'Framingham', 'Study', 'the', 'Leiden', 'Longevity', 'cohort', 'or', 'the', 'Baltimore', 'Longitudinal', 'Study', 'of', 'Aging', 'BLSA', 'and', 'the', 'Berlin', 'Aging', 'Study', 'to', 'name', 'but', 'a', 'few', 'are', 'studied', '.', '</s>']\n",
            "[1, 469, 12651, 6787, 12652, 105, 10301, 104, 65, 47, 59, 40, 1156, 65, 9173, 6182, 12653, 19, 12654, 47, 12655, 43, 10863, 105, 19, 5473, 43, 6787, 905, 3218, 12656, 513, 4332, 61, 19, 1295, 227, 43, 12657, 15, 263, 7, 6890, 3661, 980, 81, 57, 3300, 12658, 494, 6913, 267, 43, 1295, 171, 210, 3961, 761, 505, 5145, 57, 733, 9487, 1011, 15, 2255, 27, 12659, 3763, 43, 1572, 570, 8419, 3961, 51, 12660, 61, 10099, 8003, 167, 168, 618, 47, 3134, 18, 12661, 127, 640, 15, 135, 12652, 618, 105, 12048, 10, 190, 2302, 18, 15, 61, 15, 5479, 6662, 267, 47, 10, 132, 18, 15, 61, 15, 5479, 6662, 4839, 2114, 3198, 15, 4, 5, 5796, 12652, 6415, 363, 12661, 17, 105, 159, 171, 9888, 104, 36, 9687, 10145, 11593, 3778, 47, 11643, 12662, 47, 12663, 759, 61, 26, 8837, 102, 10, 11090, 1147, 15, 263, 176, 15, 43, 19, 980, 625, 12664, 11511, 9664, 11090, 11512, 47, 217, 12665, 12048, 12049, 2102, 2053, 9551, 11643, 1190, 326, 63, 6666, 9487, 6931, 11643, 1374, 363, 5601, 1295, 15, 443, 404, 36, 43, 12666, 81, 637, 652, 4292, 1190, 43, 12476, 11593, 1839, 15, 10535, 624, 18, 12661, 40, 12667, 10, 264, 5, 15, 12668, 267, 5104, 19, 12661, 81, 500, 1910, 61, 2607, 4436, 6787, 12652, 43, 485, 8212, 3961, 9553, 6765, 105, 4474, 775, 10, 8491, 8108, 15, 1424, 19, 2474, 47, 517, 7849, 3491, 19, 12652, 505, 6415, 57, 641, 4277, 17, 3154, 2207, 2241, 1128, 18, 61, 61, 47, 633, 267, 43, 1295, 15, 135, 81, 500, 370, 363, 12652, 43, 541, 3961, 104, 775, 15, 12669, 505, 5511, 61, 9984, 637, 10, 7, 3090, 61, 1060, 196, 43, 327, 808, 267, 61, 3961, 705, 2107, 403, 19, 2474, 43, 8003, 15, 2198, 25, 376, 1735, 43, 19, 51, 10, 19, 413, 43, 12153, 4713, 15, 12670, 264, 912, 6523, 978, 19, 17, 213, 7793, 296, 7567, 12671, 12672, 12672, 351, 6860, 296, 10, 4033, 12673, 57, 135, 8125, 47, 19, 11775, 587, 12674, 381, 12675, 12676, 12677, 966, 12678, 381, 12679, 12680, 12681, 2876, 5691, 5199, 12682, 12683, 12684, 12685, 5199, 12686, 12687, 2876, 12688, 12683, 12689, 3536, 12690, 12691, 12692, 3817, 12121, 2255, 7979, 966, 403, 869, 12693, 47, 12694, 3281, 306, 3961, 5691, 61, 1138, 2501, 351, 40, 12695, 2188, 12696, 47, 8242, 15, 2255, 19, 4205, 43, 19, 1468, 808, 4620, 19, 10301, 4713, 855, 3496, 61, 12697, 12652, 1923, 105, 12698, 9553, 9888, 105, 2400, 8108, 15, 182, 505, 2322, 566, 5796, 43, 12663, 1180, 19, 12652, 43, 389, 179, 12699, 18, 358, 191, 1295, 110, 15, 174, 53, 1354, 168, 480, 33, 1922, 11907, 1588, 47, 27, 459, 43, 222, 167, 6792, 7, 1371, 1778, 5081, 905, 2517, 517, 1930, 2575, 43, 72, 96, 10, 19, 5727, 43, 4566, 15, 8816, 9423, 47, 9062, 40, 12700, 363, 1745, 220, 11273, 15, 2438, 9423, 58, 3525, 36, 3470, 8305, 952, 5926, 61, 3558, 8412, 5974, 27, 476, 5662, 43, 2459, 217, 12701, 8305, 5586, 2107, 403, 1335, 10, 1735, 65, 3559, 378, 11175, 10240, 363, 8412, 285, 242, 5409, 403, 36, 27, 5100, 6072, 2731, 363, 12702, 1147, 33, 58, 7534, 61, 1908, 15, 12703, 159, 36, 363, 19, 12704, 241, 12705, 9423, 47, 1295, 804, 1588, 10, 6364, 6931, 6547, 58, 27, 2191, 9, 105, 670, 9756, 1449, 47, 10919, 6059, 12706, 8027, 389, 8252, 403, 12707, 10, 133, 12708, 15, 834, 25, 6264, 19, 81, 8092, 43, 12661, 61, 19, 12709, 5370, 5471, 10, 670, 117, 652, 2322, 1786, 19, 384, 43, 426, 12652, 10, 19, 8491, 15, 135, 4366, 43, 19, 388, 9771, 173, 285, 242, 6825, 36, 58, 472, 10, 940, 15, 135, 1275, 2063, 1295, 43, 19, 4677, 88, 208, 922, 133, 406, 12710, 43, 12711, 3558, 12712, 65, 12713, 4476, 15, 1866, 371, 40, 157, 10, 19, 3148, 43, 3558, 876, 9737, 47, 4632, 876, 220, 1468, 5455, 47, 220, 299, 500, 870, 3898, 18, 117, 57, 19, 12714, 1555, 105, 2396, 43, 12715, 15, 7169, 47, 12716, 2184, 570, 841, 1449, 43, 12714, 6863, 15, 263, 865, 3921, 646, 58, 1884, 43, 27, 208, 10, 389, 3558, 58, 1718, 7058, 65, 6992, 12717, 217, 528, 1151, 308, 312, 1528, 15, 135, 5047, 1587, 43, 8549, 51, 10, 27, 12714, 3921, 33, 6496, 12718, 9097, 4489, 5949, 9428, 43, 1617, 1982, 43, 188, 15, 135, 12719, 888, 10, 3778, 11491, 7, 1980, 54, 9984, 208, 363, 74, 7294, 61, 3558, 363, 1295, 10, 19, 3568, 43, 12720, 830, 43, 1209, 1059, 558, 19, 1761, 6311, 868, 27, 9659, 3921, 18, 27, 208, 363, 12721, 9797, 43, 133, 2376, 65, 1635, 952, 2321, 9423, 47, 19, 412, 194, 475, 36, 7, 3470, 474, 1281, 15, 135, 5081, 43, 1372, 351, 40, 12722, 61, 1176, 61, 12723, 363, 9423, 4476, 47, 1237, 377, 57, 3961, 9553, 58, 1354, 159, 36, 406, 18, 19, 12544, 23, 19, 10485, 12724, 980, 217, 19, 2523, 22, 23, 43, 8816, 9105, 47, 19, 4107, 8816, 23, 61, 12725, 326, 27, 647, 40, 1305, 15, 2]\n",
            "Abstract Reference intervals RIs for laboratory analyses by and large are provided by analytical platform providers the provenience and preanalytics of materials for the calculation of intervals often remain arcane particularly relating to the age group of donors . In an observational prospective cohort study on healthy uniracial Caucasian residents years of age frequently used lab tests were done on one blood sample . With a nonrestrictive definition of health several pathological lab results pointing to occult disease have been found and published from SENIORLAB so far . The RIs found for hemoglobin in women went from . to . g L years and in men from . to . g L confidence interval CI . This article lists RIs computed with SENIORLAB data for such frequently ordered analyses as platelet counts vitamin B and folate ferritin and analytes measured to estimate metabolic performance in glucose turnover . In fact . of the cohort showed prediabetic fasting plasma glucose FPG and or glycated hemoglobin HbA c total serum folate levels but not red blood cell folate decreased with progressing age . As much as of evaluable study participants had insufficient levels of OH vitamin D . Published reports from SENIORLAB are referenced in this article . Ten years ago the SENIORLAB study was designed to establish reference intervals RIs of current medical lab assays suitable for validation performed in elderly patients . At the onset and upon immediate evaluation the RIs were computed on cross comparative data sets usually grouping ages from to to and above years of age . The study was concluded with RIs of most lab analyses performed . Questionnaires were sent to surviving participants in an endeavor to relate quality of remaining life years to lab values determined at the onset of disease . Here we report part of the results in the context of geriatric medicine . Before this paper goes into the data obtained let us cite Georges Canguilhem Canguilhem who tells us in his thesis on The Normal and the Pathological M decins et chirurgiens ont une information clinique et utilisent aussi parfois des techniques de laboratoire qui leur permettent de savoir malades des gens qui ne se sentent pas tels English translation With clinical information at hand doctors and surgeons sometimes use lab techniques to identify persons who are sick without signs and symptoms . With the extension of the human life span the laboratory medicine community began to scrutinize RIs applicable for validating assays ordered for older patients . There were already short lists of analytes available the RIs of which substantially deviate from work force age groups . It has long been known that sex hormones decline and a number of parameters have joined an ever growing list often immediately upon initial recognition of their importance in the improvement of diagnosis . Aging senescence and maturation are denominations with slightly different meanings . Whereas senescence is interpreted as endogenous degenerative processes leading to death aging encompasses a wide array of passive or nonregulated degenerative steps determined at least in part by exogenous factors gradual deterioration with aging can be looked at as a fail safe system with collateral turnover that is prone to repair . Involution such as with the thymus may feign senescence and age related decline in T cell output is a risk factor for many cancer types and infectious diseases thymic atrophy which starts at puberty in all vertebrates . When we submitted the study protocol of SENIORLAB to the competent agencies back in many studies had already addressed the question of specific RIs in the elderly . The summary of the then screened literature can be listed as is shown in Table . The increasing mean age of the world s population concerns all those survivors of accidental death unruffled by senescent biology . That there are differences in the causes of death among species and ultimately among different human races and different individuals was made evident from studies on the survivor patterns for cohorts of organisms . Pearl and Miner described several possible types of survivor curves . In Figure curve A is characteristic of a population in which death is predominantly caused by disaster predation or other random external selection forces . The constant rate of removal results in a survivor curve that drops exponentially gradually approaching zero mortality of per unit of time . The nonaligned pattern in B denotes an initially strong surviving population with increased susceptibility to death with age in the absence of compromising influences of any type whereas the shape C shows a survival curve from a population with abrupt termination of all members by natural processes like senescence and the role it plays as an endogenous control phenomenon . The list of institutions who are trying to get to grips with senescence biology and its impact on lab assays is long such as those from the Framingham Study the Leiden Longevity cohort or the Baltimore Longitudinal Study of Aging BLSA and the Berlin Aging Study to name but a few are studied .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lPbDN736FF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Amnbpfd3YA-"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import tqdm\n",
        "import nltk\n",
        "from google.colab import files\n",
        "class SingleTurnMovieDialog_dataset(Dataset):\n",
        "    \"\"\"Single-Turn version of Cornell Movie Dialog Cropus dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, conversations, vocab, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            conversations: list of tuple (src_string, tgt_string) \n",
        "                         - src_string: String of the source sentence\n",
        "                         - tgt_string: String of the target sentence\n",
        "            vocab: Vocabulary object that contains the mapping of \n",
        "                    words to indices\n",
        "            device: cpu or cuda\n",
        "        \"\"\"\n",
        "        self.conversations = conversations\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "\n",
        "        def encode(src, tgt):\n",
        "            src_ids = self.vocab.get_ids_from_sentence(src)\n",
        "            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n",
        "            return (src_ids, tgt_ids)\n",
        "\n",
        "        # We will pre-tokenize the conversations and save in id lists for later use\n",
        "        self.tokenized_conversations = [encode(src, tgt) for src, tgt in self.conversations]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.conversations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return {\"conv_ids\":self.tokenized_conversations[idx], \"conv\":self.conversations[idx]}\n",
        "\n",
        "def collate_fn(data):\n",
        "    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, trg_seq).\n",
        "    We should build a custom collate_fn rather than using default collate_fn,\n",
        "    because merging sequences (including padding) is not supported in default.\n",
        "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
        "    Args:\n",
        "        data: list of dicts {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, trg_str)}.\n",
        "            - src_ids: list of src piece ids; variable length.\n",
        "            - tgt_ids: list of tgt piece ids; variable length.\n",
        "            - src_str: String of src\n",
        "            - tgt_str: String of tgt\n",
        "    Returns: dict { \"conv_ids\":     (src_ids, tgt_ids), \n",
        "                    \"conv\":         (src_str, tgt_str), \n",
        "                    \"conv_tensors\": (src_seqs, tgt_seqs)}\n",
        "            src_seqs: torch tensor of shape (src_padded_length, batch_size).\n",
        "            trg_seqs: torch tensor of shape (tgt_padded_length, batch_size).\n",
        "            src_padded_length = length of the longest src sequence from src_ids\n",
        "            tgt_padded_length = length of the longest tgt sequence from tgt_ids\n",
        "    \"\"\"\n",
        "    # Sort conv_ids based on decreasing order of the src_lengths.\n",
        "    # This is required for efficient GPU computations.\n",
        "    src_ids = [torch.LongTensor(e[\"conv_ids\"][0]) for e in data]\n",
        "    tgt_ids = [torch.LongTensor(e[\"conv_ids\"][1]) for e in data]\n",
        "    src_str = [e[\"conv\"][0] for e in data]\n",
        "    tgt_str = [e[\"conv\"][1] for e in data]\n",
        "    data = list(zip(src_ids, tgt_ids, src_str, tgt_str))\n",
        "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    src_ids, tgt_ids, src_str, tgt_str = zip(*data)\n",
        "    src_seqs = nn.utils.rnn.pad_sequence(src_ids, padding_value=pad_id)\n",
        "    tgt_seqs = nn.utils.rnn.pad_sequence(tgt_ids, padding_value=pad_id)\n",
        "\n",
        "    # Pad the src_ids and tgt_ids using token pad_id to create src_seqs and tgt_seqs\n",
        "    \n",
        "    # Implementation tip: You can use the nn.utils.rnn.pad_sequence utility\n",
        "    # function to combine a list of variable-length sequences with padding.\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    ...\n",
        "\n",
        "    return {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, tgt_str), \"conv_tensors\":(src_seqs.to(device), tgt_seqs.to(device))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjVvDA2NPNMC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "random.seed(10)\n",
        "text, test = train_test_split(text, test_size=0.3, random_state=10)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9wvUpNR438P"
      },
      "source": [
        "dataset = SingleTurnMovieDialog_dataset(text, vocab, device)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
        "                               shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU9qfBI15Rwr"
      },
      "source": [
        "import torch.nn as nn\n",
        "class Seq2seqBaseline(nn.Module):\n",
        "    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize your model's parameters here. To get started, we suggest\n",
        "        # setting all embedding and hidden dimensions to 300, using encoder and\n",
        "        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n",
        "\n",
        "        # Implementation tip: To create a bidirectional GRU, you don't need to\n",
        "        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n",
        "        \n",
        "        self.num_words = vocab.num_words\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.E = nn.Embedding(self.num_words, emb_dim)\n",
        "\n",
        "        self.encoder = nn.GRU(emb_dim, hidden_dim, 2, dropout=dropout, bidirectional=True)\n",
        "        self.decoder = nn.GRU(emb_dim, hidden_dim, 2, dropout=dropout, bidirectional=False)\n",
        "        self.lin = nn.Linear(emb_dim, vocab.num_words)\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def encode(self, source):\n",
        "        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_src_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                encoder_output: The output hidden representation of the encoder \n",
        "                    with shape (max_src_sequence_length, batch_size, hidden_size).\n",
        "                    Can be obtained by adding the hidden representations of both \n",
        "                    directions of the encoder bidirectional GRU. \n",
        "                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n",
        "                    batch_size) indicating which encoder outputs correspond to padding\n",
        "                    tokens. Its elements should be True at positions corresponding to\n",
        "                    padding tokens and False elsewhere.\n",
        "                encoder_hidden: The final hidden states of the bidirectional GRU \n",
        "                    (after a suitable projection) that will be used to initialize \n",
        "                    the decoder. This should be a tensor h_n with shape \n",
        "                    (num_layers, batch_size, hidden_size). Note that the hidden \n",
        "                    state returned by the bi-GRU cannot be used directly. Its \n",
        "                    initial dimension is twice the required size because it \n",
        "                    contains state from two directions.\n",
        "\n",
        "        The first two return values are not required for the baseline model and will\n",
        "        only be used later in the attention model. If desired, they can be replaced\n",
        "        with None for the initial implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: consider using packed sequences to more easily work\n",
        "        # with the variable-length sequences represented by the source tensor.\n",
        "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "\n",
        "        # Implementation tip: there are many simple ways to combine the forward\n",
        "        # and backward portions of the final hidden state, e.g. addition, averaging,\n",
        "        # or a linear transformation of the appropriate size. Any of these\n",
        "        # should let you reach the required performance.\n",
        "\n",
        "        # Compute a tensor containing the length of each source sequence.\n",
        "\n",
        "        hid = torch.zeros(4, source.shape[1], 300).cuda()\n",
        "        encoder_mask = source != 0\n",
        "        x = self.E(source)\n",
        "        x, hid = self.encoder(x, hid)\n",
        "\n",
        "        x = x[:,:,:self.hidden_dim] + x [:,:,self.hidden_dim:]\n",
        "        hid = hid[:self.num_layers,:,:] + hid[self.num_layers:,:,:]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        return  x, encoder_mask, hid\n",
        "\n",
        "    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n",
        "        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n",
        "\n",
        "        The third and fourth arguments are not used in the baseline model, but are\n",
        "        included for compatibility with the attention model in the next section.\n",
        "\n",
        "        Args:\n",
        "            decoder_input: An integer tensor with shape (1, batch_size) containing \n",
        "                the subword indices for the current decoder input.\n",
        "            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n",
        "                state of the decoder, each with shape (num_layers, batch_size,\n",
        "                hidden_size). For the first decoding step the last_hidden will be \n",
        "                encoder's final hidden representation.\n",
        "            encoder_output: The output of the encoder with shape\n",
        "                (max_src_sequence_length, batch_size, hidden_size).\n",
        "            encoder_mask: The output mask from the encoder with shape\n",
        "                (max_src_sequence_length, batch_size). Encoder outputs at positions\n",
        "                with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                logits: A tensor with shape (batch_size,\n",
        "                    vocab_size) containing unnormalized scores for the next-word\n",
        "                    predictions at each position.\n",
        "                decoder_hidden: tensor h_n with the same shape as last_hidden \n",
        "                    representing the updated decoder state after processing the \n",
        "                    decoder input.\n",
        "                attention_weights: This will be implemented later in the attention\n",
        "                    model, but in order to maintain compatible type signatures, we also\n",
        "                    include it here. This can be None or any other placeholder value.\n",
        "        \"\"\"\n",
        "\n",
        "        # These arguments are not used in the baseline model.\n",
        "        del encoder_output\n",
        "        del encoder_mask\n",
        "        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        x = self.E(decoder_input)\n",
        "        x, hid = self.decoder(x, last_hidden)\n",
        "        x = self.lin(torch.squeeze(x, 0))\n",
        "\n",
        "\n",
        "        return x, hid, 0\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "\n",
        "    def compute_loss(self, source, target):\n",
        "        \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_source_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "            target: An integer tensor with shape (max_target_sequence_length,\n",
        "                batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "        Returns:\n",
        "            A scalar float tensor representing cross-entropy loss on the current batch\n",
        "            divided by the number of target tokens in the batch.\n",
        "            Many of the target tokens will be pad tokens. You should mask the loss \n",
        "            from these tokens using appropriate mask on the target tokens loss.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: don't feed the target tensor directly to the decoder.\n",
        "        # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
        "        # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "        # suffix A B C </s>.\n",
        "        x, mask, hid = self.encode(source)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        loss = 0\n",
        "        for i in range(target.shape[0]- 1):\n",
        "          out, hid, temp = self.decode(target[i], hid, x, mask)\n",
        "\n",
        "          loss += loss_func(out, target[i + 1])\n",
        "        return loss/target.shape[0]\n",
        "\n",
        "        # You may run self.encode() on the source only once and decode the target \n",
        "        # one step at a time.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ..."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSCyajSuRWBC"
      },
      "source": [
        "def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n",
        "    \"\"\"Train the model for given number of epochs and save the trained model in \n",
        "    the final model_file.\n",
        "    \"\"\"\n",
        "\n",
        "    decoder_learning_ratio = 5.0\n",
        "    \n",
        "    encoder_parameter_names = [\"E\", \"encoder\"]\n",
        "                               \n",
        "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    encoder_params = [e[1] for e in encoder_named_params]\n",
        "    decoder_params = [e[1] for e in decoder_named_params]\n",
        "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
        "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
        "    \n",
        "    clip = 50.0\n",
        "    for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
        "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
        "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
        "        with tqdm.notebook.tqdm(\n",
        "                data_loader,\n",
        "                desc=\"epoch {}\".format(epoch + 1),\n",
        "                unit=\"batch\",\n",
        "                total=len(data_loader)) as batch_iterator:\n",
        "            model.train()\n",
        "            total_loss = 0.0\n",
        "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
        "                source, target = batch_data[\"conv_tensors\"]\n",
        "                optimizer.zero_grad()\n",
        "                loss = model.compute_loss(source, target)\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                # Gradient clipping before taking the step\n",
        "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
        "    # Save the model after training         \n",
        "    torch.save(model.state_dict(), model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "2a0feddd6747414e8572c1eff721328a",
            "d416ae261a99422684b3a49123edea5e",
            "b755f66966bb41ccaeba031f3d47ea7e",
            "d04b8b2df6c34403a9c3d97dd3ed6d73",
            "11022d0803a148f5a90df1f077c44b90",
            "374ea2ff10114be988fc493be600bebb",
            "c5e9e057fc3b44a082339f1c65643bba",
            "c208a10fbaf14da283b832b2fbe2191c",
            "37e351c02c8744ca920550a01dddb368",
            "720ae00932c54266a1b4253b6b78660b",
            "2da511e144f740069ca53f3ae0457bd5",
            "b2153bdd674942c7a6c1cb4f42579af8",
            "7a745b13b98249b0b7426a7bfa01483d",
            "604b6f56ecec496f8b3a6d23d290bf79",
            "e1cc6d5714a94b0396d60f36787c2d00",
            "ec47be82a380401baddc14b098bfb1d3"
          ]
        },
        "id": "qjKuy2ZfejW9",
        "outputId": "a6f37b2a-6b83-4163-b1d5-757a5cb00bef"
      },
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 1\n",
        "batch_size = 10\n",
        "# Reloading the data_loader to increase batch_size\n",
        "dataset = SingleTurnMovieDialog_dataset(text[0:10], vocab, device)\n",
        "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
        "                               shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "train(baseline_model, data_loader, num_epochs, \"baseline_model.pt\")\n",
        "# Download the trained model to local for future use\n",
        "#files.download('baseline_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a0feddd6747414e8572c1eff721328a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='training', max=1.0, style=ProgressStyle(description_widthâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37e351c02c8744ca920550a01dddb368",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='epoch 1', max=1.0, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajxvKsfTygWH",
        "outputId": "7d6fbe17-e798-4a59-da66-ef1aa49cbbc7"
      },
      "source": [
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "print(vocab.num_words)\n",
        "print(baseline_model.E.weight.shape)\n",
        "\n",
        "baseline_model.load_state_dict(torch.load(\"baseline_model_new.pt\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56347\n",
            "torch.Size([56347, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzbdU3bu6b0M",
        "outputId": "e16843d7-b440-4538-e235-5fd11392a231"
      },
      "source": [
        "def predict_greedy(model, sentence, max_length=100):\n",
        "    \"\"\"Make predictions for the given input using greedy inference.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: A input string.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        Model's predicted greedy response for the input, represented as string.\n",
        "    \"\"\"\n",
        "\n",
        "    # You should make only one call to model.encode() at the start of the function, \n",
        "    # and make only one call to model.decode() per inference step.\n",
        "    model.eval()\n",
        "    source = torch.unsqueeze(torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    sent = [start]\n",
        "    i = 0\n",
        "    while start != eos_id and i < 100:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "          start = torch.argmax(out[0], 0)\n",
        "          sent.append(start.item())\n",
        "          i += 1\n",
        "    sent = vocab.decode_sentence_from_ids(sent)\n",
        "    \n",
        "    return sent\n",
        "print(predict_greedy(baseline_model, text[2][0]))\n",
        "print(text[2][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baltimore longitudinal study of aging\n",
            "baltimore longitudinal study of aging blsa \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtEc6K0786Jy"
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-OjVm4K55s"
      },
      "source": [
        "def predict_beam(model, sentence, k=3, max_length=100, thresh=-9999):\n",
        "    \"\"\"Make predictions for the given inputs using beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: An input sentence, represented as string.\n",
        "        k: The size of the beam.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        A list of k beam predictions. Each element in the list should be a string\n",
        "        corresponding to one of the top k predictions for the corresponding input,\n",
        "        sorted in descending order by its final score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: once an eos_token has been generated for any beam, \n",
        "    # remove its subsequent predictions from that beam by adding a small negative \n",
        "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
        "    # candidates are removed from the beam, as its probability will be very close\n",
        "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
        "    # finished candidate\n",
        "\n",
        "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
        "    # constant for simplicity (aside from the sequence length), some special care\n",
        "    # will need to be taken on the first iteration to ensure that your beam\n",
        "    # doesn't fill up with k identical copies of the same candidate.\n",
        "    \n",
        "    # You are welcome to tweak alpha\n",
        "    alpha = 0.9\n",
        "    model.eval()\n",
        "    beams = []\n",
        "    curr = []\n",
        "    source = torch.unsqueeze(torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    \n",
        "    out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "    out = torch.log_softmax(out[0], 0)\n",
        "    values, start = torch.topk(out, k, 0)\n",
        "    for i in range(len(values)):\n",
        "      # Each beam contains the log probs at its first index and the hidden states at its last index\n",
        "      beams.append([values[i], start[i].item(), hid])\n",
        "    \n",
        "    generation = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "      curr = []\n",
        "      for j in beams:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(j[-2]).cuda(), 0), j[-1], x, mask)\n",
        "          out = torch.log_softmax(out[0], 0)\n",
        "          values, start = torch.topk(out, k, 0)\n",
        "          for z in range(len(values)):\n",
        "            temp = j.copy()\n",
        "            temp[0] = values[z] + temp[0]\n",
        "            temp.insert(-1, start[z].item())\n",
        "            temp[-1] = hid\n",
        "            curr.append(temp)\n",
        "      curr = sorted(curr,reverse=True, key=lambda x: x[0])\n",
        "      curr = curr[0:k - i]\n",
        "      beams = []\n",
        "      for j in curr:\n",
        "        if j[-2] == eos_id or len(j) > 20:\n",
        "          generation.append(j[:-1])\n",
        "          i +=1\n",
        "        else:\n",
        "          beams.append(j)\n",
        "    final = []\n",
        "    generation = sorted(generation, reverse=True, key=lambda x: x[0]/(len(x)-1)**alpha)\n",
        "    for i in generation:\n",
        "      if i[0].item() > thresh:\n",
        "        final.append(vocab.decode_sentence_from_ids(i[1:]))\n",
        "    return final\n",
        "\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    ..."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKkgBuu15R7n"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(test)):\n",
        "  if test[i][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, test[i][0], thresh=-2)\n",
        "    testing[test[i][0]] = (predictions, [test[i][1]])\n",
        "  else:\n",
        "    testing[test[i][0]][1].append(test[i][1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGe1YTnn6CU4"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  cop = predictions.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in predictions:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQny3KEV7k7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd94b163-3d91-4afd-df64-787738b6041f"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "952\n",
            "229\n",
            "0.6328877836493627\n",
            "accuracy\n",
            "0.8163592622293504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPjfZbV_hLS"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(text)):\n",
        "  if text[i][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, text[i][0], thresh=-2)\n",
        "    testing[text[i][0]] = (predictions, [text[i][1]])\n",
        "  else:\n",
        "    testing[text[i][0]][1].append(text[i][1])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpeH0f2q_1R8"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  cop = predictions.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in predictions:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MghPTSy6_3fa",
        "outputId": "be42b529-5ce4-4f9d-d6a6-8148d09581ec"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "1740\n",
            "539\n",
            "0.675217329343024\n",
            "accuracy\n",
            "0.8146492434662999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO-tUxpVazUO",
        "outputId": "993e7078-b38f-4bde-8fc6-fb70bd34e891"
      },
      "source": [
        "score = 0\n",
        "for j in range(0,len(text)):\n",
        "  predictions = predict_beam(baseline_model, text[j][0])\n",
        "  for i in predictions:\n",
        "    if jaccard(text[j][1], i) >= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"training_accuracy\")\n",
        "print(score/len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_accuracy\n",
            "0.8803301237964236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdQqe_4quTAb"
      },
      "source": [
        "Tried testing on unfiltered dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD9KQMvrbKzd"
      },
      "source": [
        "import math\n",
        "test_unfilter = pd.read_csv(\"data_false.csv\")\n",
        "test_unfilter = test_unfilter[test_unfilter.text.notnull()].reset_index()\n",
        "text_2 = []\n",
        "for i in range(len(test_unfilter)):\n",
        "\n",
        "  t = test_unfilter.loc[i][7]\n",
        "\n",
        "  text_2.append((t, test_unfilter.loc[i][6]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StQPja5ddAo0",
        "outputId": "3ffc87e6-0461-4545-cb8c-6e87045759c1"
      },
      "source": [
        "\n",
        "print(text_2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' Federal Reserve Bank of Richmond S1. Accounting for low enrollment and graduation rates at four-year colleges when returns to graduation are high Several studies claimed that there is underinvestment in education. Judd (2000) combines Capital Asset Pricing Model (CAPM) techniques with the indivisibility of human capital to compare the return to four-year college graduation with assets of similar risk to find an excess of return to the college investment option. Heckman, Lochner, and Todd (2008) evaluate the internal rate of return of the four-year college investment option relative to work to find that since 1960, internal rates of return have been around 10 percent or higher, depending on the cohort and different specifications of labor markets and taxes. Cunha, Heckman, and Navarro (2005), using data from NLSY/1979, extend the analysis to evaluate the internal rate of return for the marginal student, the agent with the lowest observable measures of ability who enrolls in four-year college, to find an unexplained wedge in returns. 1 Cunha, Heckman, and Navarro (2005) concluded that this wedge is explained by nonpecuniary costs of education, namely, tastes for school, risk aversion, and other. The evidence presented in this paper points in a different direction: the wedge in returns is explained by the existence of academic two-year colleges (in fact, it is actually more general because vocational schools explain the wedge in returns for students who enroll in academic two-year colleges), as high school graduates sort across the different enrollment alternatives. In particular, high school graduates who enroll in fouryear colleges have measures of observables that lie above those who enroll in academic two-year colleges. It follows that the high school graduate with the lowest measures of observables who enrolls in a four-year college is indifferent to enrollment at academic two-year colleges as opposed to being indifferent to joining the labor force. Using the parameterized version of the model, it is possible to quantify the return for this marginal student and how much of average returns for the population that enrolls in four-year colleges is explained by the return of the marginal student. The return for the student Nicholas Trachter: nicholas.trachter@rich.frb.org\\nThe views expressed in this article are mine and do not necessarily represent the views of the Federal Reserve Bank of Richmond or the Federal Reserve System.', 'national education longitudinal study')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iaVYmdwciOr",
        "outputId": "8374996f-7bf5-4434-df1d-b306d567b589"
      },
      "source": [
        "score = 0\n",
        "for j in range(0,len(text_2), 50):\n",
        "  predictions = predict_beam(baseline_model, text_2[j][0])\n",
        "  for i in predictions:\n",
        "    if jaccard(text_2[j][1], i) >= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"training_accuracy\")\n",
        "print(score/(len(text_2)/50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_accuracy\n",
            "0.5281361002056459\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}