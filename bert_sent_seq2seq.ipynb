{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DataCleaning_(2)_(2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMUa4s5DQvc"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.autonotebook import tqdm\n",
        "from functools import partial\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "#!pip install transformers\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "#import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeX5_0w9SBLL",
        "outputId": "3ec2f8ad-dc61-42be-905b-353321822f88"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr 30 21:02:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NcQtlQS5EHb",
        "outputId": "7e887774-7ad9-4fda-ec4e-0c732f8c95f5"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9mUxiWiEh7F",
        "outputId": "edc84c99-9c8c-4dc6-dfa9-2314b3f668eb"
      },
      "source": [
        "#data = pd.read_csv(\"data.csv\")\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "data.head()\n",
        "print(len(data))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQEH7zrm9Az"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbBZ1pv1oEv"
      },
      "source": [
        "text = []\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6]\n",
        "\n",
        "\n",
        "  text.append((t, data.loc[i][5]))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxs0JLGXn-9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e182d04f-a7de-4979-a550-e4cc325b5b16"
      },
      "source": [
        "pad_word = \"<pad>\"\n",
        "bos_word = \"<s>\"\n",
        "eos_word = \"</s>\"\n",
        "unk_word = \"<unk>\"\n",
        "pad_id = 0\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "unk_id = 3\n",
        "    \n",
        "def normalize_sentence(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
        "        self.word_count = {}\n",
        "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
        "        self.num_words = 4\n",
        "    \n",
        "    def get_ids_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
        "                               else unk_id for word in sentence.split()] + \\\n",
        "                               [eos_id]\n",
        "        return sent_ids\n",
        "    \n",
        "    def tokenized_sentence(self, sentence):\n",
        "        sent_ids = self.get_ids_from_sentence(sentence)\n",
        "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
        "\n",
        "    def decode_sentence_from_ids(self, sent_ids):\n",
        "        words = list()\n",
        "        for i, word_id in enumerate(sent_ids):\n",
        "            if word_id in [bos_id, eos_id, pad_id]:\n",
        "                # Skip these words\n",
        "                continue\n",
        "            else:\n",
        "                words.append(self.id_to_word[word_id])\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def add_words_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word_to_id:\n",
        "                # add this word to the vocabulary\n",
        "                self.word_to_id[word] = self.num_words\n",
        "                self.id_to_word[self.num_words] = word\n",
        "                self.word_count[word] = 1\n",
        "                self.num_words += 1\n",
        "            else:\n",
        "                # update the word count\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for src, tgt in text:\n",
        "    vocab.add_words_from_sentence(src)\n",
        "    vocab.add_words_from_sentence(tgt)\n",
        "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in the vocabulary = 56347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAJMAVOUYPe",
        "outputId": "7cb72570-e03a-443b-f14a-2904afcf63d0"
      },
      "source": [
        "print(vocab.id_to_word[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7n2N3wn_QF"
      },
      "source": [
        "Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lPbDN736FF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv5P8QHaKeoN",
        "outputId": "bdc4a21d-0410-4348-9072-b87658eef7be"
      },
      "source": [
        "import spacy\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4dFP8RIKqTH"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDpk8FaZLF1M"
      },
      "source": [
        "sent = SentenceTransformer(\"facebook-dpr-ctx_encoder-single-nq-base\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9wUbRGULO4z"
      },
      "source": [
        "sent_sep = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7qpzdLLjad"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQLzkWefgWk"
      },
      "source": [
        "sent = sent.cuda()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giCBNRQzgDBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRSEghjdL9A"
      },
      "source": [
        "load embeddings manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSF-qZaqL8Pd"
      },
      "source": [
        "sent_emb = []\n",
        "for j in range(len(data)):\n",
        "  x = sent_sep(data.loc[j][6])\n",
        "  y = list(x.sents)\n",
        "  z = []\n",
        "  for i in y:\n",
        "    z.append(i.text)\n",
        "  y = sent.encode(z)\n",
        "  sent_emb.append(y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Lhxi2QkZdPll",
        "outputId": "cf677ab7-916f-4c6c-986e-2c15425e8872"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "      <th>text</th>\n",
              "      <th>mask</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This article investigates an important factor...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.4189563  -0.03409137 -0.18767388 ... -0.5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>81b5f874-9b42-4d4f-8c50-b4bab24f8aed</td>\n",
              "      <td>The Gender Gap Reloaded: Are School Characteri...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This study examines the wage gender gap of yo...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.13089143 -0.26295418 -0.5080046  ... -0.6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>f35fdfa6-a3f9-4ef3-a858-c8e2efcd2349</td>\n",
              "      <td>The High School Environment and the Gender Gap...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Despite the striking reversal of the gender g...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.41674316  0.20468165 -0.3782102  ... -0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>e5a4c6af-0854-4f7e-b088-06f03c18a01a</td>\n",
              "      <td>Locus of Control and Peer Relationships Among ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Abstract Past research has shown that locus o...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.07650556 -0.12806207 -0.14252882 ... -0.7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>d5ff0ecd-ccf5-4f8a-a09c-182376eed4d0</td>\n",
              "      <td>Beyond the Laboratory: Evaluating the Survey E...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>The black-white gap in achievement, as measur...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.12524939  0.05704077 -0.537129   ... -0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         embeddings\n",
              "0           0  ...  [[ 0.4189563  -0.03409137 -0.18767388 ... -0.5...\n",
              "1           1  ...  [[ 0.13089143 -0.26295418 -0.5080046  ... -0.6...\n",
              "2           2  ...  [[ 0.41674316  0.20468165 -0.3782102  ... -0.3...\n",
              "3           3  ...  [[ 0.07650556 -0.12806207 -0.14252882 ... -0.7...\n",
              "4           4  ...  [[ 0.12524939  0.05704077 -0.537129   ... -0.2...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efz7_q4bdbO-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD4w_J_SO6U8"
      },
      "source": [
        "labels = []\n",
        "for j in range(len(data)):\n",
        "  labels.append(data.loc[j][5])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3_WIQAn0dppH",
        "outputId": "1b052c4d-159d-457b-a427-e51610184805"
      },
      "source": [
        "labels[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'national education longitudinal study'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bps11Qq_c_oH"
      },
      "source": [
        "#data[\"embeddings\"] = sent_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swTSgVVdDSZ"
      },
      "source": [
        "#data.to_csv(\"data_with_embeddings.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWax-FY_dYae"
      },
      "source": [
        "sent_emb, test_x, labels, test_y = train_test_split(sent_emb, labels, test_size=0.3, random_state=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIHVhVjaM3qn"
      },
      "source": [
        "import torch.nn as nn\n",
        "class Seq2seqBaseline(nn.Module):\n",
        "    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize your model's parameters here. To get started, we suggest\n",
        "        # setting all embedding and hidden dimensions to 300, using encoder and\n",
        "        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n",
        "\n",
        "        # Implementation tip: To create a bidirectional GRU, you don't need to\n",
        "        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n",
        "        \n",
        "        self.num_words = vocab.num_words\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.E = nn.Embedding(self.num_words, emb_dim)\n",
        "\n",
        "        self.encoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=True)\n",
        "        self.decoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=False)\n",
        "        self.lin = nn.Linear(300, vocab.num_words)\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def encode(self, source):\n",
        "        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_src_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                encoder_output: The output hidden representation of the encoder \n",
        "                    with shape (max_src_sequence_length, batch_size, hidden_size).\n",
        "                    Can be obtained by adding the hidden representations of both \n",
        "                    directions of the encoder bidirectional GRU. \n",
        "                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n",
        "                    batch_size) indicating which encoder outputs correspond to padding\n",
        "                    tokens. Its elements should be True at positions corresponding to\n",
        "                    padding tokens and False elsewhere.\n",
        "                encoder_hidden: The final hidden states of the bidirectional GRU \n",
        "                    (after a suitable projection) that will be used to initialize \n",
        "                    the decoder. This should be a tensor h_n with shape \n",
        "                    (num_layers, batch_size, hidden_size). Note that the hidden \n",
        "                    state returned by the bi-GRU cannot be used directly. Its \n",
        "                    initial dimension is twice the required size because it \n",
        "                    contains state from two directions.\n",
        "\n",
        "        The first two return values are not required for the baseline model and will\n",
        "        only be used later in the attention model. If desired, they can be replaced\n",
        "        with None for the initial implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: consider using packed sequences to more easily work\n",
        "        # with the variable-length sequences represented by the source tensor.\n",
        "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "\n",
        "        # Implementation tip: there are many simple ways to combine the forward\n",
        "        # and backward portions of the final hidden state, e.g. addition, averaging,\n",
        "        # or a linear transformation of the appropriate size. Any of these\n",
        "        # should let you reach the required performance.\n",
        "\n",
        "        # Compute a tensor containing the length of each source sequence.\n",
        "\n",
        "        hid = torch.zeros(4, source.shape[1], 300).cuda()\n",
        "        encoder_mask = source != 0\n",
        "        x, hid = self.encoder(source, hid)\n",
        "\n",
        "        x = x[:,:,:self.hidden_dim] + x [:,:,self.hidden_dim:]\n",
        "        hid = hid[:self.num_layers,:,:] + hid[self.num_layers:,:,:]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        return  x, encoder_mask, hid\n",
        "\n",
        "    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n",
        "        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n",
        "\n",
        "        The third and fourth arguments are not used in the baseline model, but are\n",
        "        included for compatibility with the attention model in the next section.\n",
        "\n",
        "        Args:\n",
        "            decoder_input: An integer tensor with shape (1, batch_size) containing \n",
        "                the subword indices for the current decoder input.\n",
        "            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n",
        "                state of the decoder, each with shape (num_layers, batch_size,\n",
        "                hidden_size). For the first decoding step the last_hidden will be \n",
        "                encoder's final hidden representation.\n",
        "            encoder_output: The output of the encoder with shape\n",
        "                (max_src_sequence_length, batch_size, hidden_size).\n",
        "            encoder_mask: The output mask from the encoder with shape\n",
        "                (max_src_sequence_length, batch_size). Encoder outputs at positions\n",
        "                with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                logits: A tensor with shape (batch_size,\n",
        "                    vocab_size) containing unnormalized scores for the next-word\n",
        "                    predictions at each position.\n",
        "                decoder_hidden: tensor h_n with the same shape as last_hidden \n",
        "                    representing the updated decoder state after processing the \n",
        "                    decoder input.\n",
        "                attention_weights: This will be implemented later in the attention\n",
        "                    model, but in order to maintain compatible type signatures, we also\n",
        "                    include it here. This can be None or any other placeholder value.\n",
        "        \"\"\"\n",
        "\n",
        "        # These arguments are not used in the baseline model.\n",
        "        del encoder_output\n",
        "        del encoder_mask\n",
        "        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        x = sent.encode(vocab.id_to_word[decoder_input[0][0].item()])\n",
        "        x = torch.Tensor(x).unsqueeze(0).cuda()\n",
        "        x = x.unsqueeze(1)\n",
        " \n",
        "        x, hid = self.decoder(x, last_hidden)\n",
        "\n",
        "        x = self.lin(torch.squeeze(x, 0))\n",
        "\n",
        "\n",
        "        return x, hid, 0\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "\n",
        "    def compute_loss(self, source, target):\n",
        "        \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_source_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "            target: An integer tensor with shape (max_target_sequence_length,\n",
        "                batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "        Returns:\n",
        "            A scalar float tensor representing cross-entropy loss on the current batch\n",
        "            divided by the number of target tokens in the batch.\n",
        "            Many of the target tokens will be pad tokens. You should mask the loss \n",
        "            from these tokens using appropriate mask on the target tokens loss.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: don't feed the target tensor directly to the decoder.\n",
        "        # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
        "        # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "        # suffix A B C </s>.\n",
        "        x, mask, hid = self.encode(source)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss = 0\n",
        "        for i in range(target.shape[0]- 1):\n",
        "          out, hid, temp = self.decode(target[i], hid, x, mask)\n",
        "\n",
        "          loss += loss_func(out, target[i + 1])\n",
        "        return loss/target.shape[0]\n",
        "\n",
        "        # You may run self.encode() on the source only once and decode the target \n",
        "        # one step at a time.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ..."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZCmCIdNJ8N"
      },
      "source": [
        "def train(model, train, label, num_epochs, model_file, learning_rate=0.0001):\n",
        "    \"\"\"Train the model for given number of epochs and save the trained model in \n",
        "    the final model_file.\n",
        "    \"\"\"\n",
        "\n",
        "    decoder_learning_ratio = 5.0\n",
        "    \n",
        "    encoder_parameter_names = [\"E\", \"encoder\"]\n",
        "                               \n",
        "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    encoder_params = [e[1] for e in encoder_named_params]\n",
        "    decoder_params = [e[1] for e in decoder_named_params]\n",
        "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
        "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
        "    \n",
        "    clip = 50.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(epoch)\n",
        "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
        "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for i in range(len(train)):\n",
        "          if i == len(train)/2:\n",
        "            print(\"half_way\")\n",
        "          source = torch.Tensor(train[i]).unsqueeze(1).cuda()\n",
        "          tgt_ids = vocab.get_ids_from_sentence(label[i])\n",
        "          target = torch.LongTensor(tgt_ids).unsqueeze(1).cuda()\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.compute_loss(source, target)\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          # Gradient clipping before taking the step\n",
        "          _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "    # Save the model after training         \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcLWK6jZOboM",
        "outputId": "7a80b2b7-781e-4722-adbe-0258e9c81f43"
      },
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "\n",
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "train(baseline_model, sent_emb, labels, num_epochs, \"sent_emb_model.pt\")\n",
        "# Download the trained model to local for future use\n",
        "#files.download('baseline_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "half_way\n",
            "1\n",
            "half_way\n",
            "2\n",
            "half_way\n",
            "3\n",
            "half_way\n",
            "4\n",
            "half_way\n",
            "5\n",
            "half_way\n",
            "6\n",
            "half_way\n",
            "7\n",
            "half_way\n",
            "8\n",
            "half_way\n",
            "9\n",
            "half_way\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzO8zqFnZMa"
      },
      "source": [
        "torch.save(baseline_model.state_dict(), \"bert_sent_seq2seq.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_O-Cko3dz3l"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPRy-Xddze8",
        "outputId": "90895ba0-a5d5-4b39-c7dc-3ccad67304b6"
      },
      "source": [
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "baseline_model.load_state_dict(torch.load(\"bert_sent_seq2seq.pt\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzYTy0sOV1C-"
      },
      "source": [
        "def predict_greedy(model, sentence, max_length=100):\n",
        "    \"\"\"Make predictions for the given input using greedy inference.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: A input string.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        Model's predicted greedy response for the input, represented as string.\n",
        "    \"\"\"\n",
        "\n",
        "    # You should make only one call to model.encode() at the start of the function, \n",
        "    # and make only one call to model.decode() per inference step.\n",
        "    model.eval()\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    sent = [start]\n",
        "    i = 0\n",
        "    while start != eos_id and i < 100:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "          start = torch.argmax(out[0], 0)\n",
        "          sent.append(start.item())\n",
        "          i += 1\n",
        "    sent = vocab.decode_sentence_from_ids(sent)\n",
        "    \n",
        "    return sent\n",
        "score = 0\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bxb7a8SoPwx"
      },
      "source": [
        "def predict_beam(model, sentence, k=3, max_length=100, thresh=-9999):\n",
        "    \"\"\"Make predictions for the given inputs using beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: An input sentence, represented as string.\n",
        "        k: The size of the beam.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        A list of k beam predictions. Each element in the list should be a string\n",
        "        corresponding to one of the top k predictions for the corresponding input,\n",
        "        sorted in descending order by its final score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: once an eos_token has been generated for any beam, \n",
        "    # remove its subsequent predictions from that beam by adding a small negative \n",
        "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
        "    # candidates are removed from the beam, as its probability will be very close\n",
        "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
        "    # finished candidate\n",
        "\n",
        "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
        "    # constant for simplicity (aside from the sequence length), some special care\n",
        "    # will need to be taken on the first iteration to ensure that your beam\n",
        "    # doesn't fill up with k identical copies of the same candidate.\n",
        "    \n",
        "    # You are welcome to tweak alpha\n",
        "    alpha = 0.9\n",
        "    model.eval()\n",
        "    beams = []\n",
        "    curr = []\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    \n",
        "    out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "    out = torch.log_softmax(out[0], 0)\n",
        "    values, start = torch.topk(out, k, 0)\n",
        "    for i in range(len(values)):\n",
        "      # Each beam contains the log probs at its first index and the hidden states at its last index\n",
        "      beams.append([values[i], start[i].item(), hid])\n",
        "    \n",
        "    generation = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "      curr = []\n",
        "      for j in beams:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(j[-2]).cuda(), 0), j[-1], x, mask)\n",
        "          out = torch.log_softmax(out[0], 0)\n",
        "          values, start = torch.topk(out, k, 0)\n",
        "          for z in range(len(values)):\n",
        "            temp = j.copy()\n",
        "            temp[0] = values[z] + temp[0]\n",
        "            temp.insert(-1, start[z].item())\n",
        "            temp[-1] = hid\n",
        "            curr.append(temp)\n",
        "      curr = sorted(curr,reverse=True, key=lambda x: x[0])\n",
        "      curr = curr[0:k - i]\n",
        "      beams = []\n",
        "      for j in curr:\n",
        "        if j[-2] == eos_id or len(j) > 20:\n",
        "          generation.append(j[:-1])\n",
        "          i +=1\n",
        "        else:\n",
        "          beams.append(j)\n",
        "    final = []\n",
        "    generation = sorted(generation, reverse=True, key=lambda x: x[0]/(len(x)-1)**alpha)\n",
        "    for i in generation:\n",
        "      if i[0].item() > thresh:\n",
        "        final.append(vocab.decode_sentence_from_ids(i[1:]))\n",
        "    return final\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyBv1bLMab9Q",
        "outputId": "71bee7b7-050e-492c-e2c8-1d58393b0a21"
      },
      "source": [
        ""
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[tensor(-0.9381, device='cuda:0', grad_fn=<AddBackward0>), 13935, 9059, 1806, 2535, 2], [tensor(-1.5582, device='cuda:0', grad_fn=<AddBackward0>), 6536, 52236, 14165, 51055, 2535, 2], [tensor(-1.7241, device='cuda:0', grad_fn=<AddBackward0>), 13329, 43, 4023, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kinw9KqKWlNr",
        "outputId": "9e3d7876-3d30-478e-c2b7-a74717cda5a0"
      },
      "source": [
        "score = 0\n",
        "for i in range(len(predictions)):\n",
        "  for j in predictions[i]:\n",
        "    if jaccard(test_y[i], vocab.decode_sentence_from_ids(j[1:])) > 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"max accuracy\")\n",
        "print(score/len(predictions))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max accuracy\n",
            "0.9286287089013633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPJ5_xjpaE2m"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(sent_emb)):\n",
        "  if np.sum(sent_emb[i]) not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, sent_emb[i], thresh=-2.5)\n",
        "    testing[np.sum(sent_emb[i])] = (predictions, [labels[i]])\n",
        "  else:\n",
        "    testing[np.sum(sent_emb[i])][1].append(labels[i])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT9PbJ_MX8Q5",
        "outputId": "a78df2a6-01bb-425c-b92b-cc443159961d"
      },
      "source": [
        "print(len(testing.keys()))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CiejJ_Bq-a-"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  prediction = i[0]\n",
        "  cop = prediction.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in prediction:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL1CcJLmrEvw",
        "outputId": "344b0fcc-a09e-46be-9707-0d44ab94f080"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "860\n",
            "277\n",
            "0.8223159868729489\n",
            "accuracy\n",
            "0.9047455295735901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8syivdHxGKr"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(test_x)):\n",
        "  if np.sum(test_x[i]) not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, test_x[i], thresh=-2.5)\n",
        "    testing[np.sum(test_x[i])] = (predictions, [test_y[i]])\n",
        "  else:\n",
        "    testing[np.sum(test_x[i])][1].append(test_y[i])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8X6qwXgxJWJ"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  prediction = i[0]\n",
        "  cop = prediction.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in prediction:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDb1o6daxWVE",
        "outputId": "2ebad0fc-193c-4cf7-f26f-7855cad2fab4"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "792\n",
            "249\n",
            "0.6572275271649655\n",
            "accuracy\n",
            "0.8003207698476343\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}