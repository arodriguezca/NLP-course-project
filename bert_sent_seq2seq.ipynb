{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DataCleaning_(2)_(2) (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e9bf25218ac4150afa2040aae50e17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f41160fb6d646a4afc6a02754572557",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e81913e75dd14dc1afa34a7911988f5c",
              "IPY_MODEL_2b6e80bb5cf0417da7416378ef630c3a"
            ]
          }
        },
        "2f41160fb6d646a4afc6a02754572557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e81913e75dd14dc1afa34a7911988f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c1bd67539af6442d87cf47b551799fc5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 405361158,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 405361158,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dac7bb4f2dac4b8d9d83bd80b425ad5f"
          }
        },
        "2b6e80bb5cf0417da7416378ef630c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ce3a13805ca41fbae88d0a1e81a5944",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 405M/405M [00:19&lt;00:00, 20.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84d9b215e4b647f9a173c05b484b5d3f"
          }
        },
        "c1bd67539af6442d87cf47b551799fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dac7bb4f2dac4b8d9d83bd80b425ad5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ce3a13805ca41fbae88d0a1e81a5944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84d9b215e4b647f9a173c05b484b5d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMUa4s5DQvc"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.autonotebook import tqdm\n",
        "from functools import partial\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "#!pip install transformers\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "#import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeX5_0w9SBLL",
        "outputId": "2bcc83c2-b43c-4922-c745-bb13185bf96e"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  1 19:32:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NcQtlQS5EHb",
        "outputId": "c09933c0-7dd8-45d8-ce68-27579ed23135"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9mUxiWiEh7F",
        "outputId": "37cc32cf-7cc9-4b15-f30e-7af3a786d13f"
      },
      "source": [
        "#data = pd.read_csv(\"data.csv\")\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "data.head()\n",
        "print(len(data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQEH7zrm9Az"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbBZ1pv1oEv"
      },
      "source": [
        "text = []\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6]\n",
        "\n",
        "\n",
        "  text.append((t, data.loc[i][5]))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxs0JLGXn-9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67db322e-4a9d-493d-cce7-92e5c9d45d57"
      },
      "source": [
        "pad_word = \"<pad>\"\n",
        "bos_word = \"<s>\"\n",
        "eos_word = \"</s>\"\n",
        "unk_word = \"<unk>\"\n",
        "pad_id = 0\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "unk_id = 3\n",
        "    \n",
        "def normalize_sentence(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
        "        self.word_count = {}\n",
        "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
        "        self.num_words = 4\n",
        "    \n",
        "    def get_ids_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
        "                               else unk_id for word in sentence.split()] + \\\n",
        "                               [eos_id]\n",
        "        return sent_ids\n",
        "    \n",
        "    def tokenized_sentence(self, sentence):\n",
        "        sent_ids = self.get_ids_from_sentence(sentence)\n",
        "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
        "\n",
        "    def decode_sentence_from_ids(self, sent_ids):\n",
        "        words = list()\n",
        "        for i, word_id in enumerate(sent_ids):\n",
        "            if word_id in [bos_id, eos_id, pad_id]:\n",
        "                # Skip these words\n",
        "                continue\n",
        "            else:\n",
        "                words.append(self.id_to_word[word_id])\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def add_words_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word_to_id:\n",
        "                # add this word to the vocabulary\n",
        "                self.word_to_id[word] = self.num_words\n",
        "                self.id_to_word[self.num_words] = word\n",
        "                self.word_count[word] = 1\n",
        "                self.num_words += 1\n",
        "            else:\n",
        "                # update the word count\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for src, tgt in text:\n",
        "    vocab.add_words_from_sentence(src)\n",
        "    vocab.add_words_from_sentence(tgt)\n",
        "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in the vocabulary = 56347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAJMAVOUYPe",
        "outputId": "5386f97b-83f4-4657-d8dd-4786be91b148"
      },
      "source": [
        "print(vocab.id_to_word[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7n2N3wn_QF"
      },
      "source": [
        "Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lPbDN736FF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv5P8QHaKeoN",
        "outputId": "97e083a1-6d40-486d-8822-bc35e9a56498"
      },
      "source": [
        "import spacy\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 8.1MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 25.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=af311e876afc4340be2e9873d4632c42729d40e9bfcf194c6efac8d9fb1d151f\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.45 sentence-transformers-1.1.0 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4dFP8RIKqTH"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDpk8FaZLF1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9e9bf25218ac4150afa2040aae50e17f",
            "2f41160fb6d646a4afc6a02754572557",
            "e81913e75dd14dc1afa34a7911988f5c",
            "2b6e80bb5cf0417da7416378ef630c3a",
            "c1bd67539af6442d87cf47b551799fc5",
            "dac7bb4f2dac4b8d9d83bd80b425ad5f",
            "5ce3a13805ca41fbae88d0a1e81a5944",
            "84d9b215e4b647f9a173c05b484b5d3f"
          ]
        },
        "outputId": "c83c27a9-4c10-4f65-a9f6-c02cdf600d74"
      },
      "source": [
        "sent = SentenceTransformer(\"facebook-dpr-ctx_encoder-single-nq-base\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e9bf25218ac4150afa2040aae50e17f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=405361158.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9wUbRGULO4z"
      },
      "source": [
        "sent_sep = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7qpzdLLjad"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQLzkWefgWk"
      },
      "source": [
        "sent = sent.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giCBNRQzgDBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRSEghjdL9A"
      },
      "source": [
        "load embeddings manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSF-qZaqL8Pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "2d6abc6b-9082-42d6-fd66-e76b5d7546d4"
      },
      "source": [
        "sent_emb = []\n",
        "for j in range(len(data)):\n",
        "  x = sent_sep(data.loc[j][6])\n",
        "  y = list(x.sents)\n",
        "  z = []\n",
        "  for i in y:\n",
        "    z.append(i.text)\n",
        "  y = sent.encode(z)\n",
        "  sent_emb.append(y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7781999f1ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0msent_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Lhxi2QkZdPll",
        "outputId": "cf677ab7-916f-4c6c-986e-2c15425e8872"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "      <th>text</th>\n",
              "      <th>mask</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
              "      <td>Parental Effort, School Resources, and Student...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This article investigates an important factor...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.4189563  -0.03409137 -0.18767388 ... -0.5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>81b5f874-9b42-4d4f-8c50-b4bab24f8aed</td>\n",
              "      <td>The Gender Gap Reloaded: Are School Characteri...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>This study examines the wage gender gap of yo...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.13089143 -0.26295418 -0.5080046  ... -0.6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>f35fdfa6-a3f9-4ef3-a858-c8e2efcd2349</td>\n",
              "      <td>The High School Environment and the Gender Gap...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Despite the striking reversal of the gender g...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.41674316  0.20468165 -0.3782102  ... -0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>e5a4c6af-0854-4f7e-b088-06f03c18a01a</td>\n",
              "      <td>Locus of Control and Peer Relationships Among ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Abstract Past research has shown that locus o...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.07650556 -0.12806207 -0.14252882 ... -0.7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>d5ff0ecd-ccf5-4f8a-a09c-182376eed4d0</td>\n",
              "      <td>Beyond the Laboratory: Evaluating the Survey E...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>The black-white gap in achievement, as measur...</td>\n",
              "      <td>True</td>\n",
              "      <td>[[ 0.12524939  0.05704077 -0.537129   ... -0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         embeddings\n",
              "0           0  ...  [[ 0.4189563  -0.03409137 -0.18767388 ... -0.5...\n",
              "1           1  ...  [[ 0.13089143 -0.26295418 -0.5080046  ... -0.6...\n",
              "2           2  ...  [[ 0.41674316  0.20468165 -0.3782102  ... -0.3...\n",
              "3           3  ...  [[ 0.07650556 -0.12806207 -0.14252882 ... -0.7...\n",
              "4           4  ...  [[ 0.12524939  0.05704077 -0.537129   ... -0.2...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efz7_q4bdbO-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD4w_J_SO6U8"
      },
      "source": [
        "labels = []\n",
        "for j in range(len(data)):\n",
        "  labels.append(data.loc[j][5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3_WIQAn0dppH",
        "outputId": "1b052c4d-159d-457b-a427-e51610184805"
      },
      "source": [
        "labels[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'national education longitudinal study'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bps11Qq_c_oH"
      },
      "source": [
        "#data[\"embeddings\"] = sent_emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swTSgVVdDSZ"
      },
      "source": [
        "#data.to_csv(\"data_with_embeddings.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWax-FY_dYae"
      },
      "source": [
        "sent_emb, test_x, labels, test_y = train_test_split(sent_emb, labels, test_size=0.3, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIHVhVjaM3qn"
      },
      "source": [
        "import torch.nn as nn\n",
        "class Seq2seqBaseline(nn.Module):\n",
        "    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize your model's parameters here. To get started, we suggest\n",
        "        # setting all embedding and hidden dimensions to 300, using encoder and\n",
        "        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n",
        "\n",
        "        # Implementation tip: To create a bidirectional GRU, you don't need to\n",
        "        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n",
        "        \n",
        "        self.num_words = vocab.num_words\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.E = nn.Embedding(self.num_words, emb_dim)\n",
        "\n",
        "        self.encoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=True)\n",
        "        self.decoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=False)\n",
        "        self.lin = nn.Linear(300, vocab.num_words)\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def encode(self, source):\n",
        "        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_src_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                encoder_output: The output hidden representation of the encoder \n",
        "                    with shape (max_src_sequence_length, batch_size, hidden_size).\n",
        "                    Can be obtained by adding the hidden representations of both \n",
        "                    directions of the encoder bidirectional GRU. \n",
        "                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n",
        "                    batch_size) indicating which encoder outputs correspond to padding\n",
        "                    tokens. Its elements should be True at positions corresponding to\n",
        "                    padding tokens and False elsewhere.\n",
        "                encoder_hidden: The final hidden states of the bidirectional GRU \n",
        "                    (after a suitable projection) that will be used to initialize \n",
        "                    the decoder. This should be a tensor h_n with shape \n",
        "                    (num_layers, batch_size, hidden_size). Note that the hidden \n",
        "                    state returned by the bi-GRU cannot be used directly. Its \n",
        "                    initial dimension is twice the required size because it \n",
        "                    contains state from two directions.\n",
        "\n",
        "        The first two return values are not required for the baseline model and will\n",
        "        only be used later in the attention model. If desired, they can be replaced\n",
        "        with None for the initial implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: consider using packed sequences to more easily work\n",
        "        # with the variable-length sequences represented by the source tensor.\n",
        "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "\n",
        "        # Implementation tip: there are many simple ways to combine the forward\n",
        "        # and backward portions of the final hidden state, e.g. addition, averaging,\n",
        "        # or a linear transformation of the appropriate size. Any of these\n",
        "        # should let you reach the required performance.\n",
        "\n",
        "        # Compute a tensor containing the length of each source sequence.\n",
        "\n",
        "        hid = torch.zeros(4, source.shape[1], 300).cuda()\n",
        "        encoder_mask = source != 0\n",
        "        x, hid = self.encoder(source, hid)\n",
        "\n",
        "        x = x[:,:,:self.hidden_dim] + x [:,:,self.hidden_dim:]\n",
        "        hid = hid[:self.num_layers,:,:] + hid[self.num_layers:,:,:]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        return  x, encoder_mask, hid\n",
        "\n",
        "    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n",
        "        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n",
        "\n",
        "        The third and fourth arguments are not used in the baseline model, but are\n",
        "        included for compatibility with the attention model in the next section.\n",
        "\n",
        "        Args:\n",
        "            decoder_input: An integer tensor with shape (1, batch_size) containing \n",
        "                the subword indices for the current decoder input.\n",
        "            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n",
        "                state of the decoder, each with shape (num_layers, batch_size,\n",
        "                hidden_size). For the first decoding step the last_hidden will be \n",
        "                encoder's final hidden representation.\n",
        "            encoder_output: The output of the encoder with shape\n",
        "                (max_src_sequence_length, batch_size, hidden_size).\n",
        "            encoder_mask: The output mask from the encoder with shape\n",
        "                (max_src_sequence_length, batch_size). Encoder outputs at positions\n",
        "                with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                logits: A tensor with shape (batch_size,\n",
        "                    vocab_size) containing unnormalized scores for the next-word\n",
        "                    predictions at each position.\n",
        "                decoder_hidden: tensor h_n with the same shape as last_hidden \n",
        "                    representing the updated decoder state after processing the \n",
        "                    decoder input.\n",
        "                attention_weights: This will be implemented later in the attention\n",
        "                    model, but in order to maintain compatible type signatures, we also\n",
        "                    include it here. This can be None or any other placeholder value.\n",
        "        \"\"\"\n",
        "\n",
        "        # These arguments are not used in the baseline model.\n",
        "        del encoder_output\n",
        "        del encoder_mask\n",
        "        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        x = sent.encode(vocab.id_to_word[decoder_input[0][0].item()])\n",
        "        x = torch.Tensor(x).unsqueeze(0).cuda()\n",
        "        x = x.unsqueeze(1)\n",
        " \n",
        "        x, hid = self.decoder(x, last_hidden)\n",
        "\n",
        "        x = self.lin(torch.squeeze(x, 0))\n",
        "\n",
        "\n",
        "        return x, hid, 0\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "\n",
        "    def compute_loss(self, source, target):\n",
        "        \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_source_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "            target: An integer tensor with shape (max_target_sequence_length,\n",
        "                batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "        Returns:\n",
        "            A scalar float tensor representing cross-entropy loss on the current batch\n",
        "            divided by the number of target tokens in the batch.\n",
        "            Many of the target tokens will be pad tokens. You should mask the loss \n",
        "            from these tokens using appropriate mask on the target tokens loss.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: don't feed the target tensor directly to the decoder.\n",
        "        # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
        "        # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "        # suffix A B C </s>.\n",
        "        x, mask, hid = self.encode(source)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss = 0\n",
        "        for i in range(target.shape[0]- 1):\n",
        "          out, hid, temp = self.decode(target[i], hid, x, mask)\n",
        "\n",
        "          loss += loss_func(out, target[i + 1])\n",
        "        return loss/target.shape[0]\n",
        "\n",
        "        # You may run self.encode() on the source only once and decode the target \n",
        "        # one step at a time.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ..."
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZCmCIdNJ8N"
      },
      "source": [
        "def train(model, train, label, num_epochs, model_file, learning_rate=0.0001):\n",
        "    \"\"\"Train the model for given number of epochs and save the trained model in \n",
        "    the final model_file.\n",
        "    \"\"\"\n",
        "\n",
        "    decoder_learning_ratio = 5.0\n",
        "    \n",
        "    encoder_parameter_names = [\"E\", \"encoder\"]\n",
        "                               \n",
        "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    encoder_params = [e[1] for e in encoder_named_params]\n",
        "    decoder_params = [e[1] for e in decoder_named_params]\n",
        "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
        "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
        "    \n",
        "    clip = 50.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(epoch)\n",
        "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
        "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for i in range(len(train)):\n",
        "          if i == len(train)/2:\n",
        "            print(\"half_way\")\n",
        "          source = torch.Tensor(train[i]).unsqueeze(1).cuda()\n",
        "          tgt_ids = vocab.get_ids_from_sentence(label[i])\n",
        "          target = torch.LongTensor(tgt_ids).unsqueeze(1).cuda()\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.compute_loss(source, target)\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          # Gradient clipping before taking the step\n",
        "          _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "    # Save the model after training         \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcLWK6jZOboM",
        "outputId": "7a80b2b7-781e-4722-adbe-0258e9c81f43"
      },
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "\n",
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "train(baseline_model, sent_emb, labels, num_epochs, \"sent_emb_model.pt\")\n",
        "# Download the trained model to local for future use\n",
        "#files.download('baseline_model.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "half_way\n",
            "1\n",
            "half_way\n",
            "2\n",
            "half_way\n",
            "3\n",
            "half_way\n",
            "4\n",
            "half_way\n",
            "5\n",
            "half_way\n",
            "6\n",
            "half_way\n",
            "7\n",
            "half_way\n",
            "8\n",
            "half_way\n",
            "9\n",
            "half_way\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzO8zqFnZMa"
      },
      "source": [
        "torch.save(baseline_model.state_dict(), \"bert_sent_seq2seq.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_O-Cko3dz3l"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWPRy-Xddze8",
        "outputId": "c924c580-008b-447d-9a4d-a0e8ce9886c7"
      },
      "source": [
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "baseline_model.load_state_dict(torch.load(\"bert_sent_seq2seq.pt\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzYTy0sOV1C-"
      },
      "source": [
        "def predict_greedy(model, sentence, max_length=100):\n",
        "    \"\"\"Make predictions for the given input using greedy inference.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: A input string.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        Model's predicted greedy response for the input, represented as string.\n",
        "    \"\"\"\n",
        "\n",
        "    # You should make only one call to model.encode() at the start of the function, \n",
        "    # and make only one call to model.decode() per inference step.\n",
        "    model.eval()\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    sent = [start]\n",
        "    i = 0\n",
        "    while start != eos_id and i < 100:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "          start = torch.argmax(out[0], 0)\n",
        "          sent.append(start.item())\n",
        "          i += 1\n",
        "    sent = vocab.decode_sentence_from_ids(sent)\n",
        "    \n",
        "    return sent\n",
        "score = 0\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bxb7a8SoPwx"
      },
      "source": [
        "def predict_beam(model, sentence, k=3, max_length=100, thresh=-9999):\n",
        "    \"\"\"Make predictions for the given inputs using beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: An input sentence, represented as string.\n",
        "        k: The size of the beam.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        A list of k beam predictions. Each element in the list should be a string\n",
        "        corresponding to one of the top k predictions for the corresponding input,\n",
        "        sorted in descending order by its final score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: once an eos_token has been generated for any beam, \n",
        "    # remove its subsequent predictions from that beam by adding a small negative \n",
        "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
        "    # candidates are removed from the beam, as its probability will be very close\n",
        "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
        "    # finished candidate\n",
        "\n",
        "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
        "    # constant for simplicity (aside from the sequence length), some special care\n",
        "    # will need to be taken on the first iteration to ensure that your beam\n",
        "    # doesn't fill up with k identical copies of the same candidate.\n",
        "    \n",
        "    # You are welcome to tweak alpha\n",
        "    alpha = 0.9\n",
        "    model.eval()\n",
        "    beams = []\n",
        "    curr = []\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    \n",
        "    out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "    out = torch.log_softmax(out[0], 0)\n",
        "    values, start = torch.topk(out, k, 0)\n",
        "    for i in range(len(values)):\n",
        "      # Each beam contains the log probs at its first index and the hidden states at its last index\n",
        "      beams.append([values[i], start[i].item(), hid])\n",
        "    \n",
        "    generation = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "      curr = []\n",
        "      for j in beams:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(j[-2]).cuda(), 0), j[-1], x, mask)\n",
        "          out = torch.log_softmax(out[0], 0)\n",
        "          values, start = torch.topk(out, k, 0)\n",
        "          for z in range(len(values)):\n",
        "            temp = j.copy()\n",
        "            temp[0] = values[z] + temp[0]\n",
        "            temp.insert(-1, start[z].item())\n",
        "            temp[-1] = hid\n",
        "            curr.append(temp)\n",
        "      curr = sorted(curr,reverse=True, key=lambda x: x[0])\n",
        "      curr = curr[0:k - i]\n",
        "      beams = []\n",
        "      for j in curr:\n",
        "        if j[-2] == eos_id or len(j) > 20:\n",
        "          generation.append(j[:-1])\n",
        "          i +=1\n",
        "        else:\n",
        "          beams.append(j)\n",
        "    final = []\n",
        "    generation = sorted(generation, reverse=True, key=lambda x: x[0]/(len(x)-1)**alpha)\n",
        "    for i in generation:\n",
        "      if i[0].item() > thresh:\n",
        "        final.append(vocab.decode_sentence_from_ids(i[1:]))\n",
        "    return final\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyBv1bLMab9Q",
        "outputId": "71bee7b7-050e-492c-e2c8-1d58393b0a21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[tensor(-0.9381, device='cuda:0', grad_fn=<AddBackward0>), 13935, 9059, 1806, 2535, 2], [tensor(-1.5582, device='cuda:0', grad_fn=<AddBackward0>), 6536, 52236, 14165, 51055, 2535, 2], [tensor(-1.7241, device='cuda:0', grad_fn=<AddBackward0>), 13329, 43, 4023, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kinw9KqKWlNr",
        "outputId": "9e3d7876-3d30-478e-c2b7-a74717cda5a0"
      },
      "source": [
        "score = 0\n",
        "for i in range(len(predictions)):\n",
        "  for j in predictions[i]:\n",
        "    if jaccard(test_y[i], vocab.decode_sentence_from_ids(j[1:])) > 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"max accuracy\")\n",
        "print(score/len(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max accuracy\n",
            "0.9286287089013633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPJ5_xjpaE2m"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(sent_emb)):\n",
        "  if np.sum(sent_emb[i]) not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, sent_emb[i], thresh=-2.5)\n",
        "    testing[np.sum(sent_emb[i])] = (predictions, [labels[i]])\n",
        "  else:\n",
        "    testing[np.sum(sent_emb[i])][1].append(labels[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT9PbJ_MX8Q5",
        "outputId": "a78df2a6-01bb-425c-b92b-cc443159961d"
      },
      "source": [
        "print(len(testing.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CiejJ_Bq-a-"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  prediction = i[0]\n",
        "  cop = prediction.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in prediction:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umZQwm3MpR26"
      },
      "source": [
        "TRAINING PERFORMANCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL1CcJLmrEvw",
        "outputId": "344b0fcc-a09e-46be-9707-0d44ab94f080"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "860\n",
            "277\n",
            "0.8223159868729489\n",
            "accuracy\n",
            "0.9047455295735901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8syivdHxGKr"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(test_x)):\n",
        "  if np.sum(test_x[i]) not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, test_x[i], thresh=-2.5)\n",
        "    testing[np.sum(test_x[i])] = (predictions, [test_y[i]])\n",
        "  else:\n",
        "    testing[np.sum(test_x[i])][1].append(test_y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8X6qwXgxJWJ"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  prediction = i[0]\n",
        "  cop = prediction.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in prediction:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYafypzab2HH"
      },
      "source": [
        "Testing Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDb1o6daxWVE",
        "outputId": "2ebad0fc-193c-4cf7-f26f-7855cad2fab4"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "792\n",
            "249\n",
            "0.6572275271649655\n",
            "accuracy\n",
            "0.8003207698476343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijFD8oqeb1Ne"
      },
      "source": [
        "\n",
        "Testing on other data that doesn't include exact label in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erdO_KkKb0fo"
      },
      "source": [
        "data = pd.read_csv(\"data_false.csv\")\n",
        "data = data[data.text.notnull()].reset_index()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Q_2WLEAHcUsk",
        "outputId": "106623ce-eca1-4cb9-a054-4a1bea111cc6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Id</th>\n",
              "      <th>pub_title</th>\n",
              "      <th>dataset_title</th>\n",
              "      <th>dataset_label</th>\n",
              "      <th>cleaned_label</th>\n",
              "      <th>text</th>\n",
              "      <th>mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
              "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Federal Reserve Bank of Richmond S1. Accounti...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1b21f60a-4022-4b19-95ce-6fd7157d4aa9</td>\n",
              "      <td>Examining Latinos Involvement in the Workforce...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>In this article, the authors report the resul...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>14</td>\n",
              "      <td>7a2d20d9-fe83-4d24-b4ce-992f92f21bd2</td>\n",
              "      <td>Immigrant educators and studentsâ€™ academic ach...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>ABSTRACT\\nUsing a dataset which allows studen...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>16</td>\n",
              "      <td>a8c7306f-908c-4d44-92b1-2afb2e066808</td>\n",
              "      <td>Demographic Differences in Patterns of Youth O...</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Abstract: Participation in structured out-of-...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>21</td>\n",
              "      <td>0ebdaf88-543a-4d88-9185-a24f8dc1e4cf</td>\n",
              "      <td>Grade Inflation</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>National Education Longitudinal Study</td>\n",
              "      <td>national education longitudinal study</td>\n",
              "      <td>Although much speculation has been devoted to...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  Unnamed: 0  ...                                               text   mask\n",
              "0      3           3  ...   Federal Reserve Bank of Richmond S1. Accounti...  False\n",
              "1      4           7  ...   In this article, the authors report the resul...  False\n",
              "2      8          14  ...   ABSTRACT\\nUsing a dataset which allows studen...  False\n",
              "3      9          16  ...   Abstract: Participation in structured out-of-...  False\n",
              "4     13          21  ...   Although much speculation has been devoted to...  False\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "bqlkhw3_dUZ1",
        "outputId": "2a4dc4a6-519d-425f-9441-f112feba2c4b"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" In this article, the authors report the results of two studies examining the participation rates of Latino students in postsecondary technical education (CTE) programs in community colleges and two-year proprietary institutions in the United States in 1994 and 2000. It is believed that the quality of the future U.S. Labor market will depend, to a great extent, on this group's education and job skills. Although Latinos are the fastest growing minority group in the United States, they are also the poorest and most undereducated when compared to other minority groups. Results of both studies show that few Latino students enroll in and graduate from postsecondary CTE programs. Of those students that do enroll in and complete CTE programs at the postsecondary level, very few complete programs that are considered high-skill, high-wage.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEKlmelgb9O_"
      },
      "source": [
        "sent_emb = []\n",
        "for j in range(4000):\n",
        "  x = sent_sep(data.loc[j][7])\n",
        "  y = list(x.sents)\n",
        "  z = []\n",
        "  for i in y:\n",
        "    z.append(i.text)\n",
        "  y = sent.encode(z)\n",
        "  sent_emb.append(y)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcxeoKl-cRhm"
      },
      "source": [
        "labels = []\n",
        "for j in range(0, 4000):\n",
        "  labels.append(data.loc[j][6])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLTqkfrYdCJ8"
      },
      "source": [
        "score = 0\n",
        "for i in range(len(sent_emb)):\n",
        "  predictions = predict_beam(baseline_model, sent_emb[i])\n",
        "  for j in predictions:\n",
        "    if jaccard(labels[i], j) > 0.5:\n",
        "      score += 1\n",
        "      break"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbrsHMKWertz",
        "outputId": "6f371b54-d670-463b-d3e3-1a7ff6729195"
      },
      "source": [
        "print(score/len(sent_emb))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H8b8hLQnnXt"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(sent_emb)):\n",
        "  if np.sum(sent_emb[i]) not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, sent_emb[i], thresh=-2.5)\n",
        "    testing[np.sum(sent_emb[i])] = (predictions, [labels[i]])\n",
        "  else:\n",
        "    testing[np.sum(sent_emb[i])][1].append(labels[i])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1A66xhtns8q"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  prediction = i[0]\n",
        "  cop = prediction.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in prediction:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DedqlSK8nyY3",
        "outputId": "ee698a2e-24b7-4ef2-f3c1-fa45d3a660c8"
      },
      "source": [
        "\n",
        "\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro F score\n",
            "3328\n",
            "1140\n",
            "0.5614448370632116\n",
            "accuracy\n",
            "0.715\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}