{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DataCleaning_(2) (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5609b2512c2b45759ea3e4ab98c77538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_df4f5b6ea83c418f8e546f146f63c444",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aed73a7e89cc449a8a7e0f2d065c8aba",
              "IPY_MODEL_a08b5a2f7f5d468eb00a53063d04bc7f"
            ]
          }
        },
        "df4f5b6ea83c418f8e546f146f63c444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aed73a7e89cc449a8a7e0f2d065c8aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6fcf10580b404eeda7c00c2eb7033591",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 405361158,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 405361158,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ea4543eaed34341a58667b24ab4492b"
          }
        },
        "a08b5a2f7f5d468eb00a53063d04bc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cf796b09958c4838ba6f5095ff74aa85",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 405M/405M [00:18&lt;00:00, 21.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1b3e07a53a1411b89fc585aa8973990"
          }
        },
        "6fcf10580b404eeda7c00c2eb7033591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ea4543eaed34341a58667b24ab4492b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf796b09958c4838ba6f5095ff74aa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1b3e07a53a1411b89fc585aa8973990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMUa4s5DQvc"
      },
      "source": [
        "#Libraries\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from tqdm.autonotebook import tqdm\n",
        "from functools import partial\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "#!pip install transformers\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "#import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeX5_0w9SBLL",
        "outputId": "1646f588-ba17-4a46-85e9-9303908b8be4"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')\n",
        "random.seed(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 29 15:05:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NcQtlQS5EHb",
        "outputId": "3d766582-5fce-45f1-9a8b-d7d689884b7c"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9mUxiWiEh7F",
        "outputId": "0ddcf294-c0de-4ee3-9440-33e924fffd33"
      },
      "source": [
        "#data = pd.read_csv(\"data.csv\")\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "data.head()\n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQEH7zrm9Az"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbBZ1pv1oEv"
      },
      "source": [
        "text = []\n",
        "for i in range(len(data)):\n",
        "  t = data.loc[i][6]\n",
        "\n",
        "\n",
        "  text.append((t, data.loc[i][5]))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxs0JLGXn-9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1f3c16-ca6c-4ba7-a66e-d7962acd2fed"
      },
      "source": [
        "pad_word = \"<pad>\"\n",
        "bos_word = \"<s>\"\n",
        "eos_word = \"</s>\"\n",
        "unk_word = \"<unk>\"\n",
        "pad_id = 0\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "unk_id = 3\n",
        "    \n",
        "def normalize_sentence(s):\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
        "        self.word_count = {}\n",
        "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
        "        self.num_words = 4\n",
        "    \n",
        "    def get_ids_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
        "                               else unk_id for word in sentence.split()] + \\\n",
        "                               [eos_id]\n",
        "        return sent_ids\n",
        "    \n",
        "    def tokenized_sentence(self, sentence):\n",
        "        sent_ids = self.get_ids_from_sentence(sentence)\n",
        "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
        "\n",
        "    def decode_sentence_from_ids(self, sent_ids):\n",
        "        words = list()\n",
        "        for i, word_id in enumerate(sent_ids):\n",
        "            if word_id in [bos_id, eos_id, pad_id]:\n",
        "                # Skip these words\n",
        "                continue\n",
        "            else:\n",
        "                words.append(self.id_to_word[word_id])\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def add_words_from_sentence(self, sentence):\n",
        "        sentence = normalize_sentence(sentence)\n",
        "        for word in sentence.split():\n",
        "            if word not in self.word_to_id:\n",
        "                # add this word to the vocabulary\n",
        "                self.word_to_id[word] = self.num_words\n",
        "                self.id_to_word[self.num_words] = word\n",
        "                self.word_count[word] = 1\n",
        "                self.num_words += 1\n",
        "            else:\n",
        "                # update the word count\n",
        "                self.word_count[word] += 1\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for src, tgt in text:\n",
        "    vocab.add_words_from_sentence(src)\n",
        "    vocab.add_words_from_sentence(tgt)\n",
        "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in the vocabulary = 56347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAJMAVOUYPe",
        "outputId": "d0d3f107-52d2-4f6c-ebde-c5760d3e9b6b"
      },
      "source": [
        "print(vocab.id_to_word[1])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz7n2N3wn_QF"
      },
      "source": [
        "Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6lPbDN736FF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzbdU3bu6b0M",
        "outputId": "e16843d7-b440-4538-e235-5fd11392a231"
      },
      "source": [
        "def predict_greedy(model, sentence, max_length=100):\n",
        "    \"\"\"Make predictions for the given input using greedy inference.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: A input string.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        Model's predicted greedy response for the input, represented as string.\n",
        "    \"\"\"\n",
        "\n",
        "    # You should make only one call to model.encode() at the start of the function, \n",
        "    # and make only one call to model.decode() per inference step.\n",
        "    model.eval()\n",
        "    source = torch.unsqueeze(torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    sent = [start]\n",
        "    i = 0\n",
        "    while start != eos_id and i < 100:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "          start = torch.argmax(out[0], 0)\n",
        "          sent.append(start.item())\n",
        "          i += 1\n",
        "    sent = vocab.decode_sentence_from_ids(sent)\n",
        "    \n",
        "    return sent\n",
        "print(predict_greedy(baseline_model, text[2][0]))\n",
        "print(text[2][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baltimore longitudinal study of aging\n",
            "baltimore longitudinal study of aging blsa \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtEc6K0786Jy",
        "outputId": "a4839b71-8a7b-4fbd-c6c7-e2de15624a1f"
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "score = 0\n",
        "for j in range(0,100):\n",
        "\n",
        "  if jaccard(text[j][1], predict_greedy(baseline_model, text[j][0])) >= 0.5:\n",
        "    score += 1\n",
        "print(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-OjVm4K55s"
      },
      "source": [
        "def predict_beam(model, sentence, k=3, max_length=100, thresh=-9999):\n",
        "    \"\"\"Make predictions for the given inputs using beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: An input sentence, represented as string.\n",
        "        k: The size of the beam.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        A list of k beam predictions. Each element in the list should be a string\n",
        "        corresponding to one of the top k predictions for the corresponding input,\n",
        "        sorted in descending order by its final score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: once an eos_token has been generated for any beam, \n",
        "    # remove its subsequent predictions from that beam by adding a small negative \n",
        "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
        "    # candidates are removed from the beam, as its probability will be very close\n",
        "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
        "    # finished candidate\n",
        "\n",
        "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
        "    # constant for simplicity (aside from the sequence length), some special care\n",
        "    # will need to be taken on the first iteration to ensure that your beam\n",
        "    # doesn't fill up with k identical copies of the same candidate.\n",
        "    \n",
        "    # You are welcome to tweak alpha\n",
        "    alpha = 0.9\n",
        "    model.eval()\n",
        "    beams = []\n",
        "    curr = []\n",
        "    source = torch.unsqueeze(torch.tensor(vocab.get_ids_from_sentence(sentence)).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    \n",
        "    out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "    out = torch.log_softmax(out[0], 0)\n",
        "    values, start = torch.topk(out, k, 0)\n",
        "    for i in range(len(values)):\n",
        "      # Each beam contains the log probs at its first index and the hidden states at its last index\n",
        "      beams.append([values[i], start[i].item(), hid])\n",
        "    \n",
        "    generation = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "      curr = []\n",
        "      for j in beams:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(j[-2]).cuda(), 0), j[-1], x, mask)\n",
        "          out = torch.log_softmax(out[0], 0)\n",
        "          values, start = torch.topk(out, k, 0)\n",
        "          for z in range(len(values)):\n",
        "            temp = j.copy()\n",
        "            temp[0] = values[z] + temp[0]\n",
        "            temp.insert(-1, start[z].item())\n",
        "            temp[-1] = hid\n",
        "            curr.append(temp)\n",
        "      curr = sorted(curr,reverse=True, key=lambda x: x[0])\n",
        "      curr = curr[0:k - i]\n",
        "      beams = []\n",
        "      for j in curr:\n",
        "        if j[-2] == eos_id or len(j) > 20:\n",
        "          generation.append(j[:-1])\n",
        "          i +=1\n",
        "        else:\n",
        "          beams.append(j)\n",
        "    final = []\n",
        "    generation = sorted(generation, reverse=True, key=lambda x: x[0]/(len(x)-1)**alpha)\n",
        "    for i in generation:\n",
        "      if i[0].item() > thresh:\n",
        "        final.append(vocab.decode_sentence_from_ids(i[1:]))\n",
        "    return final\n",
        "\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKkgBuu15R7n"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(test)):\n",
        "  if test[i][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, test[i][0], thresh=-2)\n",
        "    testing[test[i][0]] = (predictions, [test[i][1]])\n",
        "  else:\n",
        "    testing[test[i][0]][1].append(test[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGe1YTnn6CU4"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  true_pred = i[1].copy()\n",
        "  for j in predictions:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQny3KEV7k7l"
      },
      "source": [
        "print(tp)\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPjfZbV_hLS"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(text)):\n",
        "  if text[i][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, text[i][0], thresh=-2)\n",
        "    testing[text[i][0]] = (predictions, [text[i][1]])\n",
        "  else:\n",
        "    testing[text[i][0]][1].append(text[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpeH0f2q_1R8"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  true_pred = i[1].copy()\n",
        "  for j in predictions:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MghPTSy6_3fa",
        "outputId": "b54dc84f-eb1e-4765-f78f-7b70fc5760dd"
      },
      "source": [
        "print(tp)\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3349\n",
            "1967\n",
            "806\n",
            "0.7072114876992925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_bNx1eLLHQd",
        "outputId": "e9b1da60-9728-4eb4-c605-6175983ab5d6"
      },
      "source": [
        "score = 0\n",
        "\n",
        "for j in range(0,len(test), 10):\n",
        "  predictions = predict_beam(baseline_model, test[j][0])\n",
        "  for i in predictions:\n",
        "    if jaccard(test[j][1], i) >= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9F2D-5S2bJ7",
        "outputId": "2dd30aba-3fce-4d5e-f078-1cc8379db79c"
      },
      "source": [
        "print(score*10/len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500400962309543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBnfV7wmao82",
        "outputId": "7b261ae4-f2b9-4394-cc1d-f412394bebb9"
      },
      "source": [
        "print(\"testing_accuracy\")\n",
        "print(score/len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing_accuracy\n",
            "0.863672814755413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO-tUxpVazUO",
        "outputId": "993e7078-b38f-4bde-8fc6-fb70bd34e891"
      },
      "source": [
        "score = 0\n",
        "for j in range(0,len(text)):\n",
        "  predictions = predict_beam(baseline_model, text[j][0])\n",
        "  for i in predictions:\n",
        "    if jaccard(text[j][1], i) >= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"training_accuracy\")\n",
        "print(score/len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_accuracy\n",
            "0.8803301237964236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdQqe_4quTAb"
      },
      "source": [
        "Tried testing on unfiltered dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD9KQMvrbKzd"
      },
      "source": [
        "import math\n",
        "test_unfilter = pd.read_csv(\"data_false.csv\")\n",
        "test_unfilter = test_unfilter[test_unfilter.text.notnull()].reset_index()\n",
        "text_2 = []\n",
        "for i in range(len(test_unfilter)):\n",
        "\n",
        "  t = test_unfilter.loc[i][7]\n",
        "\n",
        "  text_2.append((t, test_unfilter.loc[i][6]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StQPja5ddAo0",
        "outputId": "3ffc87e6-0461-4545-cb8c-6e87045759c1"
      },
      "source": [
        "\n",
        "print(text_2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(' Federal Reserve Bank of Richmond S1. Accounting for low enrollment and graduation rates at four-year colleges when returns to graduation are high Several studies claimed that there is underinvestment in education. Judd (2000) combines Capital Asset Pricing Model (CAPM) techniques with the indivisibility of human capital to compare the return to four-year college graduation with assets of similar risk to find an excess of return to the college investment option. Heckman, Lochner, and Todd (2008) evaluate the internal rate of return of the four-year college investment option relative to work to find that since 1960, internal rates of return have been around 10 percent or higher, depending on the cohort and different specifications of labor markets and taxes. Cunha, Heckman, and Navarro (2005), using data from NLSY/1979, extend the analysis to evaluate the internal rate of return for the marginal student, the agent with the lowest observable measures of ability who enrolls in four-year college, to find an unexplained wedge in returns. 1 Cunha, Heckman, and Navarro (2005) concluded that this wedge is explained by nonpecuniary costs of education, namely, tastes for school, risk aversion, and other. The evidence presented in this paper points in a different direction: the wedge in returns is explained by the existence of academic two-year colleges (in fact, it is actually more general because vocational schools explain the wedge in returns for students who enroll in academic two-year colleges), as high school graduates sort across the different enrollment alternatives. In particular, high school graduates who enroll in fouryear colleges have measures of observables that lie above those who enroll in academic two-year colleges. It follows that the high school graduate with the lowest measures of observables who enrolls in a four-year college is indifferent to enrollment at academic two-year colleges as opposed to being indifferent to joining the labor force. Using the parameterized version of the model, it is possible to quantify the return for this marginal student and how much of average returns for the population that enrolls in four-year colleges is explained by the return of the marginal student. The return for the student Nicholas Trachter: nicholas.trachter@rich.frb.org\\nThe views expressed in this article are mine and do not necessarily represent the views of the Federal Reserve Bank of Richmond or the Federal Reserve System.', 'national education longitudinal study')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iaVYmdwciOr",
        "outputId": "8374996f-7bf5-4434-df1d-b306d567b589"
      },
      "source": [
        "score = 0\n",
        "for j in range(0,len(text_2), 50):\n",
        "  predictions = predict_beam(baseline_model, text_2[j][0])\n",
        "  for i in predictions:\n",
        "    if jaccard(text_2[j][1], i) >= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(\"training_accuracy\")\n",
        "print(score/(len(text_2)/50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_accuracy\n",
            "0.5281361002056459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv5P8QHaKeoN",
        "outputId": "9e43ce4b-de4d-4d1a-fb0a-73737cdb34f1"
      },
      "source": [
        "import spacy\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 7.5MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1MB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 55.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=af0fa2858ba7807229a204ab4f6459e53c22e51a370e95d84114e44a591783ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.45 sentence-transformers-1.1.0 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4dFP8RIKqTH"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "5609b2512c2b45759ea3e4ab98c77538",
            "df4f5b6ea83c418f8e546f146f63c444",
            "aed73a7e89cc449a8a7e0f2d065c8aba",
            "a08b5a2f7f5d468eb00a53063d04bc7f",
            "6fcf10580b404eeda7c00c2eb7033591",
            "1ea4543eaed34341a58667b24ab4492b",
            "cf796b09958c4838ba6f5095ff74aa85",
            "d1b3e07a53a1411b89fc585aa8973990"
          ]
        },
        "id": "kDpk8FaZLF1M",
        "outputId": "2cedcc28-6638-43dd-c18e-9813b87ad8c4"
      },
      "source": [
        "sent = SentenceTransformer(\"facebook-dpr-ctx_encoder-single-nq-base\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5609b2512c2b45759ea3e4ab98c77538",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=405361158.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9wUbRGULO4z"
      },
      "source": [
        "sent_sep = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7qpzdLLjad"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSF-qZaqL8Pd"
      },
      "source": [
        "sent_emb = []\n",
        "for j in range(len(data)):\n",
        "  x = sent_sep(data.loc[j][6])\n",
        "  y = list(x.sents)\n",
        "  z = []\n",
        "  for i in y:\n",
        "    z.append(i.text)\n",
        "  y = sent.encode(z)\n",
        "  sent_emb.append(y)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD4w_J_SO6U8"
      },
      "source": [
        "labels = []\n",
        "for j in range(len(data)):\n",
        "  labels.append(data.loc[j][5])"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bps11Qq_c_oH"
      },
      "source": [
        "data[\"embeddings\"] = sent_emb"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-swTSgVVdDSZ"
      },
      "source": [
        "data.to_csv(\"data_with_embeddings.csv\")"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWax-FY_dYae"
      },
      "source": [
        "sent_emb, test_x, labels, test_y = train_test_split(sent_emb, labels, test_size=0.3, random_state=10)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIHVhVjaM3qn"
      },
      "source": [
        "import torch.nn as nn\n",
        "class Seq2seqBaseline(nn.Module):\n",
        "    def __init__(self, vocab, emb_dim = 300, hidden_dim = 300, num_layers = 2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize your model's parameters here. To get started, we suggest\n",
        "        # setting all embedding and hidden dimensions to 300, using encoder and\n",
        "        # decoder GRUs with 2 layers, and using a dropout rate of 0.1.\n",
        "\n",
        "        # Implementation tip: To create a bidirectional GRU, you don't need to\n",
        "        # create two GRU networks. Instead use nn.GRU(..., bidirectional=True).\n",
        "        \n",
        "        self.num_words = vocab.num_words\n",
        "        self.emb_dim = emb_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.E = nn.Embedding(self.num_words, emb_dim)\n",
        "\n",
        "        self.encoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=True)\n",
        "        self.decoder = nn.GRU(768, hidden_dim, 2, dropout=dropout, bidirectional=False)\n",
        "        self.lin = nn.Linear(300, vocab.num_words)\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def encode(self, source):\n",
        "        \"\"\"Encode the source batch using a bidirectional GRU encoder.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_src_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                encoder_output: The output hidden representation of the encoder \n",
        "                    with shape (max_src_sequence_length, batch_size, hidden_size).\n",
        "                    Can be obtained by adding the hidden representations of both \n",
        "                    directions of the encoder bidirectional GRU. \n",
        "                encoder_mask: A boolean tensor with shape (max_src_sequence_length,\n",
        "                    batch_size) indicating which encoder outputs correspond to padding\n",
        "                    tokens. Its elements should be True at positions corresponding to\n",
        "                    padding tokens and False elsewhere.\n",
        "                encoder_hidden: The final hidden states of the bidirectional GRU \n",
        "                    (after a suitable projection) that will be used to initialize \n",
        "                    the decoder. This should be a tensor h_n with shape \n",
        "                    (num_layers, batch_size, hidden_size). Note that the hidden \n",
        "                    state returned by the bi-GRU cannot be used directly. Its \n",
        "                    initial dimension is twice the required size because it \n",
        "                    contains state from two directions.\n",
        "\n",
        "        The first two return values are not required for the baseline model and will\n",
        "        only be used later in the attention model. If desired, they can be replaced\n",
        "        with None for the initial implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: consider using packed sequences to more easily work\n",
        "        # with the variable-length sequences represented by the source tensor.\n",
        "        # See https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence.\n",
        "\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "\n",
        "        # Implementation tip: there are many simple ways to combine the forward\n",
        "        # and backward portions of the final hidden state, e.g. addition, averaging,\n",
        "        # or a linear transformation of the appropriate size. Any of these\n",
        "        # should let you reach the required performance.\n",
        "\n",
        "        # Compute a tensor containing the length of each source sequence.\n",
        "\n",
        "        hid = torch.zeros(4, source.shape[1], 300).cuda()\n",
        "        encoder_mask = source != 0\n",
        "        x, hid = self.encoder(source, hid)\n",
        "\n",
        "        x = x[:,:,:self.hidden_dim] + x [:,:,self.hidden_dim:]\n",
        "        hid = hid[:self.num_layers,:,:] + hid[self.num_layers:,:,:]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "        return  x, encoder_mask, hid\n",
        "\n",
        "    def decode(self, decoder_input, last_hidden, encoder_output, encoder_mask):\n",
        "        \"\"\"Run the decoder GRU for one decoding step from the last hidden state.\n",
        "\n",
        "        The third and fourth arguments are not used in the baseline model, but are\n",
        "        included for compatibility with the attention model in the next section.\n",
        "\n",
        "        Args:\n",
        "            decoder_input: An integer tensor with shape (1, batch_size) containing \n",
        "                the subword indices for the current decoder input.\n",
        "            last_hidden: A pair of tensors h_{t-1} representing the last hidden\n",
        "                state of the decoder, each with shape (num_layers, batch_size,\n",
        "                hidden_size). For the first decoding step the last_hidden will be \n",
        "                encoder's final hidden representation.\n",
        "            encoder_output: The output of the encoder with shape\n",
        "                (max_src_sequence_length, batch_size, hidden_size).\n",
        "            encoder_mask: The output mask from the encoder with shape\n",
        "                (max_src_sequence_length, batch_size). Encoder outputs at positions\n",
        "                with a True value correspond to padding tokens and should be ignored.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with three elements:\n",
        "                logits: A tensor with shape (batch_size,\n",
        "                    vocab_size) containing unnormalized scores for the next-word\n",
        "                    predictions at each position.\n",
        "                decoder_hidden: tensor h_n with the same shape as last_hidden \n",
        "                    representing the updated decoder state after processing the \n",
        "                    decoder input.\n",
        "                attention_weights: This will be implemented later in the attention\n",
        "                    model, but in order to maintain compatible type signatures, we also\n",
        "                    include it here. This can be None or any other placeholder value.\n",
        "        \"\"\"\n",
        "\n",
        "        # These arguments are not used in the baseline model.\n",
        "        del encoder_output\n",
        "        del encoder_mask\n",
        "        decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        x = sent.encode(vocab.id_to_word[decoder_input[0][0].item()])\n",
        "        x = torch.Tensor(x).unsqueeze(0).cuda()\n",
        "        x = x.unsqueeze(1)\n",
        " \n",
        "        x, hid = self.decoder(x, last_hidden)\n",
        "\n",
        "        x = self.lin(torch.squeeze(x, 0))\n",
        "\n",
        "\n",
        "        return x, hid, 0\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "\n",
        "    def compute_loss(self, source, target):\n",
        "        \"\"\"Run the model on the source and compute the loss on the target.\n",
        "\n",
        "        Args:\n",
        "            source: An integer tensor with shape (max_source_sequence_length,\n",
        "                batch_size) containing subword indices for the source sentences.\n",
        "            target: An integer tensor with shape (max_target_sequence_length,\n",
        "                batch_size) containing subword indices for the target sentences.\n",
        "\n",
        "        Returns:\n",
        "            A scalar float tensor representing cross-entropy loss on the current batch\n",
        "            divided by the number of target tokens in the batch.\n",
        "            Many of the target tokens will be pad tokens. You should mask the loss \n",
        "            from these tokens using appropriate mask on the target tokens loss.\n",
        "        \"\"\"\n",
        "\n",
        "        # Implementation tip: don't feed the target tensor directly to the decoder.\n",
        "        # To see why, note that for a target sequence like <s> A B C </s>, you would\n",
        "        # want to run the decoder on the prefix <s> A B C and have it predict the\n",
        "        # suffix A B C </s>.\n",
        "        x, mask, hid = self.encode(source)\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss = 0\n",
        "        for i in range(target.shape[0]- 1):\n",
        "          out, hid, temp = self.decode(target[i], hid, x, mask)\n",
        "\n",
        "          loss += loss_func(out, target[i + 1])\n",
        "        return loss/target.shape[0]\n",
        "\n",
        "        # You may run self.encode() on the source only once and decode the target \n",
        "        # one step at a time.\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        ..."
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZCmCIdNJ8N"
      },
      "source": [
        "def train(model, train, label, num_epochs, model_file, learning_rate=0.0001):\n",
        "    \"\"\"Train the model for given number of epochs and save the trained model in \n",
        "    the final model_file.\n",
        "    \"\"\"\n",
        "\n",
        "    decoder_learning_ratio = 5.0\n",
        "    \n",
        "    encoder_parameter_names = [\"E\", \"encoder\"]\n",
        "                               \n",
        "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
        "    encoder_params = [e[1] for e in encoder_named_params]\n",
        "    decoder_params = [e[1] for e in decoder_named_params]\n",
        "    optimizer = torch.optim.AdamW([{'params': encoder_params},\n",
        "                {'params': decoder_params, 'lr': learning_rate * decoder_learning_ratio}], lr=learning_rate)\n",
        "    \n",
        "    clip = 50.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(epoch)\n",
        "        # print(f\"Total training instances = {len(train_dataset)}\")\n",
        "        # print(f\"train_data_loader = {len(train_data_loader)} {1180 > len(train_data_loader)/20}\")\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for i in range(len(train)):\n",
        "          if i == len(train)/2:\n",
        "            print(\"half_way\")\n",
        "          source = torch.Tensor(train[i]).unsqueeze(1).cuda()\n",
        "          tgt_ids = vocab.get_ids_from_sentence(label[i])\n",
        "          target = torch.LongTensor(tgt_ids).unsqueeze(1).cuda()\n",
        "          optimizer.zero_grad()\n",
        "          loss = model.compute_loss(source, target)\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          # Gradient clipping before taking the step\n",
        "          _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "    # Save the model after training         \n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcLWK6jZOboM",
        "outputId": "7a80b2b7-781e-4722-adbe-0258e9c81f43"
      },
      "source": [
        "# You are welcome to adjust these parameters based on your model implementation.\n",
        "num_epochs = 10\n",
        "\n",
        "baseline_model = Seq2seqBaseline(vocab).cuda()\n",
        "train(baseline_model, sent_emb, labels, num_epochs, \"sent_emb_model.pt\")\n",
        "# Download the trained model to local for future use\n",
        "#files.download('baseline_model.pt')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "half_way\n",
            "1\n",
            "half_way\n",
            "2\n",
            "half_way\n",
            "3\n",
            "half_way\n",
            "4\n",
            "half_way\n",
            "5\n",
            "half_way\n",
            "6\n",
            "half_way\n",
            "7\n",
            "half_way\n",
            "8\n",
            "half_way\n",
            "9\n",
            "half_way\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzO8zqFnZMa"
      },
      "source": [
        "torch.save(baseline_model.state_dict(), \"bert_sent_seq2seq.pt\")"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzYTy0sOV1C-",
        "outputId": "a89165d8-09d7-4b77-c3f9-cb17670273b0"
      },
      "source": [
        "def predict_greedy(model, sentence, max_length=100):\n",
        "    \"\"\"Make predictions for the given input using greedy inference.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: A input string.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        Model's predicted greedy response for the input, represented as string.\n",
        "    \"\"\"\n",
        "\n",
        "    # You should make only one call to model.encode() at the start of the function, \n",
        "    # and make only one call to model.decode() per inference step.\n",
        "    model.eval()\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    sent = [start]\n",
        "    i = 0\n",
        "    while start != eos_id and i < 100:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "          start = torch.argmax(out[0], 0)\n",
        "          sent.append(start.item())\n",
        "          i += 1\n",
        "    sent = vocab.decode_sentence_from_ids(sent)\n",
        "    \n",
        "    return sent\n",
        "score = 0\n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "for i in range(0, len(sent_emb), 10):\n",
        "  if jaccard(predict_greedy(baseline_model, sent_emb[i]), labels[i])>= 0.5:\n",
        "    score += 1\n",
        "print(score*10/len(sent_emb))\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7255845942228336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bxb7a8SoPwx",
        "outputId": "c25511a0-6005-400e-d1b7-dff5740ccfa2"
      },
      "source": [
        "def predict_beam(model, sentence, k=3, max_length=100, thresh=-9999):\n",
        "    \"\"\"Make predictions for the given inputs using beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: A sequence-to-sequence model.\n",
        "        sentence: An input sentence, represented as string.\n",
        "        k: The size of the beam.\n",
        "        max_length: The maximum length at which to truncate outputs in order to\n",
        "            avoid non-terminating inference.\n",
        "    \n",
        "    Returns:\n",
        "        A list of k beam predictions. Each element in the list should be a string\n",
        "        corresponding to one of the top k predictions for the corresponding input,\n",
        "        sorted in descending order by its final score.\n",
        "    \"\"\"\n",
        "\n",
        "    # Implementation tip: once an eos_token has been generated for any beam, \n",
        "    # remove its subsequent predictions from that beam by adding a small negative \n",
        "    # number like -1e9 to the appropriate logits. This will ensure that the \n",
        "    # candidates are removed from the beam, as its probability will be very close\n",
        "    # to 0. Using this method, uou will be able to reuse the beam of an already \n",
        "    # finished candidate\n",
        "\n",
        "    # Implementation tip: while you are encouraged to keep your tensor dimensions\n",
        "    # constant for simplicity (aside from the sequence length), some special care\n",
        "    # will need to be taken on the first iteration to ensure that your beam\n",
        "    # doesn't fill up with k identical copies of the same candidate.\n",
        "    \n",
        "    # You are welcome to tweak alpha\n",
        "    alpha = 0.9\n",
        "    model.eval()\n",
        "    beams = []\n",
        "    curr = []\n",
        "    source = torch.unsqueeze(torch.tensor(sentence).cuda(), 1)\n",
        "    x, mask, hid = model.encode(source)\n",
        "    start = bos_id\n",
        "    \n",
        "    out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(start).cuda(), 0), hid, x, mask)\n",
        "    out = torch.log_softmax(out[0], 0)\n",
        "    values, start = torch.topk(out, k, 0)\n",
        "    for i in range(len(values)):\n",
        "      # Each beam contains the log probs at its first index and the hidden states at its last index\n",
        "      beams.append([values[i], start[i].item(), hid])\n",
        "    \n",
        "    generation = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "      curr = []\n",
        "      for j in beams:\n",
        "          out, hid, temp = model.decode(torch.unsqueeze(torch.tensor(j[-2]).cuda(), 0), j[-1], x, mask)\n",
        "          out = torch.log_softmax(out[0], 0)\n",
        "          values, start = torch.topk(out, k, 0)\n",
        "          for z in range(len(values)):\n",
        "            temp = j.copy()\n",
        "            temp[0] = values[z] + temp[0]\n",
        "            temp.insert(-1, start[z].item())\n",
        "            temp[-1] = hid\n",
        "            curr.append(temp)\n",
        "      curr = sorted(curr,reverse=True, key=lambda x: x[0])\n",
        "      curr = curr[0:k - i]\n",
        "      beams = []\n",
        "      for j in curr:\n",
        "        if j[-2] == eos_id or len(j) > 20:\n",
        "          generation.append(j[:-1])\n",
        "          i +=1\n",
        "        else:\n",
        "          beams.append(j)\n",
        "    final = []\n",
        "    generation = sorted(generation, reverse=True, key=lambda x: x[0]/(len(x)-1)**alpha)\n",
        "    for i in generation:\n",
        "      if i[0].item() > thresh:\n",
        "        final.append(vocab.decode_sentence_from_ids(i[1:]))\n",
        "    return final\n",
        "score = 0\n",
        "for i in range(0, len(sent_emb), 10):\n",
        "  predictions = predict_beam(baseline_model, sent_emb[i], thresh=-2)\n",
        "  for j in predictions:\n",
        "    if jaccard(j, labels[i])>= 0.5:\n",
        "      score += 1\n",
        "      break\n",
        "print(score*10/len(sent_emb))\n"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.859697386519945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf49lCAIn3jC"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(sent_emb)):\n",
        "  if sent_emb[i][0][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, sent_emb[i], thresh=-3)\n",
        "    testing[sent_emb[i][0][0]] = (predictions, [labels[i]])\n",
        "  else:\n",
        "    testing[sent_emb[i][0][0]][1].append(labels[i])"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CiejJ_Bq-a-"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  cop = predictions.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in predictions:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL1CcJLmrEvw",
        "outputId": "486711e1-1da6-45fe-86c4-70609718c5fb"
      },
      "source": [
        "print(\"training performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training performance\n",
            "micro F score\n",
            "1000\n",
            "481\n",
            "0.7662194159431729\n",
            "accuracy\n",
            "0.8345942228335625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8syivdHxGKr"
      },
      "source": [
        "testing = {}\n",
        "for i in range(0, len(test_x)):\n",
        "  if test_x[i][0][0] not in testing.keys():\n",
        "    predictions = predict_beam(baseline_model, test_x[i], thresh=-3)\n",
        "    testing[test_x[i][0][0]] = (predictions, [test_y[i]])\n",
        "  else:\n",
        "    testing[test_x[i][0][0]][1].append(test_y[i])"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8X6qwXgxJWJ"
      },
      "source": [
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "for i in testing.values():\n",
        "  predictions = i[0]\n",
        "  cop = predictions.copy()\n",
        "  true_pred = i[1].copy()\n",
        "  check = False\n",
        "  #check exact match first\n",
        "  for j in predictions:\n",
        "    if j in true_pred:\n",
        "      tp += 1\n",
        "      true_pred.remove(j)\n",
        "      cop.remove(j)\n",
        "  #then check rest for jaccard score\n",
        "  for j in cop:\n",
        "    found = False\n",
        "    removal = 0\n",
        "    for k in true_pred:\n",
        "      if jaccard(j, k) >= 0.5:\n",
        "        found = True\n",
        "        removal = k\n",
        "        break\n",
        "    if found:\n",
        "      tp += 1\n",
        "      true_pred.remove(removal)\n",
        "    else:\n",
        "      fp += 1\n",
        "  fn += len(true_pred)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDb1o6daxWVE",
        "outputId": "760f5c0e-5843-40d0-8389-fad59f45e640"
      },
      "source": [
        "print(\"testing performance\")\n",
        "print(\"micro F score\")\n",
        "print(fp)\n",
        "print(fn)\n",
        "print(tp/(tp + 1/2*(fp+fn)))\n",
        "print(\"accuracy\")\n",
        "print(tp/(tp+fn))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing performance\n",
            "micro F score\n",
            "863\n",
            "255\n",
            "0.6395873629916183\n",
            "accuracy\n",
            "0.7955092221331195\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}